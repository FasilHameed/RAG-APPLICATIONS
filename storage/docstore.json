{"docstore/metadata": {"6f5ac2d6-458e-46bd-a16b-99e79c1169f0": {"doc_hash": "eaa9b8d1c5ee85a48711fb889b39edf62c2aae1bcd0b3d39bda8b9b15e5eadea"}, "08df43f0-66b4-4f7b-a2a4-867e7ceed120": {"doc_hash": "89ea2e081c2766fd50e9be0737542a409527b4f3ebad9b794ab3326376771245"}, "07f99e2a-8a29-4863-9dd7-8a6fc6669058": {"doc_hash": "f85dc7f3b9a99fff4634fd76bd4ba4163e5731f95acf4e5e57dd7a5de09cfb17"}, "0dd8e9fc-8037-4781-b79a-5ac2a43654a0": {"doc_hash": "1cdd28e03d8e6e36936f60ec9f0106145722ae85c5391775f7ce7153ee2ebc47"}, "3a7bbee4-26b3-484a-ae14-16f0033a6b78": {"doc_hash": "b64a2a8dba2cf085f8cfc535588e72e5b17f2de16a10541e24a9518e18eb43f2"}, "6fc3e839-d2ba-4c04-b128-29a06c66ce37": {"doc_hash": "33538290e3d4ec2f759ce5af6db5ef7959cbfaac54c5db73f5ca4334eae6f0ed"}, "309371d3-c649-4d38-978d-129cda618a4b": {"doc_hash": "eba5f01e8457b39a21bb3785737aa791d4ce923e87a0e506b5c264c4ed3b78e2"}, "f83328ea-b7cd-4770-a9fc-0921614750d5": {"doc_hash": "842e4667416522f8acb402c60225279c957318a020092ee3d534fc79d689cf57"}, "c2d98846-01f8-4c7b-93cc-58b59152ec6a": {"doc_hash": "abcbe9598aeb83453758b7f6c6c719fb6efa9b12fc7f3bbfa6e5e25f7c63dc83"}, "097e20b6-c825-47ac-bde3-e819ada0d7a6": {"doc_hash": "0552fc6232e3ee7a20463d5c954df89c685ff9cb1f46a33e60e021edd783e510"}, "b739cd9f-479e-40af-b5f9-ca1130fd22ef": {"doc_hash": "0eeaa97bcf95d8884da2dff5ad6585269c61380cbc1c377dc44c5505b77e6807"}, "fb93cfb2-c772-4087-b80f-6c33fa1ac276": {"doc_hash": "15cf079524395e43197b32e3a762070bb48716cf6b3ff9d5f695ecd1cbfb6f54"}, "ba3bbb6e-e313-48ed-8ecb-22b67481b822": {"doc_hash": "3f251563fc9801c32ba1c6547a471582f8aa9991cbb1678dd0d8fe7e56b9e249"}, "000ec4ee-73c7-489a-a0a0-4f2ddbe96279": {"doc_hash": "c6a0a03b02c712643a4cc9e378454513881dcf9ba64723bc45266731806d45ea"}, "8218156a-6485-4c4f-87ff-d61a5c7713fb": {"doc_hash": "74ac7a8e5301faac2f4d82fe34039c3f39dc221633cf8bf3aaaa87ddf414b72d"}, "6640492e-0115-4fb7-8c98-f82749b683e1": {"doc_hash": "2b704c6c39816d523c2e550f8979ae9c2c96221b3902ad4756bb590bb981e815"}, "8ad2a569-cd4b-4305-84e7-073c6fd01aa8": {"doc_hash": "1d72aeedc9de1e4ae623183fac6e826600b207936fd67b792283d7171b91e375"}, "27a7f5ca-a0ad-4c4e-bc7a-e7ba1ddddd65": {"doc_hash": "1ab2201f5b043e411e1fdd424502ec064865e1d5a63291b31195a18ac0158af6"}, "da46a3dd-d186-4e85-96d2-4ce903ed0f84": {"doc_hash": "ddcbbcb03a663dad97334eb0249362ee1533711914f7df4e50a7dacb8e992b4c"}, "1c4b5901-ea90-4603-9e64-f3e49de73a06": {"doc_hash": "8374cd17b828ae83e891a976be0fa4e79917868c55d534659cd4ed2aaf05dcc7"}, "d8c32baf-e976-4c41-9e49-c5fb6c01e93c": {"doc_hash": "8bf396675eb4c728f2afa57b29ce433761b3abf678e4863100cf8dc550e0054f"}, "b709ba6e-cc70-4811-902c-a513de62c8ac": {"doc_hash": "2626cf4d38ea8f4ce1543961e1d8f68a8d191a3339ac25e80b4888c3bc33c759"}, "5046b664-6d35-4b4d-9529-de82cf07730c": {"doc_hash": "41ab4cac59398a3512fff2942a483b2e2b4985c6b7fcdb7d6eabdd64c4862a7f"}, "801abaf9-b805-4411-8488-efee2556f926": {"doc_hash": "5145f1a2e2a375b2a97152caee718fe92bc84a15cef1ee9cbea6d8ff2d38dfbf"}, "12073eb4-59a6-476f-896b-94c1a65bf9ab": {"doc_hash": "01698d3b0e187b4c5ab458f90ed73f0c75e58969c5c0ca031fc795b3c8462044"}, "09b6b1c1-a608-47bb-978d-70d9e5a34236": {"doc_hash": "716b477782a907d28fc2e5d32238097f1616c607bd933ae6a66594fd173e56c8"}, "883382e7-f13e-4195-99d7-08fc8c05a2de": {"doc_hash": "d2389c2321900bdb7cba804be19c5d839afcb3c078e3962818e0ce39e2a67ec5"}, "2aed3d7f-8075-4833-a35e-7b41d917fbdd": {"doc_hash": "8eb3a08f7d5aef1f33b8845b7f443ea283ed3d87fe0ee0082e7f49a161653295"}, "95fa3d9e-798b-4cce-90e9-16caeecf2d2c": {"doc_hash": "1d81436903451d21354cdc076fe1167ee5e936d53240bc609a41e5c8a655e71e"}, "d8877978-9e4a-4524-9da1-dde2d09bb59e": {"doc_hash": "198b8ef76d69b467480802ac7dff43ca9f9334cabbb2d8bc51ebe07839b4f3d5"}, "66961e48-a891-4a3a-b1da-5b410e742284": {"doc_hash": "1ee182610232113ad32760323748d95671b1e578fc75afa233de82a9e13a8ced"}, "35a17c4d-a332-4238-894a-aa4d925b0281": {"doc_hash": "326c08fcb87972494fbc221349dfae83e337857816290013da18993b048fc9bf"}, "d3aec5d3-5d75-4830-90fd-a7ac97c2f698": {"doc_hash": "867cfc2154096e43d158c622debe1a67a06232c507b5ed5f8481cf4a38cb1b70"}, "1e556286-3fcd-4921-a794-873084fd4807": {"doc_hash": "7ad738638b47d27372c080724c9867f34480ad2aba720dc38f48c50639e0c7ab"}, "d34d3d5f-b1d2-41f9-820e-9c007640f7a4": {"doc_hash": "d47ac87cc1a1d01897b7464f7bdd4dc9c68c4650ebdd1e9ccfe16300743529fd"}, "c6cc38c7-6e41-4e35-a796-7a6352bcecb6": {"doc_hash": "9f1c2f1a54a5cc9d6e7755430b47d074c4c79275f4cdcc70ebf9e69c562f71dc"}, "b25babec-d753-464f-9c63-da898433b600": {"doc_hash": "eaa9b8d1c5ee85a48711fb889b39edf62c2aae1bcd0b3d39bda8b9b15e5eadea", "ref_doc_id": "6f5ac2d6-458e-46bd-a16b-99e79c1169f0"}, "37ee8168-10b0-43c1-8191-cf2875ce6c9e": {"doc_hash": "89ea2e081c2766fd50e9be0737542a409527b4f3ebad9b794ab3326376771245", "ref_doc_id": "08df43f0-66b4-4f7b-a2a4-867e7ceed120"}, "41e021e8-4f27-40b0-a43b-edcd006fc13c": {"doc_hash": "f85dc7f3b9a99fff4634fd76bd4ba4163e5731f95acf4e5e57dd7a5de09cfb17", "ref_doc_id": "07f99e2a-8a29-4863-9dd7-8a6fc6669058"}, "94eef090-f7a4-42a4-ae6e-6e296fe45603": {"doc_hash": "1cdd28e03d8e6e36936f60ec9f0106145722ae85c5391775f7ce7153ee2ebc47", "ref_doc_id": "0dd8e9fc-8037-4781-b79a-5ac2a43654a0"}, "f10575cd-5ac6-4b3f-867e-c7e443de4786": {"doc_hash": "b64a2a8dba2cf085f8cfc535588e72e5b17f2de16a10541e24a9518e18eb43f2", "ref_doc_id": "3a7bbee4-26b3-484a-ae14-16f0033a6b78"}, "aad02db8-130b-4882-b822-9b5604601742": {"doc_hash": "33538290e3d4ec2f759ce5af6db5ef7959cbfaac54c5db73f5ca4334eae6f0ed", "ref_doc_id": "6fc3e839-d2ba-4c04-b128-29a06c66ce37"}, "03dfa554-549e-4184-803b-ffd2aba6c2e9": {"doc_hash": "eba5f01e8457b39a21bb3785737aa791d4ce923e87a0e506b5c264c4ed3b78e2", "ref_doc_id": "309371d3-c649-4d38-978d-129cda618a4b"}, "d7f23b71-9f6b-4952-bc7e-1c816d0fb36d": {"doc_hash": "842e4667416522f8acb402c60225279c957318a020092ee3d534fc79d689cf57", "ref_doc_id": "f83328ea-b7cd-4770-a9fc-0921614750d5"}, "bd70091a-7b9d-486a-8709-181ecf63a6b5": {"doc_hash": "abcbe9598aeb83453758b7f6c6c719fb6efa9b12fc7f3bbfa6e5e25f7c63dc83", "ref_doc_id": "c2d98846-01f8-4c7b-93cc-58b59152ec6a"}, "c649e544-0987-4c6c-bc36-a03c5a599d32": {"doc_hash": "c9ed4f1ad23d38a87451b31a5fa96604d894f62cd980218d21f040d921be96be", "ref_doc_id": "097e20b6-c825-47ac-bde3-e819ada0d7a6"}, "8a3726c7-9dbe-4ce1-b409-efeca9546a71": {"doc_hash": "00d40a1ff615e5628229b61a95d874f4e86690a630cc08bd9ee5995647c74da9", "ref_doc_id": "097e20b6-c825-47ac-bde3-e819ada0d7a6"}, "49a48f98-d8c5-4e15-854f-8d2fee6d5149": {"doc_hash": "0eeaa97bcf95d8884da2dff5ad6585269c61380cbc1c377dc44c5505b77e6807", "ref_doc_id": "b739cd9f-479e-40af-b5f9-ca1130fd22ef"}, "9e24a353-fe5c-40b7-9561-0ee1cad5acc7": {"doc_hash": "ad2d7e724d0738da2384fb6797abee407b8d19b8e04b658c6c94f5c1457f2563", "ref_doc_id": "fb93cfb2-c772-4087-b80f-6c33fa1ac276"}, "b98890a0-fc8f-4232-a2e8-d56e191d21a8": {"doc_hash": "8ffaff6a9f222a403734a31d3c493f7ab118fe1247260fe67e403252edcc4b1f", "ref_doc_id": "fb93cfb2-c772-4087-b80f-6c33fa1ac276"}, "0d5715c6-9a1a-4a43-b057-0a0c637f1e0b": {"doc_hash": "3f251563fc9801c32ba1c6547a471582f8aa9991cbb1678dd0d8fe7e56b9e249", "ref_doc_id": "ba3bbb6e-e313-48ed-8ecb-22b67481b822"}, "c4ae4461-f176-47ec-a42e-2b0342905e08": {"doc_hash": "03e7f88c1df0c6a08ab4d5ab0ad395805cc8142fadef1577cb6260ce52c5db09", "ref_doc_id": "000ec4ee-73c7-489a-a0a0-4f2ddbe96279"}, "cd159237-020e-4168-9332-44b3a195357d": {"doc_hash": "8cc8882b31bc1abf423df3b3e033102a8c301a8319ba2a22f5cf7a9bed8c8ffe", "ref_doc_id": "000ec4ee-73c7-489a-a0a0-4f2ddbe96279"}, "9f01979a-5629-442b-b668-b0dd8fee2294": {"doc_hash": "f4faa2fc83b4ea67514051c086cb1197420cf30d23aa3f19f909d6c6fb5eb93a", "ref_doc_id": "8218156a-6485-4c4f-87ff-d61a5c7713fb"}, "4b831f59-ee10-4f29-a349-2384d00f78f4": {"doc_hash": "d9026cbe1a868ffb43cde4aa49c427e448748826a06b03c3de88d9b211f4acba", "ref_doc_id": "8218156a-6485-4c4f-87ff-d61a5c7713fb"}, "c552f6bc-9894-40b9-b6ba-945c256f37c4": {"doc_hash": "c51eec2634795b7d0239da82872802952c1f08f5ff73c84c6239741b7ddd061b", "ref_doc_id": "6640492e-0115-4fb7-8c98-f82749b683e1"}, "fc148e0d-c2c6-487a-8682-d08f2c02284d": {"doc_hash": "0dc59c77f627d28924d49b4523228e3b68f164e7815576460a178c3aa85f45b2", "ref_doc_id": "6640492e-0115-4fb7-8c98-f82749b683e1"}, "d32811f7-f324-4379-a1e2-33e35a000b54": {"doc_hash": "3e2cbc056bb10373e3c2d460d615b17eae9d3ed9f13b246ebb13bed5533e95dc", "ref_doc_id": "6640492e-0115-4fb7-8c98-f82749b683e1"}, "3c9ce8fa-7f23-49f9-9a51-789f4e03ff6c": {"doc_hash": "a3b9ca8a42118473e246a45e816e45edf631989cf402ad256a2cca2f9edd8e5c", "ref_doc_id": "6640492e-0115-4fb7-8c98-f82749b683e1"}, "b204266c-faee-4602-a169-9b886c8a763d": {"doc_hash": "3dac5c6cb8ceaf3844bdb8617b5e074178c4d9069ff32f84069c68ef66d69936", "ref_doc_id": "6640492e-0115-4fb7-8c98-f82749b683e1"}, "37f30d8e-eac0-46c2-a85f-fe48615e784f": {"doc_hash": "291d914eddae357f53835d216616af77bcd1777fd0f44ecabf528212a5b9878d", "ref_doc_id": "8ad2a569-cd4b-4305-84e7-073c6fd01aa8"}, "190dc89d-720f-4245-a8e7-b5a38479c977": {"doc_hash": "1bb1746c9fc5193068bda03599ab2f189e1f677aea999c3d3b51057ec60a051b", "ref_doc_id": "8ad2a569-cd4b-4305-84e7-073c6fd01aa8"}, "f47f5810-f649-4fbd-b2f9-5e1330569c56": {"doc_hash": "d0cb762f5580021c95fbded22c1e0ff605f6ddeff675e93f1e126180df8505fb", "ref_doc_id": "8ad2a569-cd4b-4305-84e7-073c6fd01aa8"}, "0db7839f-7f06-4c25-be55-09aac901c7b4": {"doc_hash": "2fef59987d145cb7912c4fbfa4923502411711f30ae14799f2156567f18d216a", "ref_doc_id": "8ad2a569-cd4b-4305-84e7-073c6fd01aa8"}, "cd12dccb-a60b-45af-a183-436b82bdfb10": {"doc_hash": "ee4f1b0b5c718f6a987a86c0cd80f8c7f09a191a3d2af0479f4b05a4ab756c87", "ref_doc_id": "8ad2a569-cd4b-4305-84e7-073c6fd01aa8"}, "6ad428dc-65b0-4505-bda8-97106168845e": {"doc_hash": "dc5f6801847351e50014f9bbf1b27b926b1e957c9f8fc293e55def0c4ad0ee8f", "ref_doc_id": "8ad2a569-cd4b-4305-84e7-073c6fd01aa8"}, "b3b1a13f-927d-4c6b-b6fb-f79dbb543aed": {"doc_hash": "23cb3c5c9c075b6a9b009c55b3731a75e203eef546a4b13a0f9d804d1ef8e9c0", "ref_doc_id": "8ad2a569-cd4b-4305-84e7-073c6fd01aa8"}, "62356069-fe6e-40f6-84f7-383d57128a4d": {"doc_hash": "fc71816377c225785fe492740376500e4f61553fb6348619fb0ad06ebd50f488", "ref_doc_id": "8ad2a569-cd4b-4305-84e7-073c6fd01aa8"}, "10c6d2b7-9e6f-43c5-848c-ad53aa7f6455": {"doc_hash": "39b8f0d45e0c1f65a0e51c62c4fc6bccdeda896a248d4cba8869eaf21252ce08", "ref_doc_id": "8ad2a569-cd4b-4305-84e7-073c6fd01aa8"}, "605490da-c024-48e5-91cd-803d434f2701": {"doc_hash": "8d0cc7250bcd4be46704cb966532cebecfed10ec5f6fe7b55fc8448948308c45", "ref_doc_id": "8ad2a569-cd4b-4305-84e7-073c6fd01aa8"}, "b1b2e1a6-b0ca-4205-b2b3-4d6e393d1363": {"doc_hash": "886aee12b49923efbf51446c6e1f86c46159c2b3a8fb62a1352268ba3c7dc08f", "ref_doc_id": "8ad2a569-cd4b-4305-84e7-073c6fd01aa8"}, "04ca27d0-7812-42de-bbc6-24a6d2f4acdc": {"doc_hash": "8916cec4203095cc01c2068c2dd4452d17fc21ec64b1b8ad5f83b51c61961273", "ref_doc_id": "27a7f5ca-a0ad-4c4e-bc7a-e7ba1ddddd65"}, "6822cb04-b0a8-412a-8aeb-69b3d02ed83d": {"doc_hash": "b5dca853c132412195f71bffde61cc81afbdf5d7cc14a3e6e5bffe1266f481a0", "ref_doc_id": "27a7f5ca-a0ad-4c4e-bc7a-e7ba1ddddd65"}, "2aab20ce-7dba-468d-b904-64998792342b": {"doc_hash": "16c84d7d1777e8f7b868118935fe9b119eeefa555aeac9a959aa354ef0891520", "ref_doc_id": "da46a3dd-d186-4e85-96d2-4ce903ed0f84"}, "0ac8ead8-2866-4dbc-8d68-af957df18bf7": {"doc_hash": "95952352b66f552f063e72ca200dc475c541bd21c92567f9120d80c41fd71c7b", "ref_doc_id": "da46a3dd-d186-4e85-96d2-4ce903ed0f84"}, "a9155316-bdd4-43a4-a71e-8cafc54c5c05": {"doc_hash": "8374cd17b828ae83e891a976be0fa4e79917868c55d534659cd4ed2aaf05dcc7", "ref_doc_id": "1c4b5901-ea90-4603-9e64-f3e49de73a06"}, "ad1a9dda-5747-44f8-a302-6b3b9ac20cdf": {"doc_hash": "a3c5fbc7146643ba831efa439874e53f81166af63f5d80a832d04c78e578e665", "ref_doc_id": "d8c32baf-e976-4c41-9e49-c5fb6c01e93c"}, "a3dc245f-7b97-4133-9223-110012d6ce3e": {"doc_hash": "ddf0ef11c975f777b06911ce518c67bdad62801208dd2d270436651bb3a316fd", "ref_doc_id": "d8c32baf-e976-4c41-9e49-c5fb6c01e93c"}, "655ecc22-74fb-4999-8586-e93b4beb56d0": {"doc_hash": "00375b9f35ce3e6d9894a6b625d71f47d4d894ec12da6c975eec939a5459099f", "ref_doc_id": "b709ba6e-cc70-4811-902c-a513de62c8ac"}, "128a3420-93d6-4152-be00-bd6b56d67b01": {"doc_hash": "2ec05d542d71ec1ba528a6173828b8f4fbc49bf2ebcbaf31aab1974f875a4657", "ref_doc_id": "b709ba6e-cc70-4811-902c-a513de62c8ac"}, "204329bd-79eb-4787-a511-f017b6c322e5": {"doc_hash": "93436786d442663a487bd2aa3be8824954a07e80db4ecdc96dd175fb47e5160d", "ref_doc_id": "5046b664-6d35-4b4d-9529-de82cf07730c"}, "b53883d1-11f9-4df9-9cde-9cbd73e45c7e": {"doc_hash": "d76c8578600c2393afaafa1c06046bd32c9e3b99ba2331769592edbdd5948e49", "ref_doc_id": "5046b664-6d35-4b4d-9529-de82cf07730c"}, "2c892c7b-3dc6-4ae9-9787-bec7737985ee": {"doc_hash": "5145f1a2e2a375b2a97152caee718fe92bc84a15cef1ee9cbea6d8ff2d38dfbf", "ref_doc_id": "801abaf9-b805-4411-8488-efee2556f926"}, "3564a4b7-270f-4311-80e5-71a0933efb0f": {"doc_hash": "01698d3b0e187b4c5ab458f90ed73f0c75e58969c5c0ca031fc795b3c8462044", "ref_doc_id": "12073eb4-59a6-476f-896b-94c1a65bf9ab"}, "ec564054-994c-448b-91a9-7d2184f7b475": {"doc_hash": "b33610628cddcdb93bf465f32223fc35cd16ef9f839a43869bbf57e22d10edc9", "ref_doc_id": "09b6b1c1-a608-47bb-978d-70d9e5a34236"}, "fc0ef815-7688-4082-a740-dfb5782eb18c": {"doc_hash": "a3975e8b6227dda3b2aea28ba6501f17174cfbb1642651992ae8e448bad935ad", "ref_doc_id": "09b6b1c1-a608-47bb-978d-70d9e5a34236"}, "c2b13b35-b269-4f53-b4c4-599b82d2c7ca": {"doc_hash": "56cf6d197fb94609838713ff8a1394b3bc2bad4424bfb807f8c2b0f7e8ce6a70", "ref_doc_id": "883382e7-f13e-4195-99d7-08fc8c05a2de"}, "7a8fa370-76a5-48b2-84b9-f22e8077bfe1": {"doc_hash": "2bb35f838b372741dd1cd860f5de86aec2e7568c40f5858ab2cc7e5b433f1e83", "ref_doc_id": "883382e7-f13e-4195-99d7-08fc8c05a2de"}, "1e10b9a7-5cf8-444b-9c18-cee7ba2db12a": {"doc_hash": "8eb3a08f7d5aef1f33b8845b7f443ea283ed3d87fe0ee0082e7f49a161653295", "ref_doc_id": "2aed3d7f-8075-4833-a35e-7b41d917fbdd"}, "dbb96f28-a554-41b5-9791-74ee7bdc13ef": {"doc_hash": "2dba12628f82b6252dbf4da3165b6384cba7c304e2381ad33cb27629fd444ece", "ref_doc_id": "95fa3d9e-798b-4cce-90e9-16caeecf2d2c"}, "4be0ac67-5599-4c5e-9d1f-13cc13249d39": {"doc_hash": "4e05115d6d401255138e6c42ae584a177ab916418e0d4949f03d3e7f45a9f616", "ref_doc_id": "95fa3d9e-798b-4cce-90e9-16caeecf2d2c"}, "2b1cfe90-f9d0-41ac-a129-46d147ad209d": {"doc_hash": "198b8ef76d69b467480802ac7dff43ca9f9334cabbb2d8bc51ebe07839b4f3d5", "ref_doc_id": "d8877978-9e4a-4524-9da1-dde2d09bb59e"}, "094ef49a-395e-4575-8d39-13c15a1c34d7": {"doc_hash": "1ee182610232113ad32760323748d95671b1e578fc75afa233de82a9e13a8ced", "ref_doc_id": "66961e48-a891-4a3a-b1da-5b410e742284"}, "04a7ab6a-e449-49c2-88ff-551eaf09a7e5": {"doc_hash": "326c08fcb87972494fbc221349dfae83e337857816290013da18993b048fc9bf", "ref_doc_id": "35a17c4d-a332-4238-894a-aa4d925b0281"}, "084b76d2-9d47-49fe-a704-b3176e748bac": {"doc_hash": "b10ccb5ffe7a0fcbbeb190ddceec3737e4b0af6525f2650c03bf3ca9b3a6c227", "ref_doc_id": "d3aec5d3-5d75-4830-90fd-a7ac97c2f698"}, "344844a4-ac2d-4848-ba1f-ad29e005d00b": {"doc_hash": "497058a5ebe59d7dda39bb234f0c750152b94343008a6cf68b52fd0a7efef966", "ref_doc_id": "d3aec5d3-5d75-4830-90fd-a7ac97c2f698"}, "dac45c93-a982-4297-a161-7ffde6437e8d": {"doc_hash": "0ad51989327aaffdcd08fff8cdf3f08a9c462ef51036da0d7add49f92369581d", "ref_doc_id": "1e556286-3fcd-4921-a794-873084fd4807"}, "f81b47a7-9715-484a-b708-285119d6544b": {"doc_hash": "ce393d2647de4fae037b22723501b8d781eb2d573fdc6e5dffb77dfad98fb0fc", "ref_doc_id": "1e556286-3fcd-4921-a794-873084fd4807"}, "91640ef9-5917-42ca-9b8c-d7b1f107157e": {"doc_hash": "b02350bfe6c45b69b3a131a3abad49bb4939cb99fbd1171563b34fdc58f497a8", "ref_doc_id": "1e556286-3fcd-4921-a794-873084fd4807"}, "96acc44e-3d51-402d-ae65-1743756780a8": {"doc_hash": "5dac32248638e56398e86e5b7d048ad1e61c3c79cfc7d22d0148fc949df4ec28", "ref_doc_id": "d34d3d5f-b1d2-41f9-820e-9c007640f7a4"}, "3448e5cd-a0d7-4de7-a954-26dfef75c13a": {"doc_hash": "fc32ffd267c526b51c3029b1e922ae61542ad2be8f5180be849a68064fe98866", "ref_doc_id": "d34d3d5f-b1d2-41f9-820e-9c007640f7a4"}, "562bcee7-4979-4141-af4b-061ac126fe93": {"doc_hash": "0489052b52fa33bd0db894bc6c9e247513e45a038aac0cfa9aac31e8f776c6c5", "ref_doc_id": "d34d3d5f-b1d2-41f9-820e-9c007640f7a4"}, "873df3c6-ad2d-4813-abc2-aae7e5f93997": {"doc_hash": "d6be34e9ad3eb67a3b7e072de332883654f17b77f4e57e8eefb0dde7e772ab4f", "ref_doc_id": "c6cc38c7-6e41-4e35-a796-7a6352bcecb6"}, "353ddba9-89a1-4894-8fed-be6422271f9d": {"doc_hash": "0536e4bbeb5c54569b02cc47be4451ab9a0649ab91819590d9cf33668611df12", "ref_doc_id": "c6cc38c7-6e41-4e35-a796-7a6352bcecb6"}}, "docstore/data": {"b25babec-d753-464f-9c63-da898433b600": {"__data__": {"id_": "b25babec-d753-464f-9c63-da898433b600", "embedding": null, "metadata": {"page_label": "1", "file_name": "Attention Is ALl You Need.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\Attention Is ALl You Need.pdf", "file_type": "application/pdf", "file_size": 569417, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6f5ac2d6-458e-46bd-a16b-99e79c1169f0", "node_type": "4", "metadata": {"page_label": "1", "file_name": "Attention Is ALl You Need.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\Attention Is ALl You Need.pdf", "file_type": "application/pdf", "file_size": 569417, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "eaa9b8d1c5ee85a48711fb889b39edf62c2aae1bcd0b3d39bda8b9b15e5eadea", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "37ee8168-10b0-43c1-8191-cf2875ce6c9e", "node_type": "1", "metadata": {}, "hash": "098801c6a80cf60576c3afb94950e3e5824382cd0c29027a300c0c796210604f", "class_name": "RelatedNodeInfo"}}, "text": "Attention Is All You Need\nAshish Vaswani\u2217\nGoogle Brain\navaswani@google.comNoam Shazeer\u2217\nGoogle Brain\nnoam@google.comNiki Parmar\u2217\nGoogle Research\nnikip@google.comJakob Uszkoreit\u2217\nGoogle Research\nusz@google.com\nLlion Jones\u2217\nGoogle Research\nllion@google.comAidan N. Gomez\u2217\u2020\nUniversity of Toronto\naidan@cs.toronto.edu\u0141ukasz Kaiser\u2217\nGoogle Brain\nlukaszkaiser@google.com\nIllia Polosukhin\u2217\u2021\nillia.polosukhin@gmail.com\nAbstract\nThe dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks that include an encoder and a decoder. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to\nbe superior in quality while being more parallelizable and requiring signi\ufb01cantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German translation task, improving over the existing best results, including\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\nbest models from the literature.\n1 Introduction\nRecurrent neural networks, long short-term memory [ 12] and gated recurrent [ 7] neural networks\nin particular, have been \ufb01rmly established as state of the art approaches in sequence modeling and\ntransduction problems such as language modeling and machine translation [ 29,2,5]. Numerous\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\narchitectures [31, 21, 13].\n\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the \ufb01rst Transformer models and\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\nattention and the parameter-free position representation and became the other person involved in nearly every\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\nef\ufb01cient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\nour research.\n\u2020Work performed while at Google Brain.\n\u2021Work performed while at Google Research.\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.", "start_char_idx": 0, "end_char_idx": 2903, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "37ee8168-10b0-43c1-8191-cf2875ce6c9e": {"__data__": {"id_": "37ee8168-10b0-43c1-8191-cf2875ce6c9e", "embedding": null, "metadata": {"page_label": "2", "file_name": "Attention Is ALl You Need.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\Attention Is ALl You Need.pdf", "file_type": "application/pdf", "file_size": 569417, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "08df43f0-66b4-4f7b-a2a4-867e7ceed120", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Attention Is ALl You Need.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\Attention Is ALl You Need.pdf", "file_type": "application/pdf", "file_size": 569417, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "89ea2e081c2766fd50e9be0737542a409527b4f3ebad9b794ab3326376771245", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b25babec-d753-464f-9c63-da898433b600", "node_type": "1", "metadata": {"page_label": "1", "file_name": "Attention Is ALl You Need.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\Attention Is ALl You Need.pdf", "file_type": "application/pdf", "file_size": 569417, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "eaa9b8d1c5ee85a48711fb889b39edf62c2aae1bcd0b3d39bda8b9b15e5eadea", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "41e021e8-4f27-40b0-a43b-edcd006fc13c", "node_type": "1", "metadata": {}, "hash": "094b35af0786c659a4e69f19dbfaa4704d8fb4169db52774a6a5b3aa7e4e07ae", "class_name": "RelatedNodeInfo"}}, "text": "Recurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstatesht, as a function of the previous hidden state ht\u22121and the input for position t. This inherently\nsequential nature precludes parallelization within training examples, which becomes critical at longer\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\nsigni\ufb01cant improvements in computational ef\ufb01ciency through factorization tricks [ 18] and conditional\ncomputation [ 26], while also improving model performance in case of the latter. The fundamental\nconstraint of sequential computation, however, remains.\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [ 2,16]. In all but a few cases [ 22], however, such attention mechanisms\nare used in conjunction with a recurrent network.\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\nThe Transformer allows for signi\ufb01cantly more parallelization and can reach a new state of the art in\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\n2 Background\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n[20], ByteNet [ 15] and ConvS2S [ 8], all of which use convolutional neural networks as basic building\nblock, computing hidden representations in parallel for all input and output positions. In these models,\nthe number of operations required to relate signals from two arbitrary input or output positions grows\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\nit more dif\ufb01cult to learn dependencies between distant positions [ 11]. In the Transformer this is\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribed in section 3.2.\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\ntextual entailment and learning task-independent sentence representations [4, 22, 23, 19].\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\naligned recurrence and have been shown to perform well on simple-language question answering and\nlanguage modeling tasks [28].\nTo the best of our knowledge, however, the Transformer is the \ufb01rst transduction model relying\nentirely on self-attention to compute representations of its input and output without using sequence-\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\nself-attention and discuss its advantages over models such as [14, 15] and [8].\n3 Model Architecture\nMost competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,29].\nHere, the encoder maps an input sequence of symbol representations (x1,...,x n)to a sequence\nof continuous representations z= (z1,...,z n). Given z, the decoder then generates an output\nsequence (y1,...,y m)of symbols one element at a time. At each step the model is auto-regressive\n[9], consuming the previously generated symbols as additional input when generating the next.\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\nrespectively.\n3.1 Encoder and Decoder Stacks\nEncoder: The encoder is composed of a stack of N= 6 identical layers. Each layer has two\nsub-layers. The \ufb01rst is a multi-head self-attention mechanism, and the second is a simple, position-\n2", "start_char_idx": 0, "end_char_idx": 4249, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "41e021e8-4f27-40b0-a43b-edcd006fc13c": {"__data__": {"id_": "41e021e8-4f27-40b0-a43b-edcd006fc13c", "embedding": null, "metadata": {"page_label": "3", "file_name": "Attention Is ALl You Need.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\Attention Is ALl You Need.pdf", "file_type": "application/pdf", "file_size": 569417, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "07f99e2a-8a29-4863-9dd7-8a6fc6669058", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Attention Is ALl You Need.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\Attention Is ALl You Need.pdf", "file_type": "application/pdf", "file_size": 569417, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "f85dc7f3b9a99fff4634fd76bd4ba4163e5731f95acf4e5e57dd7a5de09cfb17", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "37ee8168-10b0-43c1-8191-cf2875ce6c9e", "node_type": "1", "metadata": {"page_label": "2", "file_name": "Attention Is ALl You Need.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\Attention Is ALl You Need.pdf", "file_type": "application/pdf", "file_size": 569417, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "89ea2e081c2766fd50e9be0737542a409527b4f3ebad9b794ab3326376771245", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "94eef090-f7a4-42a4-ae6e-6e296fe45603", "node_type": "1", "metadata": {}, "hash": "7380b0c1f827eaab574a7aab294f2fba30575a8b1d4a7caf2c2c34ae4ae598c8", "class_name": "RelatedNodeInfo"}}, "text": "Figure 1: The Transformer - model architecture.\nwise fully connected feed-forward network. We employ a residual connection [ 10] around each of\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\nLayerNorm( x+ Sublayer( x)), where Sublayer(x)is the function implemented by the sub-layer\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512 .\nDecoder: The decoder is also composed of a stack of N= 6identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\npredictions for position ican depend only on the known outputs at positions less than i.\n3.2 Attention\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\nof the values, where the weight assigned to each value is computed by a compatibility function of the\nquery with the corresponding key.\n3.2.1 Scaled Dot-Product Attention\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\n3", "start_char_idx": 0, "end_char_idx": 1755, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "94eef090-f7a4-42a4-ae6e-6e296fe45603": {"__data__": {"id_": "94eef090-f7a4-42a4-ae6e-6e296fe45603", "embedding": null, "metadata": {"page_label": "4", "file_name": "Attention Is ALl You Need.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\Attention Is ALl You Need.pdf", "file_type": "application/pdf", "file_size": 569417, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0dd8e9fc-8037-4781-b79a-5ac2a43654a0", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Attention Is ALl You Need.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\Attention Is ALl You Need.pdf", "file_type": "application/pdf", "file_size": 569417, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "1cdd28e03d8e6e36936f60ec9f0106145722ae85c5391775f7ce7153ee2ebc47", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "41e021e8-4f27-40b0-a43b-edcd006fc13c", "node_type": "1", "metadata": {"page_label": "3", "file_name": "Attention Is ALl You Need.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\Attention Is ALl You Need.pdf", "file_type": "application/pdf", "file_size": 569417, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "f85dc7f3b9a99fff4634fd76bd4ba4163e5731f95acf4e5e57dd7a5de09cfb17", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f10575cd-5ac6-4b3f-867e-c7e443de4786", "node_type": "1", "metadata": {}, "hash": "b4286a0268e09ef21a09d31d0a4cb02782aa6130a55908a0a041449e114f0461", "class_name": "RelatedNodeInfo"}}, "text": "Scaled Dot-Product Attention\n Multi-Head Attention\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\nattention layers running in parallel.\nquery with all keys, divide each by\u221adk, and apply a softmax function to obtain the weights on the\nvalues.\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\ninto a matrix Q. The keys and values are also packed together into matrices KandV. We compute\nthe matrix of outputs as:\nAttention(Q,K,V ) = softmax(QKT\n\u221adk)V (1)\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\nof1\u221adk. Additive attention computes the compatibility function using a feed-forward network with\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\nmuch faster and more space-ef\ufb01cient in practice, since it can be implemented using highly optimized\nmatrix multiplication code.\nWhile for small values of dkthe two mechanisms perform similarly, additive attention outperforms\ndot product attention without scaling for larger values of dk[3]. We suspect that for large values of\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\nextremely small gradients4. To counteract this effect, we scale the dot products by1\u221adk.\n3.2.2 Multi-Head Attention\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\nwe found it bene\ufb01cial to linearly project the queries, keys and values htimes with different, learned\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\noutput values. These are concatenated and once again projected, resulting in the \ufb01nal values, as\ndepicted in Figure 2.\nMulti-head attention allows the model to jointly attend to information from different representation\nsubspaces at different positions. With a single attention head, averaging inhibits this.\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\nvariables with mean 0and variance 1. Then their dot product, q\u00b7k=\u2211dk\ni=1qiki, has mean 0and variance dk.\n4", "start_char_idx": 0, "end_char_idx": 2419, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f10575cd-5ac6-4b3f-867e-c7e443de4786": {"__data__": {"id_": "f10575cd-5ac6-4b3f-867e-c7e443de4786", "embedding": null, "metadata": {"page_label": "5", "file_name": "Attention Is ALl You Need.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\Attention Is ALl You Need.pdf", "file_type": "application/pdf", "file_size": 569417, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3a7bbee4-26b3-484a-ae14-16f0033a6b78", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Attention Is ALl You Need.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\Attention Is ALl You Need.pdf", "file_type": "application/pdf", "file_size": 569417, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "b64a2a8dba2cf085f8cfc535588e72e5b17f2de16a10541e24a9518e18eb43f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "94eef090-f7a4-42a4-ae6e-6e296fe45603", "node_type": "1", "metadata": {"page_label": "4", "file_name": "Attention Is ALl You Need.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\Attention Is ALl You Need.pdf", "file_type": "application/pdf", "file_size": 569417, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "1cdd28e03d8e6e36936f60ec9f0106145722ae85c5391775f7ce7153ee2ebc47", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aad02db8-130b-4882-b822-9b5604601742", "node_type": "1", "metadata": {}, "hash": "1196b1950b6cde02bc95be910798706cf07bf11a965650be986eeca76d4d113e", "class_name": "RelatedNodeInfo"}}, "text": "MultiHead( Q,K,V ) = Concat(head 1,...,head h)WO\nwhere head i= Attention( QWQ\ni,KWK\ni,VWV\ni)\nWhere the projections are parameter matrices WQ\ni\u2208Rdmodel\u00d7dk,WK\ni\u2208Rdmodel\u00d7dk,WV\ni\u2208Rdmodel\u00d7dv\nandWO\u2208Rhdv\u00d7dmodel.\nIn this work we employ h= 8 parallel attention layers, or heads. For each of these we use\ndk=dv=dmodel/h= 64 . Due to the reduced dimension of each head, the total computational cost\nis similar to that of single-head attention with full dimensionality.\n3.2.3 Applications of Attention in our Model\nThe Transformer uses multi-head attention in three different ways:\n\u2022In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\nand the memory keys and values come from the output of the encoder. This allows every\nposition in the decoder to attend over all positions in the input sequence. This mimics the\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n[31, 2, 8].\n\u2022The encoder contains self-attention layers. In a self-attention layer all of the keys, values\nand queries come from the same place, in this case, the output of the previous layer in the\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\nencoder.\n\u2022Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\nall positions in the decoder up to and including that position. We need to prevent leftward\ninformation \ufb02ow in the decoder to preserve the auto-regressive property. We implement this\ninside of scaled dot-product attention by masking out (setting to \u2212\u221e) all values in the input\nof the softmax which correspond to illegal connections. See Figure 2.\n3.3 Position-wise Feed-Forward Networks\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\nconnected feed-forward network, which is applied to each position separately and identically. This\nconsists of two linear transformations with a ReLU activation in between.\nFFN(x) = max(0,xW 1+b1)W2+b2 (2)\nWhile the linear transformations are the same across different positions, they use different parameters\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\nThe dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality\ndff= 2048 .\n3.4 Embeddings and Softmax\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\nlinear transformation, similar to [ 24]. In the embedding layers, we multiply those weights by\u221admodel.\n3.5 Positional Encoding\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\norder of the sequence, we must inject some information about the relative or absolute position of the\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\n5", "start_char_idx": 0, "end_char_idx": 3174, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "aad02db8-130b-4882-b822-9b5604601742": {"__data__": {"id_": "aad02db8-130b-4882-b822-9b5604601742", "embedding": null, "metadata": {"page_label": "6", "file_name": "Attention Is ALl You Need.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\Attention Is ALl You Need.pdf", "file_type": "application/pdf", "file_size": 569417, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6fc3e839-d2ba-4c04-b128-29a06c66ce37", "node_type": "4", "metadata": {"page_label": "6", "file_name": "Attention Is ALl You Need.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\Attention Is ALl You Need.pdf", "file_type": "application/pdf", "file_size": 569417, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "33538290e3d4ec2f759ce5af6db5ef7959cbfaac54c5db73f5ca4334eae6f0ed", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f10575cd-5ac6-4b3f-867e-c7e443de4786", "node_type": "1", "metadata": {"page_label": "5", "file_name": "Attention Is ALl You Need.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\Attention Is ALl You Need.pdf", "file_type": "application/pdf", "file_size": 569417, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "b64a2a8dba2cf085f8cfc535588e72e5b17f2de16a10541e24a9518e18eb43f2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "03dfa554-549e-4184-803b-ffd2aba6c2e9", "node_type": "1", "metadata": {}, "hash": "b3e6451af07fcdcbfef3de6e94cdee447d2255180fcef621b20c6680714f21f2", "class_name": "RelatedNodeInfo"}}, "text": "Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\nLayer Type Complexity per Layer Sequential Maximum Path Length\nOperations\nSelf-Attention O(n2\u00b7d) O(1) O(1)\nRecurrent O(n\u00b7d2) O(n) O(n)\nConvolutional O(k\u00b7n\u00b7d2)O(1) O(logk(n))\nSelf-Attention (restricted) O(r\u00b7n\u00b7d)O(1) O(n/r)\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\nlearned and \ufb01xed [8].\nIn this work, we use sine and cosine functions of different frequencies:\nPE(pos,2i)=sin(pos/100002i/d model)\nPE(pos,2i+1)=cos(pos/100002i/d model)\nwhereposis the position and iis the dimension. That is, each dimension of the positional encoding\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2\u03c0to10000\u00b72\u03c0. We\nchose this function because we hypothesized it would allow the model to easily learn to attend by\nrelative positions, since for any \ufb01xed offset k,PEpos+kcan be represented as a linear function of\nPEpos.\nWe also experimented with using learned positional embeddings [ 8] instead, and found that the two\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\nduring training.\n4 Why Self-Attention\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\ntional layers commonly used for mapping one variable-length sequence of symbol representations\n(x1,...,x n)to another sequence of equal length (z1,...,z n), withxi,zi\u2208Rd, such as a hidden\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\nconsider three desiderata.\nOne is the total computational complexity per layer. Another is the amount of computation that can\nbe parallelized, as measured by the minimum number of sequential operations required.\nThe third is the path length between long-range dependencies in the network. Learning long-range\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\nability to learn such dependencies is the length of the paths forward and backward signals have to\ntraverse in the network. The shorter these paths between any combination of positions in the input\nand output sequences, the easier it is to learn long-range dependencies [ 11]. Hence we also compare\nthe maximum path length between any two input and output positions in networks composed of the\ndifferent layer types.\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\nexecuted operations, whereas a recurrent layer requires O(n)sequential operations. In terms of\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\nlengthnis smaller than the representation dimensionality d, which is most often the case with\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\n[31] and byte-pair [ 25] representations. To improve computational performance for tasks involving\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\n6", "start_char_idx": 0, "end_char_idx": 3508, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "03dfa554-549e-4184-803b-ffd2aba6c2e9": {"__data__": {"id_": "03dfa554-549e-4184-803b-ffd2aba6c2e9", "embedding": null, "metadata": {"page_label": "7", "file_name": "Attention Is ALl You Need.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\Attention Is ALl You Need.pdf", "file_type": "application/pdf", "file_size": 569417, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "309371d3-c649-4d38-978d-129cda618a4b", "node_type": "4", "metadata": {"page_label": "7", "file_name": "Attention Is ALl You Need.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\Attention Is ALl You Need.pdf", "file_type": "application/pdf", "file_size": 569417, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "eba5f01e8457b39a21bb3785737aa791d4ce923e87a0e506b5c264c4ed3b78e2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "aad02db8-130b-4882-b822-9b5604601742", "node_type": "1", "metadata": {"page_label": "6", "file_name": "Attention Is ALl You Need.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\Attention Is ALl You Need.pdf", "file_type": "application/pdf", "file_size": 569417, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "33538290e3d4ec2f759ce5af6db5ef7959cbfaac54c5db73f5ca4334eae6f0ed", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d7f23b71-9f6b-4952-bc7e-1c816d0fb36d", "node_type": "1", "metadata": {}, "hash": "e5dcc5dfe04273c4e72e901a932ac018f78cb611162d620a6786585a6fc37490", "class_name": "RelatedNodeInfo"}}, "text": "the input sequence centered around the respective output position. This would increase the maximum\npath length to O(n/r). We plan to investigate this approach further in future work.\nA single convolutional layer with kernel width k<n does not connect all pairs of input and output\npositions. Doing so requires a stack of O(n/k)convolutional layers in the case of contiguous kernels,\norO(logk(n))in the case of dilated convolutions [ 15], increasing the length of the longest paths\nbetween any two positions in the network. Convolutional layers are generally more expensive than\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\nconsiderably, to O(k\u00b7n\u00b7d+n\u00b7d2). Even with k=n, however, the complexity of a separable\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\nthe approach we take in our model.\nAs side bene\ufb01t, self-attention could yield more interpretable models. We inspect attention distributions\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\nand semantic structure of the sentences.\n5 Training\nThis section describes the training regime for our models.\n5.1 Training Data and Batching\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\ntarget vocabulary of about 37000 tokens. For English-French, we used the signi\ufb01cantly larger WMT\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\nvocabulary [ 31]. Sentence pairs were batched together by approximate sequence length. Each training\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\ntarget tokens.\n5.2 Hardware and Schedule\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\n(3.5 days).\n5.3 Optimizer\nWe used the Adam optimizer [ 17] with\u03b21= 0.9,\u03b22= 0.98and\u03f5= 10\u22129. We varied the learning\nrate over the course of training, according to the formula:\nlrate =d\u22120.5\nmodel\u00b7min(step_num\u22120.5,step _num\u00b7warmup _steps\u22121.5) (3)\nThis corresponds to increasing the learning rate linearly for the \ufb01rst warmup _steps training steps,\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\nwarmup _steps = 4000 .\n5.4 Regularization\nWe employ three types of regularization during training:\nResidual Dropout We apply dropout [ 27] to the output of each sub-layer, before it is added to the\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\nPdrop= 0.1.\n7", "start_char_idx": 0, "end_char_idx": 3209, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d7f23b71-9f6b-4952-bc7e-1c816d0fb36d": {"__data__": {"id_": "d7f23b71-9f6b-4952-bc7e-1c816d0fb36d", "embedding": null, "metadata": {"page_label": "8", "file_name": "Attention Is ALl You Need.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\Attention Is ALl You Need.pdf", "file_type": "application/pdf", "file_size": 569417, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f83328ea-b7cd-4770-a9fc-0921614750d5", "node_type": "4", "metadata": {"page_label": "8", "file_name": "Attention Is ALl You Need.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\Attention Is ALl You Need.pdf", "file_type": "application/pdf", "file_size": 569417, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "842e4667416522f8acb402c60225279c957318a020092ee3d534fc79d689cf57", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "03dfa554-549e-4184-803b-ffd2aba6c2e9", "node_type": "1", "metadata": {"page_label": "7", "file_name": "Attention Is ALl You Need.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\Attention Is ALl You Need.pdf", "file_type": "application/pdf", "file_size": 569417, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "eba5f01e8457b39a21bb3785737aa791d4ce923e87a0e506b5c264c4ed3b78e2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bd70091a-7b9d-486a-8709-181ecf63a6b5", "node_type": "1", "metadata": {}, "hash": "1b640ee956d8ddec9dfcf725e71ce8de866c266a0f80a8ad1d0ec364bb43a117", "class_name": "RelatedNodeInfo"}}, "text": "Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\nModelBLEU Training Cost (FLOPs)\nEN-DE EN-FR EN-DE EN-FR\nByteNet [15] 23.75\nDeep-Att + PosUnk [32] 39.2 1.0\u00b71020\nGNMT + RL [31] 24.6 39.92 2.3\u00b710191.4\u00b71020\nConvS2S [8] 25.16 40.46 9.6\u00b710181.5\u00b71020\nMoE [26] 26.03 40.56 2.0\u00b710191.2\u00b71020\nDeep-Att + PosUnk Ensemble [32] 40.4 8.0\u00b71020\nGNMT + RL Ensemble [31] 26.30 41.16 1.8\u00b710201.1\u00b71021\nConvS2S Ensemble [8] 26.36 41.29 7.7\u00b710191.2\u00b71021\nTransformer (base model) 27.3 38.1 3.3\u00b71018\nTransformer (big) 28.4 41.0 2.3\u00b71019\nLabel Smoothing During training, we employed label smoothing of value \u03f5ls= 0.1[30]. This\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\n6 Results\n6.1 Machine Translation\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The con\ufb01guration of this model is\nlisted in the bottom line of Table 3. Training took 3.5days on 8P100 GPUs. Even our base model\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\nthe competitive models.\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\noutperforming all of the previously published single models, at less than 1/4the training cost of the\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\ndropout rate Pdrop= 0.1, instead of 0.3.\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\nused beam search with a beam size of 4and length penalty \u03b1= 0.6[31]. These hyperparameters\nwere chosen after experimentation on the development set. We set the maximum output length during\ninference to input length + 50, but terminate early when possible [31].\nTable 2 summarizes our results and compares our translation quality and training costs to other model\narchitectures from the literature. We estimate the number of \ufb02oating point operations used to train a\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\nsingle-precision \ufb02oating-point capacity of each GPU5.\n6.2 Model Variations\nTo evaluate the importance of different components of the Transformer, we varied our base model\nin different ways, measuring the change in performance on English-to-German translation on the\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\ncheckpoint averaging. We present these results in Table 3.\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\n8", "start_char_idx": 0, "end_char_idx": 3283, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bd70091a-7b9d-486a-8709-181ecf63a6b5": {"__data__": {"id_": "bd70091a-7b9d-486a-8709-181ecf63a6b5", "embedding": null, "metadata": {"page_label": "9", "file_name": "Attention Is ALl You Need.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\Attention Is ALl You Need.pdf", "file_type": "application/pdf", "file_size": 569417, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c2d98846-01f8-4c7b-93cc-58b59152ec6a", "node_type": "4", "metadata": {"page_label": "9", "file_name": "Attention Is ALl You Need.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\Attention Is ALl You Need.pdf", "file_type": "application/pdf", "file_size": 569417, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "abcbe9598aeb83453758b7f6c6c719fb6efa9b12fc7f3bbfa6e5e25f7c63dc83", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d7f23b71-9f6b-4952-bc7e-1c816d0fb36d", "node_type": "1", "metadata": {"page_label": "8", "file_name": "Attention Is ALl You Need.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\Attention Is ALl You Need.pdf", "file_type": "application/pdf", "file_size": 569417, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "842e4667416522f8acb402c60225279c957318a020092ee3d534fc79d689cf57", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c649e544-0987-4c6c-bc36-a03c5a599d32", "node_type": "1", "metadata": {}, "hash": "8816300106d358c5b0ac5d20d8345eff6b84ba1db8cce425e66c736e48075855", "class_name": "RelatedNodeInfo"}}, "text": "Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\nper-word perplexities.\nN d modeldffh d kdvPdrop\u03f5lstrain PPL BLEU params\nsteps (dev) (dev)\u00d7106\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\n(A)1 512 512 5.29 24.9\n4 128 128 5.00 25.5\n16 32 32 4.91 25.8\n32 16 16 5.01 25.4\n(B)16 5.16 25.1 58\n32 5.01 25.4 60\n(C)2 6.11 23.7 36\n4 5.19 25.3 50\n8 4.88 25.5 80\n256 32 32 5.75 24.5 28\n1024 128 128 4.66 26.0 168\n1024 5.12 25.4 53\n4096 4.75 26.2 90\n(D)0.0 5.77 24.6\n0.2 4.95 25.5\n0.0 4.67 25.3\n0.2 5.47 25.7\n(E) positional embedding instead of sinusoids 4.92 25.7\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\nfunction than dot product may be bene\ufb01cial. We further observe in rows (C) and (D) that, as expected,\nbigger models are better, and dropout is very helpful in avoiding over-\ufb01tting. In row (E) we replace our\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\nresults to the base model.\n7 Conclusion\nIn this work, we presented the Transformer, the \ufb01rst sequence transduction model based entirely on\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\nmulti-headed self-attention.\nFor translation tasks, the Transformer can be trained signi\ufb01cantly faster than architectures based\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\nmodel outperforms even all previously reported ensembles.\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\nplan to extend the Transformer to problems involving input and output modalities other than text and\nto investigate local, restricted attention mechanisms to ef\ufb01ciently handle large inputs and outputs\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\nThe code we used to train and evaluate our models is available at https://github.com/\ntensorflow/tensor2tensor .\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\ncomments, corrections and inspiration.\n9", "start_char_idx": 0, "end_char_idx": 2609, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c649e544-0987-4c6c-bc36-a03c5a599d32": {"__data__": {"id_": "c649e544-0987-4c6c-bc36-a03c5a599d32", "embedding": null, "metadata": {"page_label": "10", "file_name": "Attention Is ALl You Need.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\Attention Is ALl You Need.pdf", "file_type": "application/pdf", "file_size": 569417, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "097e20b6-c825-47ac-bde3-e819ada0d7a6", "node_type": "4", "metadata": {"page_label": "10", "file_name": "Attention Is ALl You Need.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\Attention Is ALl You Need.pdf", "file_type": "application/pdf", "file_size": 569417, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "0552fc6232e3ee7a20463d5c954df89c685ff9cb1f46a33e60e021edd783e510", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bd70091a-7b9d-486a-8709-181ecf63a6b5", "node_type": "1", "metadata": {"page_label": "9", "file_name": "Attention Is ALl You Need.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\Attention Is ALl You Need.pdf", "file_type": "application/pdf", "file_size": 569417, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "abcbe9598aeb83453758b7f6c6c719fb6efa9b12fc7f3bbfa6e5e25f7c63dc83", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8a3726c7-9dbe-4ce1-b409-efeca9546a71", "node_type": "1", "metadata": {}, "hash": "457af481a1ac171746ace145f1443dd8f7e4bb5aac609990bfc0d370133d3767", "class_name": "RelatedNodeInfo"}}, "text": "References\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450 , 2016.\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\nreading. arXiv preprint arXiv:1601.06733 , 2016.\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\nmachine translation. CoRR , abs/1406.1078, 2014.\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\npreprint arXiv:1610.02357 , 2016.\n[7]Junyoung Chung, \u00c7aglar G\u00fcl\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\narXiv:1308.0850 , 2013.\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition , pages 770\u2013778, 2016.\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\u00fcrgen Schmidhuber. Gradient \ufb02ow in\nrecurrent nets: the dif\ufb01culty of learning long-term dependencies, 2001.\n[12] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation ,\n9(8):1735\u20131780, 1997.\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\n[14] \u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\non Learning Representations (ICLR) , 2016.\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\n2017.\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\nInInternational Conference on Learning Representations , 2017.\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\narXiv:1703.10722 , 2017.\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding.", "start_char_idx": 0, "end_char_idx": 2918, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8a3726c7-9dbe-4ce1-b409-efeca9546a71": {"__data__": {"id_": "8a3726c7-9dbe-4ce1-b409-efeca9546a71", "embedding": null, "metadata": {"page_label": "10", "file_name": "Attention Is ALl You Need.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\Attention Is ALl You Need.pdf", "file_type": "application/pdf", "file_size": 569417, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "097e20b6-c825-47ac-bde3-e819ada0d7a6", "node_type": "4", "metadata": {"page_label": "10", "file_name": "Attention Is ALl You Need.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\Attention Is ALl You Need.pdf", "file_type": "application/pdf", "file_size": 569417, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "0552fc6232e3ee7a20463d5c954df89c685ff9cb1f46a33e60e021edd783e510", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c649e544-0987-4c6c-bc36-a03c5a599d32", "node_type": "1", "metadata": {"page_label": "10", "file_name": "Attention Is ALl You Need.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\Attention Is ALl You Need.pdf", "file_type": "application/pdf", "file_size": 569417, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "c9ed4f1ad23d38a87451b31a5fa96604d894f62cd980218d21f040d921be96be", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "49a48f98-d8c5-4e15-854f-8d2fee6d5149", "node_type": "1", "metadata": {}, "hash": "0e889494686fd7937a6074bcaa060f0950356e233044d0557c36114a392b2b5f", "class_name": "RelatedNodeInfo"}}, "text": "Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\n2017.\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\nInInternational Conference on Learning Representations , 2017.\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\narXiv:1703.10722 , 2017.\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\narXiv:1703.03130 , 2017.\n[20] Samy Bengio \u0141ukasz Kaiser. Can active memory replace attention? In Advances in Neural\nInformation Processing Systems, (NIPS) , 2016.\n10", "start_char_idx": 2302, "end_char_idx": 3099, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "49a48f98-d8c5-4e15-854f-8d2fee6d5149": {"__data__": {"id_": "49a48f98-d8c5-4e15-854f-8d2fee6d5149", "embedding": null, "metadata": {"page_label": "11", "file_name": "Attention Is ALl You Need.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\Attention Is ALl You Need.pdf", "file_type": "application/pdf", "file_size": 569417, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b739cd9f-479e-40af-b5f9-ca1130fd22ef", "node_type": "4", "metadata": {"page_label": "11", "file_name": "Attention Is ALl You Need.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\Attention Is ALl You Need.pdf", "file_type": "application/pdf", "file_size": 569417, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "0eeaa97bcf95d8884da2dff5ad6585269c61380cbc1c377dc44c5505b77e6807", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8a3726c7-9dbe-4ce1-b409-efeca9546a71", "node_type": "1", "metadata": {"page_label": "10", "file_name": "Attention Is ALl You Need.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\Attention Is ALl You Need.pdf", "file_type": "application/pdf", "file_size": 569417, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "00d40a1ff615e5628229b61a95d874f4e86690a630cc08bd9ee5995647c74da9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9e24a353-fe5c-40b7-9561-0ee1cad5acc7", "node_type": "1", "metadata": {}, "hash": "47b0381f273ece81c8175edd125b5900257f0c1113655594e490ee8f86c97df6", "class_name": "RelatedNodeInfo"}}, "text": "[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\n[22] Ankur Parikh, Oscar T\u00e4ckstr\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\nmodel. In Empirical Methods in Natural Language Processing , 2016.\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\n[24] O\ufb01r Press and Lior Wolf. Using the output embedding to improve language models. arXiv\npreprint arXiv:1608.05859 , 2016.\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\nnov. Dropout: a simple way to prevent neural networks from over\ufb01tting. Journal of Machine\nLearning Research , 15(1):1929\u20131958, 2014.\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\nAdvances in Neural Information Processing Systems 28 , pages 2440\u20132448. Curran Associates,\nInc., 2015.\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\nnetworks. In Advances in Neural Information Processing Systems , pages 3104\u20133112, 2014.\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\u2019s neural machine\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\narXiv:1609.08144 , 2016.\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\n11", "start_char_idx": 0, "end_char_idx": 2337, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9e24a353-fe5c-40b7-9561-0ee1cad5acc7": {"__data__": {"id_": "9e24a353-fe5c-40b7-9561-0ee1cad5acc7", "embedding": null, "metadata": {"page_label": "1", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fb93cfb2-c772-4087-b80f-6c33fa1ac276", "node_type": "4", "metadata": {"page_label": "1", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "15cf079524395e43197b32e3a762070bb48716cf6b3ff9d5f695ecd1cbfb6f54", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "49a48f98-d8c5-4e15-854f-8d2fee6d5149", "node_type": "1", "metadata": {"page_label": "11", "file_name": "Attention Is ALl You Need.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\Attention Is ALl You Need.pdf", "file_type": "application/pdf", "file_size": 569417, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "0eeaa97bcf95d8884da2dff5ad6585269c61380cbc1c377dc44c5505b77e6807", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b98890a0-fc8f-4232-a2e8-d56e191d21a8", "node_type": "1", "metadata": {}, "hash": "0730b6384d8410557ceb0cb20ce178f4788467dca3fcf03632c6911e765da22e", "class_name": "RelatedNodeInfo"}}, "text": "Citation: Hussain, M. YOLO-v1 to\nYOLO-v8, the Rise of YOLO and Its\nComplementary Nature toward\nDigital Manufacturing and Industrial\nDefect Detection. Machines 2023 ,11,\n677. https://doi.org/10.3390/\nmachines11070677\nAcademic Editor: Sang Do Noh\nReceived: 30 May 2023\nRevised: 15 June 2023\nAccepted: 21 June 2023\nPublished: 23 June 2023\nCopyright: \u00a9 2023 by the author.\nLicensee MDPI, Basel, Switzerland.\nThis article is an open access article\ndistributed under the terms and\nconditions of the Creative Commons\nAttribution (CC BY) license (https://\ncreativecommons.org/licenses/by/\n4.0/).\nmachines\nReview\nYOLO-v1 to YOLO-v8, the Rise of YOLO and Its\nComplementary Nature toward Digital Manufacturing and\nIndustrial Defect Detection\nMuhammad Hussain\nDepartment of Computer Science, School of Computing and Engineering, University of Hudders\ufb01eld,\nQueensgate, Hudders\ufb01eld HD1 3DH, UK; m.hussain@hud.ac.uk\nAbstract: Since its inception in 2015, the YOLO (You Only Look Once) variant of object detectors has\nrapidly grown, with the latest release of YOLO-v8 in January 2023. YOLO variants are underpinned\nby the principle of real-time and high-classi\ufb01cation performance, based on limited but ef\ufb01cient\ncomputational parameters. This principle has been found within the DNA of all YOLO variants\nwith increasing intensity, as the variants evolve addressing the requirements of automated quality\ninspection within the industrial surface defect detection domain, such as the need for fast detection,\nhigh accuracy, and deployment onto constrained edge devices. This paper is the \ufb01rst to provide an\nin-depth review of the YOLO evolution from the original YOLO to the recent release (YOLO-v8) from\nthe perspective of industrial manufacturing. The review explores the key architectural advancements\nproposed at each iteration, followed by examples of industrial deployment for surface defect detection\nendorsing its compatibility with industrial requirements.\nKeywords: industrial defect detection; object detection; smart manufacturing; quality inspection\n1. Introduction\nHumans via the visual cortex, a primary cortical region of the brain responsible for\nprocessing visual information [ 1], are able to observe, recognize [ 2], and differentiate\nbetween objects instantaneously [ 3]. Studying the inner workings of the visual cortex and\nthe brain in general has paved the way for arti\ufb01cial neural networks (ANNs) [ 4] along\nwith a myriad of computational architectures residing under the deep learning umbrella.\nIn the last decade, owing to rapid and revolutionary advancements in the \ufb01eld of deep\nlearning [5], researchers have exerted their efforts on providing ef\ufb01cient simulation of the\nhuman visual system to computers, i.e., enabling computers to detect objects of interest\nwithin static images and video [ 6], a \ufb01eld known as computer vision (CV) [ 7]. CV is\na prevalent research area for deep learning researchers and practitioners in the present\ndecade. It is composed of sub\ufb01elds consisting of image classi\ufb01cation [ 8], object detection [ 9],\nand object segmentation [ 10]. All three \ufb01elds share a common architectural theme, namely,\nmanipulation of convolutional neural networks (CNNs) [ 11]. CNNs are accepted as the de\nfacto when dealing with image data. In comparison with conventional image processing\nand arti\ufb01cial defection methods, CNNs utilize multiple convolutional layers coupled with\naggregation, i.e., pooling structures aiming to unearth deep semantic features hidden away\nwithin the pixels of the image [12].\nArti\ufb01cial intelligence (AI) has found opportunities in industries across the spectrum\nfrom renewable energy [ 13,14] and security to healthcare [ 15] and the education sector.\nHowever, one industry that is poised for signi\ufb01cant automation through CV is the manu-\nfacturing industry. Quality inspection (QI) is an integral part of any manufacturing domain\nproviding integrity and con\ufb01dence to the clients on the quality of the manufactured prod-\nucts [ 16].", "start_char_idx": 0, "end_char_idx": 3979, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b98890a0-fc8f-4232-a2e8-d56e191d21a8": {"__data__": {"id_": "b98890a0-fc8f-4232-a2e8-d56e191d21a8", "embedding": null, "metadata": {"page_label": "1", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fb93cfb2-c772-4087-b80f-6c33fa1ac276", "node_type": "4", "metadata": {"page_label": "1", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "15cf079524395e43197b32e3a762070bb48716cf6b3ff9d5f695ecd1cbfb6f54", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9e24a353-fe5c-40b7-9561-0ee1cad5acc7", "node_type": "1", "metadata": {"page_label": "1", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "ad2d7e724d0738da2384fb6797abee407b8d19b8e04b658c6c94f5c1457f2563", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0d5715c6-9a1a-4a43-b057-0a0c637f1e0b", "node_type": "1", "metadata": {}, "hash": "b33442124f9b30a0cecce4ae11109a256d9fc1de6c66399d1087144d8a232f5d", "class_name": "RelatedNodeInfo"}}, "text": "CNNs are accepted as the de\nfacto when dealing with image data. In comparison with conventional image processing\nand arti\ufb01cial defection methods, CNNs utilize multiple convolutional layers coupled with\naggregation, i.e., pooling structures aiming to unearth deep semantic features hidden away\nwithin the pixels of the image [12].\nArti\ufb01cial intelligence (AI) has found opportunities in industries across the spectrum\nfrom renewable energy [ 13,14] and security to healthcare [ 15] and the education sector.\nHowever, one industry that is poised for signi\ufb01cant automation through CV is the manu-\nfacturing industry. Quality inspection (QI) is an integral part of any manufacturing domain\nproviding integrity and con\ufb01dence to the clients on the quality of the manufactured prod-\nucts [ 16]. Manufacturing has wide scope for automation; however, when dealing with\nsurface inspection [ 17], defects can take sophisticated forms [ 18], making human-based\nMachines 2023 ,11, 677. https://doi.org/10.3390/machines11070677 https://www.mdpi.com/journal/machines", "start_char_idx": 3193, "end_char_idx": 4243, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0d5715c6-9a1a-4a43-b057-0a0c637f1e0b": {"__data__": {"id_": "0d5715c6-9a1a-4a43-b057-0a0c637f1e0b", "embedding": null, "metadata": {"page_label": "2", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ba3bbb6e-e313-48ed-8ecb-22b67481b822", "node_type": "4", "metadata": {"page_label": "2", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "3f251563fc9801c32ba1c6547a471582f8aa9991cbb1678dd0d8fe7e56b9e249", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b98890a0-fc8f-4232-a2e8-d56e191d21a8", "node_type": "1", "metadata": {"page_label": "1", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "8ffaff6a9f222a403734a31d3c493f7ab118fe1247260fe67e403252edcc4b1f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c4ae4461-f176-47ec-a42e-2b0342905e08", "node_type": "1", "metadata": {}, "hash": "d284d32ea4ac2c29106f1ac4d7f1277d1c077f69660bb7155bc2ca6b1f0f38ae", "class_name": "RelatedNodeInfo"}}, "text": "Machines 2023 ,11, 677 2 of 25\nquality inspection a cumbersome task with manifold inef\ufb01ciencies linked to human bias,\nfatigue, cost, and downtime [ 19]. These inef\ufb01ciencies provide an opportunity for CV-based\nsolutions to present automated quality inspection that can be integrated within existing\nsurface defect inspection processes, increasing ef\ufb01ciency whilst overcoming bottlenecks\npresented via conventional inspection methodologies [20].\nHowever, for success, CV-based architectures must conform to a stringent set of\ndeployment requirements that can vary from one manufacturing sector to another [ 21]. In\nthe majority of applications, the focus is not only on the determination of the defect, but also\non multiple defects along with the locality details of each [ 22]. Therefore, object detection\nis preferred over image classi\ufb01cation since the latter only focuses on determination of\nobject within the image without providing any locality information. Architectures within\nthe object detection domain can be classi\ufb01ed into single-stage or two-stage detectors [ 23].\nTwo-stage detectors split the detection process into two stages: Feature extraction/proposal\nfollowed by regression and classi\ufb01cation for acquiring the output [ 24]. Although this can\nprovide high accuracy, it comes with a high computational demand making it inef\ufb01cient for\nreal-time deployment onto constrained edge devices. Single-stage detectors, on the other\nhand, merge the two processes into one, enabling the classi\ufb01cation and regression via a\nsingle pass, signi\ufb01cantly reduce the computational demand, and provide a more compelling\ncase for production-based deployment [ 25]. Although many single-stage detectors have\nbeen introduced, such as single shot detector (SSD) [ 26], deconvolutional single shot\ndetector (D-SSD) [ 27], and RetinaNet [ 28], the YOLO (You Only Look Once) [ 29] family of\narchitectures seems to be gaining high traction due to its high compatibility with industrial\nrequirements, such as accuracy, lightweight, and edge-friendly deployment conditions.\nThe last half-a-decade has been dominated by the introduction of YOLO variants, with the\nmost recent variant introduced in 2022 as YOLO-v8.\nTo the best of our knowledge, there is no cohesive review of the advancing YOLO\nvariants, benchmarking technical advancements, and their implications on industrial\ndeployment. This paper reviews the YOLO variants released to the present date, focusing\non presenting the key technical contributions of each YOLO iteration and its impact on key\nindustrial metrics required for deployment, such as accuracy, speed, and computational\nef\ufb01cacy. As a result, the aim is to provide researchers and practitioners with a better\nunderstanding of the inner workings of each variant, enabling them to select the most\nrelevant architecture based on their industrial requirements. Additionally, literature on\nthe deployment of YOLO architectures for various industrial surface defect detection\napplications is presented.\nThe subsequent structure of the review is as follows. The \ufb01rst section provides an\nintroduction to single- and two-stage detectors and the anatomy for single-stage object\ndetectors. Next, the evolution of YOLO variants is presented, detailing the key contributions\nfrom YOLO-v1 to YOLO-v8, followed by a review of the literature focused on YOLO-based\nimplementation of industrial surface defect detection. Finally, the discussion section\nfocuses on summarizing the reviewed literature, followed by extracted conclusions, future\ndirections, and challenges are presented.\nObject Detection\nCNNs can be categorized as convolution-based feed forward neural networks for\nclassi\ufb01cation purposes [ 30]. The input layer is followed by multiple convolutional layers\nto acquire an increased set of smaller-scale feature maps. These feature maps post further\nmanipulation are transformed into one-dimensional feature vectors before being used as\ninput to the fully connected layer(s). The process of feature extraction and feature map\nmanipulation is vital to the overall accuracy of the network; therefore, this can involve the\nstacking of multiple convolutional and pooling layers for richer feature maps. Popular\narchitectures for feature extraction include AlexNet [ 31], VGGNet [ 32], GoogleNet [ 33], and\nResNet [ 34]. AlexNet is proposed in 2012 and consists of \ufb01ve convolutional, three pooling,\nand three fully connected layers primarily utilized for image classi\ufb01cation tasks. VGGNet", "start_char_idx": 0, "end_char_idx": 4485, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c4ae4461-f176-47ec-a42e-2b0342905e08": {"__data__": {"id_": "c4ae4461-f176-47ec-a42e-2b0342905e08", "embedding": null, "metadata": {"page_label": "3", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "000ec4ee-73c7-489a-a0a0-4f2ddbe96279", "node_type": "4", "metadata": {"page_label": "3", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "c6a0a03b02c712643a4cc9e378454513881dcf9ba64723bc45266731806d45ea", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0d5715c6-9a1a-4a43-b057-0a0c637f1e0b", "node_type": "1", "metadata": {"page_label": "2", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "3f251563fc9801c32ba1c6547a471582f8aa9991cbb1678dd0d8fe7e56b9e249", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cd159237-020e-4168-9332-44b3a195357d", "node_type": "1", "metadata": {}, "hash": "3cc56f341b05f1f6509eb2094f8d562b0f850c792b7379e46549d8b4618b363a", "class_name": "RelatedNodeInfo"}}, "text": "Machines 2023 ,11, 677 3 of 25\nfocused on performance enhancement by increasing the internal depth of the architecture,\nintroducing several variants with increased layers, VGG-16/19. GoogleNet introduced the\ncascading concept by cascading multiple \u2018inception\u2019 modules, whilst ResNet introduced\nthe concept of skip-connections for preserving information and making it available from\nthe earlier to the later layers of the architecture.\nThe motive for an object detector is to infer whether the object(s) of interest are\nresiding in the image or present the frame of a video. If the object(s) of interest are\npresent, the detector returns the respective class and locality, i.e., location dimensions\nof the object(s). Object detection can be further divided into two sub-categories: Two-\nstage methods and one-stage methods as shown in Figure 1. The former initiates the\n\ufb01rst stage with the selection of numerous proposals, then in the second stage, performs\nprediction on the proposed regions. Examples of two-stage detectors include the famous\nR-CNN [ 35] variants, such as Fast R-CNN [ 36] and Faster R-CNN [ 37], boasting high\naccuracies but low computational ef\ufb01ciency. The latter transforms the task into a regression\nproblem, eliminating the need for an initial stage dedicated to selecting candidate regions;\ntherefore, the candidate selection and prediction is achieved in a single pass. As a result,\narchitectures falling into this category are computationally less demanding, generating\nhigher FPS and detection speed, but in general the accuracy tends to be inferior with respect\nto two-stage detectors.\nMachines 2023 , 11, x FOR PEER REVIEW 3 of 26 \n \n and ResNet [34]. AlexNet is prop osed in 2012 and consists of \ufb01ve convolutional, three \npooling, and three fully connected layers primarily utilized for image classi \ufb01cation tasks. \nVGGNet focused on performance enhancement by increasing the internal depth of the \narchitecture, introducing several variants with increased layers, VGG-16/19. GoogleNet \nintroduced the cascading concept by cascading multiple \u2018inception\u2019 modules, whilst Res-\nNet introduced the concept of skip-connections  for preserving information and making it \navailable from the earlier to the later layers of the architecture. \nThe motive for an object detector is to infe r whether the object(s) of interest are resid-\ning in the image or present the frame of a video. If the object(s) of interest are present, the \ndetector returns the respective class and locality , i.e., location dimensions of the object(s). \nObject detection can be further divided into two sub-categories: Two-stage methods and one-stage methods as shown in Figure 1. The former initiates the \ufb01rst stage with the se-\nlection of numerous proposals, then in the second stage, performs prediction on the pro-\nposed regions. Examples of two-stage detect ors include the famous R-CNN [35] variants, \nsuch as Fast R-CNN [36] and Faster R-CNN [37], boasting high accuracies but low com-\nputational e \ufb03ciency. The la tter transforms the task into a regression problem, eliminating \nthe need for an initial stage dedicated to selecting candidate regions; therefore, the candi-\ndate selection and prediction is achieved in a single pass. As a result, architectures falling \ninto this category are computationally less demanding, generating higher FPS and detec-\ntion speed, but in general the accuracy tends to be inferior with respect to two-stage de-\ntectors. \n \nFigure 1. Object detector anatomy. \n2. Original YOLO Algorithm \nYOLO was introduced to the computer vision community via a paper release in 2015 \nby Joseph Redmon et al. [29] titled \u2018You Only Look Once: Uni \ufb01ed, Real-Time Object De-\ntection\u2019. The paper reframed object detection, presenting it essentially as a single pass re-\ngression problem, initiating with image pixels and moving to bounding box and class \nprobabilities. The proposed approach based on the \u2018uni \ufb01ed\u2019 concept enabled the simulta-\nneous prediction of multiple bounding boxe s and class probabilities, improving both \nspeed and accuracy. \nSince its inception in 2016 until the present year (2023), the YOLO family has contin-\nued to evolve at a rapid pace.", "start_char_idx": 0, "end_char_idx": 4190, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cd159237-020e-4168-9332-44b3a195357d": {"__data__": {"id_": "cd159237-020e-4168-9332-44b3a195357d", "embedding": null, "metadata": {"page_label": "3", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "000ec4ee-73c7-489a-a0a0-4f2ddbe96279", "node_type": "4", "metadata": {"page_label": "3", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "c6a0a03b02c712643a4cc9e378454513881dcf9ba64723bc45266731806d45ea", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c4ae4461-f176-47ec-a42e-2b0342905e08", "node_type": "1", "metadata": {"page_label": "3", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "03e7f88c1df0c6a08ab4d5ab0ad395805cc8142fadef1577cb6260ce52c5db09", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9f01979a-5629-442b-b668-b0dd8fee2294", "node_type": "1", "metadata": {}, "hash": "921d8200b3a6c16649fbef8f33bf324445d59188fb29a9fc22e13574f5074a70", "class_name": "RelatedNodeInfo"}}, "text": "Figure 1. Object detector anatomy. \n2. Original YOLO Algorithm \nYOLO was introduced to the computer vision community via a paper release in 2015 \nby Joseph Redmon et al. [29] titled \u2018You Only Look Once: Uni \ufb01ed, Real-Time Object De-\ntection\u2019. The paper reframed object detection, presenting it essentially as a single pass re-\ngression problem, initiating with image pixels and moving to bounding box and class \nprobabilities. The proposed approach based on the \u2018uni \ufb01ed\u2019 concept enabled the simulta-\nneous prediction of multiple bounding boxe s and class probabilities, improving both \nspeed and accuracy. \nSince its inception in 2016 until the present year (2023), the YOLO family has contin-\nued to evolve at a rapid pace. Although the initial author (Joseph Redmon) halted further \nwork within the computer vision domain at YOLO-v3 [38], the e \ufb00ectiveness and potential \nof the core \u2018uni \ufb01ed\u2019 concept have been further developed by several authors, with the \nlatest addition to the YOLO family coming in  the form of YOLO-v8. Figure 2 presents the \nYOLO evolution timeline. \nFigure 1. Object detector anatomy.\n2. Original YOLO Algorithm\nYOLO was introduced to the computer vision community via a paper release in 2015 by\nJoseph Redmon et al. [ 29] titled \u2018You Only Look Once: Uni\ufb01ed, Real-Time Object Detection\u2019.\nThe paper reframed object detection, presenting it essentially as a single pass regression\nproblem, initiating with image pixels and moving to bounding box and class probabilities.\nThe proposed approach based on the \u2018uni\ufb01ed\u2019 concept enabled the simultaneous prediction\nof multiple bounding boxes and class probabilities, improving both speed and accuracy.\nSince its inception in 2016 until the present year (2023), the YOLO family has continued\nto evolve at a rapid pace. Although the initial author (Joseph Redmon) halted further work\nwithin the computer vision domain at YOLO-v3 [ 38], the effectiveness and potential of\nthe core \u2018uni\ufb01ed\u2019 concept have been further developed by several authors, with the latest\naddition to the YOLO family coming in the form of YOLO-v8. Figure 2 presents the YOLO\nevolution timeline.\n2.1. Original YOLO\nThe core principle proposed by YOLO-v1 was the imposing of a grid cell with dimen-\nsions of s\u00d7s onto the image. In the case of the center of the object of interest falling into one\nof the grid cells, that particular grid cell would be responsible for the detection of that object.\nThis permitted other cells to disregard that object in the case of multiple appearances.", "start_char_idx": 3465, "end_char_idx": 5989, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9f01979a-5629-442b-b668-b0dd8fee2294": {"__data__": {"id_": "9f01979a-5629-442b-b668-b0dd8fee2294", "embedding": null, "metadata": {"page_label": "4", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8218156a-6485-4c4f-87ff-d61a5c7713fb", "node_type": "4", "metadata": {"page_label": "4", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "74ac7a8e5301faac2f4d82fe34039c3f39dc221633cf8bf3aaaa87ddf414b72d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cd159237-020e-4168-9332-44b3a195357d", "node_type": "1", "metadata": {"page_label": "3", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "8cc8882b31bc1abf423df3b3e033102a8c301a8319ba2a22f5cf7a9bed8c8ffe", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4b831f59-ee10-4f29-a349-2384d00f78f4", "node_type": "1", "metadata": {}, "hash": "cacdde2b208cf204167a29afade1762bff3f7140f4ed52690855b4c9194eb871", "class_name": "RelatedNodeInfo"}}, "text": "Machines 2023 ,11, 677 4 of 25\nMachines 2023 , 11, x FOR PEER REVIEW 4 of 26 \n \n  \nFigure 2. YOLO evolution timeline. \n2.1. Original YOLO \nThe core principle proposed by YOLO-v1 wa s the imposing of a grid cell with di-\nmensions of s\u00d7s onto the image. In the case of  the center of the object of interest falling \ninto one of the grid cells, that particular grid cell would be responsible for the detection \nof that object. This permi tted other cells to disregard that object in the case of multiple \nappearances. \nFor implementation of object detection, each grid cell would predict B bounding \nboxes along with the dimensions and con \ufb01dence scores. The con \ufb01dence score was indic-\native of the absence or presence of an object within the bounding box. Therefore, the con-\n\ufb01dence score can be expressed as Equation (1): \n\ud835\udc50\ud835\udc5c\ud835\udc5b\ud835\udc53\ud835\udc56\ud835\udc51\ud835\udc52\ud835\udc5b\ud835\udc50\ud835\udc52 \ud835\udc60\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52 = \ud835\udc5d (\ud835\udc5c\ud835\udc4f\ud835\udc57\ud835\udc52\ud835\udc50\ud835\udc61 )\u2217\ud835\udc3c \ud835\udc5c \ud835\udc48\u0be3\u0be5\u0bd8\u0bd7\u0be7\u0be5\u0be8\u0be7\u0bdb  (1)\nwhere \ud835\udc5d(\ud835\udc5c\ud835\udc4f\ud835\udc57\ud835\udc52\ud835\udc50\ud835\udc61 ) signi \ufb01ed the probability of the object being present, with a range of 0\u20131 \nwith 0 indicating that the object is not present and \ud835\udc3c\ud835\udc5c\ud835\udc48\u0be3\u0be5\u0bd8\u0bd7\u0be7\u0be5\u0be8\u0be7\u0bdb represented the intersection-\nover-union with the predicted bounding box with respect to the ground truth bounding \nbox. \nEach bounding box consisted of \ufb01ve components ( x, y, w, h, and the  con\ufb01dence score) \nwith the \ufb01rst four components  corresponding to center coordinates ( x, y, width, and height ) \nof the respective bounding box as shown in Figure 3.  \nFigure 2. YOLO evolution timeline.\nFor implementation of object detection, each grid cell would predict Bbounding boxes\nalong with the dimensions and con\ufb01dence scores. The con\ufb01dence score was indicative of\nthe absence or presence of an object within the bounding box. Therefore, the con\ufb01dence\nscore can be expressed as Equation (1):\ncon f idence score =p(object )\u2217IoUtruth\npred(1)\nwhere p(object )signi\ufb01ed the probability of the object being present, with a range of 0\u20131 with\n0 indicating that the object is not present and IoUtruth\npredrepresented the intersection-over-\nunion with the predicted bounding box with respect to the ground truth bounding box.\nEach bounding box consisted of \ufb01ve components ( x,y,w,h,and the con\ufb01dence score)\nwith the \ufb01rst four components corresponding to center coordinates ( x,y,width, and height )\nof the respective bounding box as shown in Figure 3.\nMachines 2023 , 11, x FOR PEER REVIEW 5 of 26 \n \n  \nFigure 3. YOLO-v1 preliminary architecture. \nAs alluded to earlier, the input image is split into s \u00d7 s grid cells (default = 7 \u00d7 7), with \neach cell predicting B bounding boxes, each containing \ufb01ve parameters and sharing pre-\ndiction probabilities of classes ( C). Therefore, the parameter output would take the fol-\nlowing form, expressed in (2): \n\ud835\udc60\u00d7\ud835\udc60\u00d7 (5\u2217\ud835\udc35+\ud835\udc36 )  (2)\nConsidering the example of YOLO network wi th each cell boundi ng box prediction \nset to 2 and evaluating the benchmark COCO dataset consisting of 80 classes, the param-eter output would be give n as expressed in (3): \n7\u00d77\u00d7( 5\u22172+8 0 )   (3)\nThe fundamental motive of YOLO and object detection in general is the object detec-\ntion and localization via bounding boxes. Ther efore, two sets of bounding box vectors are \nrequired, i.e., vector y is the representative of ground truth and vector \ud835\udc66\u1236 is the predicted \nvector.", "start_char_idx": 0, "end_char_idx": 3216, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4b831f59-ee10-4f29-a349-2384d00f78f4": {"__data__": {"id_": "4b831f59-ee10-4f29-a349-2384d00f78f4", "embedding": null, "metadata": {"page_label": "4", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8218156a-6485-4c4f-87ff-d61a5c7713fb", "node_type": "4", "metadata": {"page_label": "4", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "74ac7a8e5301faac2f4d82fe34039c3f39dc221633cf8bf3aaaa87ddf414b72d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9f01979a-5629-442b-b668-b0dd8fee2294", "node_type": "1", "metadata": {"page_label": "4", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "f4faa2fc83b4ea67514051c086cb1197420cf30d23aa3f19f909d6c6fb5eb93a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c552f6bc-9894-40b9-b6ba-945c256f37c4", "node_type": "1", "metadata": {}, "hash": "c7ae6e4b77d23796754b05652bbe94b7f46e30cd95f16f00e3e53a821bd00c8d", "class_name": "RelatedNodeInfo"}}, "text": "Therefore, the parameter output would take the fol-\nlowing form, expressed in (2): \n\ud835\udc60\u00d7\ud835\udc60\u00d7 (5\u2217\ud835\udc35+\ud835\udc36 )  (2)\nConsidering the example of YOLO network wi th each cell boundi ng box prediction \nset to 2 and evaluating the benchmark COCO dataset consisting of 80 classes, the param-eter output would be give n as expressed in (3): \n7\u00d77\u00d7( 5\u22172+8 0 )   (3)\nThe fundamental motive of YOLO and object detection in general is the object detec-\ntion and localization via bounding boxes. Ther efore, two sets of bounding box vectors are \nrequired, i.e., vector y is the representative of ground truth and vector \ud835\udc66\u1236 is the predicted \nvector. To address multiple bounding boxes containing no object or the same object, \nYOLO opts for non-maximum suppression (NMS). By de \ufb01ning a threshold value for \nNMS, all overlapping predic ted bounding boxes with an IoU lower than the de \ufb01ned NMS \nvalue are eliminated. \nThe original YOLO based on the Darknet framework consisted of two sub-variants. \nThe \ufb01rst architecture comprised of 24 convolutional layers with the \ufb01nal layer providing \na connection into the \ufb01rst of the two fully connected layers. Whereas the \u2018Fast YOLO\u2019 var-\niant consisted of only nine co nvolutional layers hosting fewer \ufb01lters each. Inspired by the \ninception module in GoogleNet, a sequence of 1\u00d71   convolutional layers was imple-\nmented for reducing the resultant feature sp ace from the preceding layers. The prelimi-\nnary architecture for YOLO-v1 is presented in Figure 3. \nTo address the issue of multiple bounding boxes for the same object or with a con \ufb01-\ndence score of zero, i.e., no object, the authors decided to greatly penalize predictions from bounding boxes containing objects ( \ud835\udefe\n\u0bd6\u0be2\u0be2\u0be5\u0bd7 =5) and the lowest penalization for prediction \ncontaining no object ( \ud835\udefe\u0be1\u0be2\u0be2\u0bd5\u0bdd =0 . 5 ). The authors calculated the loss function by taking the \nsum of all bounding  box parameters ( x, y, width , height , con\ufb01dence score, and class prob-\nability). As a result, the \ufb01rst part of the equation computes the loss of the bounding box \nprediction with respect to the ground tr uth bounding box based on the coordinates \n\ud835\udc65\u0bd6\u0bd8\u0be1\u0be7\u0bd8\u0be5 , \ud835\udc66\u0bd6\u0bd8\u0be1\u0be7\u0bd8\u0be5 . \ud835\udd5d\u0bdc\u0bdd\u0be2\u0bd5\u0bdd is set as 1 in the case of the object residing within  \ud835\udc57\u0be7\u0bdb bounding box \nprediction in \ud835\udc56\u0be7\u0bdb cell; otherwise, it is set as 0. The selected, i.e., predicted bounding box \nwould be tasked with predicting an object wi th the greatest IoU, as expressed in (4): \n\ud835\udefe\u0bd6\u0be2\u0be2\u0be5\u0bd7 \u2211\u2211 \ud835\udd5d\u0bdc\u0bdd\u0be2\u0bd5\u0bdd[(\ud835\udc65\u0bdc\u2212\ud835\udc65 \u0c2a\u0ddd)\u0b36+(\ud835\udc66\u0bdc\u2212\ud835\udc66 \u0c2a\u0ddd)\u0b36]\u0bbb\n\u0bdd\u0b40\u0b34\u0bcc\u0c2e\n\u0bdc\u0b40\u0b34   (4)\nThe next component of the loss function computes the prediction error in width and \nheight of the bounding box, similar to the preceding compon ent. However, the scale of \nerror in the large boxes has lesser impact co mpared to the small boxes. The normalization \nof width and height between the range 0 and 1 indicates that their square roots increase \nFigure 3. YOLO-v1 preliminary architecture.", "start_char_idx": 2594, "end_char_idx": 5444, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c552f6bc-9894-40b9-b6ba-945c256f37c4": {"__data__": {"id_": "c552f6bc-9894-40b9-b6ba-945c256f37c4", "embedding": null, "metadata": {"page_label": "5", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6640492e-0115-4fb7-8c98-f82749b683e1", "node_type": "4", "metadata": {"page_label": "5", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "2b704c6c39816d523c2e550f8979ae9c2c96221b3902ad4756bb590bb981e815", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4b831f59-ee10-4f29-a349-2384d00f78f4", "node_type": "1", "metadata": {"page_label": "4", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "d9026cbe1a868ffb43cde4aa49c427e448748826a06b03c3de88d9b211f4acba", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fc148e0d-c2c6-487a-8682-d08f2c02284d", "node_type": "1", "metadata": {}, "hash": "97ffcb62db7418c7a2904574379b7d55801b95c92f3557ba959b225af5617d2d", "class_name": "RelatedNodeInfo"}}, "text": "Machines 2023 ,11, 677 5 of 25\nAs alluded to earlier, the input image is split into s \u00d7s grid cells (default = 7 \u00d77),\nwith each cell predicting Bbounding boxes, each containing \ufb01ve parameters and sharing\nprediction probabilities of classes ( C). Therefore, the parameter output would take the\nfollowing form, expressed in (2):\ns\u00d7s\u00d7(5\u2217B+C) (2)\nConsidering the example of YOLO network with each cell bounding box prediction\nset to 2 and evaluating the benchmark COCO dataset consisting of 80 classes, the parameter\noutput would be given as expressed in (3):\n7\u00d77\u00d7(5\u22172+80) (3)\nThe fundamental motive of YOLO and object detection in general is the object detection\nand localization via bounding boxes. Therefore, two sets of bounding box vectors are\nrequired, i.e., vector y is the representative of ground truth and vector.yis the predicted\nvector. To address multiple bounding boxes containing no object or the same object, YOLO\nopts for non-maximum suppression (NMS). By de\ufb01ning a threshold value for NMS, all\noverlapping predicted bounding boxes with an IoU lower than the de\ufb01ned NMS value\nare eliminated.\nThe original YOLO based on the Darknet framework consisted of two sub-variants.\nThe \ufb01rst architecture comprised of 24 convolutional layers with the \ufb01nal layer providing\na connection into the \ufb01rst of the two fully connected layers. Whereas the \u2018Fast YOLO\u2019\nvariant consisted of only nine convolutional layers hosting fewer \ufb01lters each. Inspired\nby the inception module in GoogleNet, a sequence of 1\u00d71convolutional layers was\nimplemented for reducing the resultant feature space from the preceding layers. The\npreliminary architecture for YOLO-v1 is presented in Figure 3.\nTo address the issue of multiple bounding boxes for the same object or with a con\ufb01-\ndence score of zero, i.e., no object, the authors decided to greatly penalize predictions from\nbounding boxes containing objects ( \u03b3coord =5) and the lowest penalization for prediction\ncontaining no object ( \u03b3noobj =0.5). The authors calculated the loss function by taking\nthe sum of all bounding box parameters ( x,y,width ,height , con\ufb01dence score, and class\nprobability). As a result, the \ufb01rst part of the equation computes the loss of the bounding box\nprediction with respect to the ground truth bounding box based on the coordinates xcenter ,\nycenter .\nMachines 2023 , 11, x FOR PEER REVIEW  5 of 26 \n \n  \nFigure 3. YOLO-v1 preliminary architecture.  \nAs alluded to earlier, the input image is split into s  \u00d7 s grid cells (default = 7  \u00d7 7), with \neach cell predicting B bounding boxes, each containing five parameters and sharing pre-\ndiction probabilities of classes ( C). Therefore,  the parameter output would take the fol-\nlowing form, expressed in (2): \n\ud835\udc60\u00d7\ud835\udc60\u00d7(5\u2217\ud835\udc35+\ud835\udc36)  (2) \nConsidering  the example of YOLO network with each cell bounding box prediction \nset to 2 and evaluating the benchmark COCO dataset consisting of 80 classes, the param-\neter output would be given as  expressed in  (3): \n7\u00d77\u00d7(5\u22172+80)  (3) \nThe fundamental motive of YOLO and object detection in general is the object detec-\ntion and localization via bounding boxes. Therefore , two sets of bounding box vectors are \nrequired , i.e., vector y is the representative of ground truth  and vector \ud835\udc66\u0307 is the predicted \nvector. To address multiple bounding  boxes containing no  object or the same object, \nYOLO opts for non -maximum suppression (NMS). By defining a threshold value for \nNMS, all overlapping predicted bounding boxes with an IoU lower than the defined NM S \nvalue are eliminated.  \nThe original YOLO based on the Dark net framework consisted of two sub -variants. \nThe first architecture comprised of 24 convolutional layers with the final layer providing \na connection  into the first of the two fully connected layers. Whereas the \u2018Fast YOLO \u2019 var-\niant consisted of only nine convolutional layers hosting fewer filters each.", "start_char_idx": 0, "end_char_idx": 3879, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fc148e0d-c2c6-487a-8682-d08f2c02284d": {"__data__": {"id_": "fc148e0d-c2c6-487a-8682-d08f2c02284d", "embedding": null, "metadata": {"page_label": "5", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6640492e-0115-4fb7-8c98-f82749b683e1", "node_type": "4", "metadata": {"page_label": "5", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "2b704c6c39816d523c2e550f8979ae9c2c96221b3902ad4756bb590bb981e815", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c552f6bc-9894-40b9-b6ba-945c256f37c4", "node_type": "1", "metadata": {"page_label": "5", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "c51eec2634795b7d0239da82872802952c1f08f5ff73c84c6239741b7ddd061b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d32811f7-f324-4379-a1e2-33e35a000b54", "node_type": "1", "metadata": {}, "hash": "868bfed535ddde6463e1964bef056b4891355f22a410fcec89fed7f8f4e0344b", "class_name": "RelatedNodeInfo"}}, "text": "Therefore , two sets of bounding box vectors are \nrequired , i.e., vector y is the representative of ground truth  and vector \ud835\udc66\u0307 is the predicted \nvector. To address multiple bounding  boxes containing no  object or the same object, \nYOLO opts for non -maximum suppression (NMS). By defining a threshold value for \nNMS, all overlapping predicted bounding boxes with an IoU lower than the defined NM S \nvalue are eliminated.  \nThe original YOLO based on the Dark net framework consisted of two sub -variants. \nThe first architecture comprised of 24 convolutional layers with the final layer providing \na connection  into the first of the two fully connected layers. Whereas the \u2018Fast YOLO \u2019 var-\niant consisted of only nine convolutional layers hosting fewer filters each. Inspired by the \ninception module in GoogleNet, a sequence of 1\u00d71  convolution al layers w as imple-\nmented for reducing the resultant feature space from the preceding layers.  The prelimi-\nnary architecture for YOLO -v1 is presented in Figure 3.  \nTo address the issue of multiple bounding boxes for the same object or with a confi-\ndence score of zero , i.e., no object,  the authors decided to greatly penalize predictions from \nbounding boxes containing objects ( \ud835\udefe\ud835\udc50\ud835\udc5c\ud835\udc5c\ud835\udc5f\ud835\udc51 =5) and the lowest penalization for prediction \ncontaining no object ( \ud835\udefe\ud835\udc5b\ud835\udc5c\ud835\udc5c\ud835\udc4f\ud835\udc57 =0.5). The authors calculated the loss function by taking the \nsum of a ll bounding box parameters ( x, y, width, height, confidence score , and class prob-\nability). As a result,  the first part of the equation computes the loss of the bounding box \nprediction with respect to the ground truth bounding box based on the coordinates \n\ud835\udc65\ud835\udc50\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5f , \ud835\udc66\ud835\udc50\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5f. \ud835\udd5d\ud835\udc56\ud835\udc57\ud835\udc5c\ud835\udc4f\ud835\udc57 is set as 1 in the case of the object residing within  \ud835\udc57\ud835\udc61\u210e bounding box \nprediction in  \ud835\udc56\ud835\udc61\u210e cell; otherwise , it is set as 0. The selected , i.e., predicted bounding box \nwould be tasked with predicting an object with the greatest IoU, as expressed  in (4): \n\ud835\udefe\ud835\udc50\ud835\udc5c\ud835\udc5c\ud835\udc5f\ud835\udc51 \u2211 \u2211 \ud835\udd5d\ud835\udc56\ud835\udc57\ud835\udc5c\ud835\udc4f\ud835\udc57[(\ud835\udc65\ud835\udc56\u2212\ud835\udc65\ud835\udc56\u0302)2+(\ud835\udc66\ud835\udc56\u2212\ud835\udc66\ud835\udc56\u0302)2]\ud835\udc35\n\ud835\udc57=0\ud835\udc462\n\ud835\udc56=0   (4) \nThe next component of the loss function computes the prediction error in width and \nheight of the bounding box, similar to the preceding component. However, the scale of \nerror in the large boxes has lesser impact compared to the small boxes. The normalizati on \nof width and height between the range 0 and 1 indicates that  their square roots increase \nobj\nijis set as 1 in the case of the object residing within jthbounding box prediction in\nithcell; otherwise, it is set as 0. The selected, i.e., predicted bounding box would be tasked\nwith predicting an object with the greatest IoU, as expressed in (4):\n\u03b3coord\u2211S2\ni=0\u2211B\nj=0\nMachines 2023 , 11, x FOR PEER REVIEW  5 of 26 \n \n  \nFigure 3. YOLO-v1 preliminary architecture.  \nAs alluded to earlier, the input image is split into s  \u00d7 s grid cells (default = 7  \u00d7 7), with \neach cell predicting B bounding boxes, each containing five parameters and sharing pre-\ndiction probabilities of classes ( C).", "start_char_idx": 3108, "end_char_idx": 6060, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d32811f7-f324-4379-a1e2-33e35a000b54": {"__data__": {"id_": "d32811f7-f324-4379-a1e2-33e35a000b54", "embedding": null, "metadata": {"page_label": "5", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6640492e-0115-4fb7-8c98-f82749b683e1", "node_type": "4", "metadata": {"page_label": "5", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "2b704c6c39816d523c2e550f8979ae9c2c96221b3902ad4756bb590bb981e815", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fc148e0d-c2c6-487a-8682-d08f2c02284d", "node_type": "1", "metadata": {"page_label": "5", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "0dc59c77f627d28924d49b4523228e3b68f164e7815576460a178c3aa85f45b2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3c9ce8fa-7f23-49f9-9a51-789f4e03ff6c", "node_type": "1", "metadata": {}, "hash": "e973aa23fa03edbdb641617002507f9698cf50930491755e7c417afb5e9f2ae3", "class_name": "RelatedNodeInfo"}}, "text": "The normalizati on \nof width and height between the range 0 and 1 indicates that  their square roots increase \nobj\nijis set as 1 in the case of the object residing within jthbounding box prediction in\nithcell; otherwise, it is set as 0. The selected, i.e., predicted bounding box would be tasked\nwith predicting an object with the greatest IoU, as expressed in (4):\n\u03b3coord\u2211S2\ni=0\u2211B\nj=0\nMachines 2023 , 11, x FOR PEER REVIEW  5 of 26 \n \n  \nFigure 3. YOLO-v1 preliminary architecture.  \nAs alluded to earlier, the input image is split into s  \u00d7 s grid cells (default = 7  \u00d7 7), with \neach cell predicting B bounding boxes, each containing five parameters and sharing pre-\ndiction probabilities of classes ( C). Therefore,  the parameter output would take the fol-\nlowing form, expressed in (2): \n\ud835\udc60\u00d7\ud835\udc60\u00d7(5\u2217\ud835\udc35+\ud835\udc36)  (2) \nConsidering  the example of YOLO network with each cell bounding box prediction \nset to 2 and evaluating the benchmark COCO dataset consisting of 80 classes, the param-\neter output would be given as  expressed in  (3): \n7\u00d77\u00d7(5\u22172+80)  (3) \nThe fundamental motive of YOLO and object detection in general is the object detec-\ntion and localization via bounding boxes. Therefore , two sets of bounding box vectors are \nrequired , i.e., vector y is the representative of ground truth  and vector \ud835\udc66\u0307 is the predicted \nvector. To address multiple bounding  boxes containing no  object or the same object, \nYOLO opts for non -maximum suppression (NMS). By defining a threshold value for \nNMS, all overlapping predicted bounding boxes with an IoU lower than the defined NM S \nvalue are eliminated.  \nThe original YOLO based on the Dark net framework consisted of two sub -variants. \nThe first architecture comprised of 24 convolutional layers with the final layer providing \na connection  into the first of the two fully connected layers. Whereas the \u2018Fast YOLO \u2019 var-\niant consisted of only nine convolutional layers hosting fewer filters each. Inspired by the \ninception module in GoogleNet, a sequence of 1\u00d71  convolution al layers w as imple-\nmented for reducing the resultant feature space from the preceding layers.  The prelimi-\nnary architecture for YOLO -v1 is presented in Figure 3.  \nTo address the issue of multiple bounding boxes for the same object or with a confi-\ndence score of zero , i.e., no object,  the authors decided to greatly penalize predictions from \nbounding boxes containing objects ( \ud835\udefe\ud835\udc50\ud835\udc5c\ud835\udc5c\ud835\udc5f\ud835\udc51 =5) and the lowest penalization for prediction \ncontaining no object ( \ud835\udefe\ud835\udc5b\ud835\udc5c\ud835\udc5c\ud835\udc4f\ud835\udc57 =0.5). The authors calculated the loss function by taking the \nsum of a ll bounding box parameters ( x, y, width, height, confidence score , and class prob-\nability). As a result,  the first part of the equation computes the loss of the bounding box \nprediction with respect to the ground truth bounding box based on the coordinates \n\ud835\udc65\ud835\udc50\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5f , \ud835\udc66\ud835\udc50\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5f. \ud835\udd5d\ud835\udc56\ud835\udc57\ud835\udc5c\ud835\udc4f\ud835\udc57 is set as 1 in the case of the object residing within  \ud835\udc57\ud835\udc61\u210e bounding box \nprediction in  \ud835\udc56\ud835\udc61\u210e cell; otherwise , it is set as 0.", "start_char_idx": 5352, "end_char_idx": 8339, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3c9ce8fa-7f23-49f9-9a51-789f4e03ff6c": {"__data__": {"id_": "3c9ce8fa-7f23-49f9-9a51-789f4e03ff6c", "embedding": null, "metadata": {"page_label": "5", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6640492e-0115-4fb7-8c98-f82749b683e1", "node_type": "4", "metadata": {"page_label": "5", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "2b704c6c39816d523c2e550f8979ae9c2c96221b3902ad4756bb590bb981e815", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d32811f7-f324-4379-a1e2-33e35a000b54", "node_type": "1", "metadata": {"page_label": "5", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "3e2cbc056bb10373e3c2d460d615b17eae9d3ed9f13b246ebb13bed5533e95dc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b204266c-faee-4602-a169-9b886c8a763d", "node_type": "1", "metadata": {}, "hash": "01d7aee0d6d13b93076efe76f7fb04b7821c51d439e8c6d0b7b62cda5b80900d", "class_name": "RelatedNodeInfo"}}, "text": "The authors calculated the loss function by taking the \nsum of a ll bounding box parameters ( x, y, width, height, confidence score , and class prob-\nability). As a result,  the first part of the equation computes the loss of the bounding box \nprediction with respect to the ground truth bounding box based on the coordinates \n\ud835\udc65\ud835\udc50\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5f , \ud835\udc66\ud835\udc50\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5f. \ud835\udd5d\ud835\udc56\ud835\udc57\ud835\udc5c\ud835\udc4f\ud835\udc57 is set as 1 in the case of the object residing within  \ud835\udc57\ud835\udc61\u210e bounding box \nprediction in  \ud835\udc56\ud835\udc61\u210e cell; otherwise , it is set as 0. The selected , i.e., predicted bounding box \nwould be tasked with predicting an object with the greatest IoU, as expressed  in (4): \n\ud835\udefe\ud835\udc50\ud835\udc5c\ud835\udc5c\ud835\udc5f\ud835\udc51 \u2211 \u2211 \ud835\udd5d\ud835\udc56\ud835\udc57\ud835\udc5c\ud835\udc4f\ud835\udc57[(\ud835\udc65\ud835\udc56\u2212\ud835\udc65\ud835\udc56\u0302)2+(\ud835\udc66\ud835\udc56\u2212\ud835\udc66\ud835\udc56\u0302)2]\ud835\udc35\n\ud835\udc57=0\ud835\udc462\n\ud835\udc56=0   (4) \nThe next component of the loss function computes the prediction error in width and \nheight of the bounding box, similar to the preceding component. However, the scale of \nerror in the large boxes has lesser impact compared to the small boxes. The normalizati on \nof width and height between the range 0 and 1 indicates that  their square roots increase \nobj\nij[\n(xi\u2212\u02c6xi)2+(yi\u2212\u02c6yi)2]\n(4)\nThe next component of the loss function computes the prediction error in width and\nheight of the bounding box, similar to the preceding component. However, the scale of\nerror in the large boxes has lesser impact compared to the small boxes. The normalization\nof width and height between the range 0 and 1 indicates that their square roots increase\nthe differences for smaller values to a higher degree compared to that of larger values,\nexpressed as (5):\n\u03b3coord\u2211S2\ni=0\u2211B\nj=0\nMachines 2023 , 11, x FOR PEER REVIEW  5 of 26 \n \n  \nFigure 3. YOLO-v1 preliminary architecture.  \nAs alluded to earlier, the input image is split into s  \u00d7 s grid cells (default = 7  \u00d7 7), with \neach cell predicting B bounding boxes, each containing five parameters and sharing pre-\ndiction probabilities of classes ( C). Therefore,  the parameter output would take the fol-\nlowing form, expressed in (2): \n\ud835\udc60\u00d7\ud835\udc60\u00d7(5\u2217\ud835\udc35+\ud835\udc36)  (2) \nConsidering  the example of YOLO network with each cell bounding box prediction \nset to 2 and evaluating the benchmark COCO dataset consisting of 80 classes, the param-\neter output would be given as  expressed in  (3): \n7\u00d77\u00d7(5\u22172+80)  (3) \nThe fundamental motive of YOLO and object detection in general is the object detec-\ntion and localization via bounding boxes. Therefore , two sets of bounding box vectors are \nrequired , i.e., vector y is the representative of ground truth  and vector \ud835\udc66\u0307 is the predicted \nvector. To address multiple bounding  boxes containing no  object or the same object, \nYOLO opts for non -maximum suppression (NMS). By defining a threshold value for \nNMS, all overlapping predicted bounding boxes with an IoU lower than the defined NM S \nvalue are eliminated.  \nThe original YOLO based on the Dark net framework consisted of two sub -variants. \nThe first architecture comprised of 24 convolutional layers with the final layer providing \na connection  into the first of the two fully connected layers. Whereas the \u2018Fast YOLO \u2019 var-\niant consisted of only nine convolutional layers hosting fewer filters each. Inspired by the \ninception module in GoogleNet, a sequence of 1\u00d71  convolution al layers w as imple-\nmented for reducing the resultant feature space from the preceding layers.", "start_char_idx": 7861, "end_char_idx": 11130, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b204266c-faee-4602-a169-9b886c8a763d": {"__data__": {"id_": "b204266c-faee-4602-a169-9b886c8a763d", "embedding": null, "metadata": {"page_label": "5", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6640492e-0115-4fb7-8c98-f82749b683e1", "node_type": "4", "metadata": {"page_label": "5", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "2b704c6c39816d523c2e550f8979ae9c2c96221b3902ad4756bb590bb981e815", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3c9ce8fa-7f23-49f9-9a51-789f4e03ff6c", "node_type": "1", "metadata": {"page_label": "5", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "a3b9ca8a42118473e246a45e816e45edf631989cf402ad256a2cca2f9edd8e5c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "37f30d8e-eac0-46c2-a85f-fe48615e784f", "node_type": "1", "metadata": {}, "hash": "93b5f35b2659a21d64acef3284817d75e6440f4926a895e72a74177d7e51ceec", "class_name": "RelatedNodeInfo"}}, "text": "To address multiple bounding  boxes containing no  object or the same object, \nYOLO opts for non -maximum suppression (NMS). By defining a threshold value for \nNMS, all overlapping predicted bounding boxes with an IoU lower than the defined NM S \nvalue are eliminated.  \nThe original YOLO based on the Dark net framework consisted of two sub -variants. \nThe first architecture comprised of 24 convolutional layers with the final layer providing \na connection  into the first of the two fully connected layers. Whereas the \u2018Fast YOLO \u2019 var-\niant consisted of only nine convolutional layers hosting fewer filters each. Inspired by the \ninception module in GoogleNet, a sequence of 1\u00d71  convolution al layers w as imple-\nmented for reducing the resultant feature space from the preceding layers.  The prelimi-\nnary architecture for YOLO -v1 is presented in Figure 3.  \nTo address the issue of multiple bounding boxes for the same object or with a confi-\ndence score of zero , i.e., no object,  the authors decided to greatly penalize predictions from \nbounding boxes containing objects ( \ud835\udefe\ud835\udc50\ud835\udc5c\ud835\udc5c\ud835\udc5f\ud835\udc51 =5) and the lowest penalization for prediction \ncontaining no object ( \ud835\udefe\ud835\udc5b\ud835\udc5c\ud835\udc5c\ud835\udc4f\ud835\udc57 =0.5). The authors calculated the loss function by taking the \nsum of a ll bounding box parameters ( x, y, width, height, confidence score , and class prob-\nability). As a result,  the first part of the equation computes the loss of the bounding box \nprediction with respect to the ground truth bounding box based on the coordinates \n\ud835\udc65\ud835\udc50\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5f , \ud835\udc66\ud835\udc50\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5f. \ud835\udd5d\ud835\udc56\ud835\udc57\ud835\udc5c\ud835\udc4f\ud835\udc57 is set as 1 in the case of the object residing within  \ud835\udc57\ud835\udc61\u210e bounding box \nprediction in  \ud835\udc56\ud835\udc61\u210e cell; otherwise , it is set as 0. The selected , i.e., predicted bounding box \nwould be tasked with predicting an object with the greatest IoU, as expressed  in (4): \n\ud835\udefe\ud835\udc50\ud835\udc5c\ud835\udc5c\ud835\udc5f\ud835\udc51 \u2211 \u2211 \ud835\udd5d\ud835\udc56\ud835\udc57\ud835\udc5c\ud835\udc4f\ud835\udc57[(\ud835\udc65\ud835\udc56\u2212\ud835\udc65\ud835\udc56\u0302)2+(\ud835\udc66\ud835\udc56\u2212\ud835\udc66\ud835\udc56\u0302)2]\ud835\udc35\n\ud835\udc57=0\ud835\udc462\n\ud835\udc56=0   (4) \nThe next component of the loss function computes the prediction error in width and \nheight of the bounding box, similar to the preceding component. However, the scale of \nerror in the large boxes has lesser impact compared to the small boxes. The normalizati on \nof width and height between the range 0 and 1 indicates that  their square roots increase \nobj\nij[(\u221awi\u2212\u221a\n\u02c6wi)2\n+(\u221a\nhi\u2212\u221a\n\u02c6hi)2]\n(5)\nNext, the loss of the con\ufb01dence score is computed based on whether the object is\npresent or absent with respect to the bounding box. Penalization of the object con\ufb01dence\nerror is only executed by the loss function if that predictor was responsible for the ground", "start_char_idx": 10338, "end_char_idx": 12840, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "37f30d8e-eac0-46c2-a85f-fe48615e784f": {"__data__": {"id_": "37f30d8e-eac0-46c2-a85f-fe48615e784f", "embedding": null, "metadata": {"page_label": "6", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8ad2a569-cd4b-4305-84e7-073c6fd01aa8", "node_type": "4", "metadata": {"page_label": "6", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "1d72aeedc9de1e4ae623183fac6e826600b207936fd67b792283d7171b91e375", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b204266c-faee-4602-a169-9b886c8a763d", "node_type": "1", "metadata": {"page_label": "5", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "3dac5c6cb8ceaf3844bdb8617b5e074178c4d9069ff32f84069c68ef66d69936", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "190dc89d-720f-4245-a8e7-b5a38479c977", "node_type": "1", "metadata": {}, "hash": "18e6a25f3fae9f4dcd2f6617183a6ca826f3eec5e90f61a47d7d710ef8b4a028", "class_name": "RelatedNodeInfo"}}, "text": "Machines 2023 ,11, 677 6 of 25\ntruth bounding box.\nMachines 2023 , 11, x FOR PEER REVIEW  5 of 26 \n \n  \nFigure 3. YOLO-v1 preliminary architecture.  \nAs alluded to earlier, the input image is split into s  \u00d7 s grid cells (default = 7  \u00d7 7), with \neach cell predicting B bounding boxes, each containing five parameters and sharing pre-\ndiction probabilities of classes ( C). Therefore,  the parameter output would take the fol-\nlowing form, expressed in (2): \n\ud835\udc60\u00d7\ud835\udc60\u00d7(5\u2217\ud835\udc35+\ud835\udc36)  (2) \nConsidering  the example of YOLO network with each cell bounding box prediction \nset to 2 and evaluating the benchmark COCO dataset consisting of 80 classes, the param-\neter output would be given as  expressed in  (3): \n7\u00d77\u00d7(5\u22172+80)  (3) \nThe fundamental motive of YOLO and object detection in general is the object detec-\ntion and localization via bounding boxes. Therefore , two sets of bounding box vectors are \nrequired , i.e., vector y is the representative of ground truth  and vector \ud835\udc66\u0307 is the predicted \nvector. To address multiple bounding  boxes containing no  object or the same object, \nYOLO opts for non -maximum suppression (NMS). By defining a threshold value for \nNMS, all overlapping predicted bounding boxes with an IoU lower than the defined NM S \nvalue are eliminated.  \nThe original YOLO based on the Dark net framework consisted of two sub -variants. \nThe first architecture comprised of 24 convolutional layers with the final layer providing \na connection  into the first of the two fully connected layers. Whereas the \u2018Fast YOLO \u2019 var-\niant consisted of only nine convolutional layers hosting fewer filters each. Inspired by the \ninception module in GoogleNet, a sequence of 1\u00d71  convolution al layers w as imple-\nmented for reducing the resultant feature space from the preceding layers.  The prelimi-\nnary architecture for YOLO -v1 is presented in Figure 3.  \nTo address the issue of multiple bounding boxes for the same object or with a confi-\ndence score of zero , i.e., no object,  the authors decided to greatly penalize predictions from \nbounding boxes containing objects ( \ud835\udefe\ud835\udc50\ud835\udc5c\ud835\udc5c\ud835\udc5f\ud835\udc51 =5) and the lowest penalization for prediction \ncontaining no object ( \ud835\udefe\ud835\udc5b\ud835\udc5c\ud835\udc5c\ud835\udc4f\ud835\udc57 =0.5). The authors calculated the loss function by taking the \nsum of a ll bounding box parameters ( x, y, width, height, confidence score , and class prob-\nability). As a result,  the first part of the equation computes the loss of the bounding box \nprediction with respect to the ground truth bounding box based on the coordinates \n\ud835\udc65\ud835\udc50\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5f , \ud835\udc66\ud835\udc50\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5f. \ud835\udd5d\ud835\udc56\ud835\udc57\ud835\udc5c\ud835\udc4f\ud835\udc57 is set as 1 in the case of the object residing within  \ud835\udc57\ud835\udc61\u210e bounding box \nprediction in  \ud835\udc56\ud835\udc61\u210e cell; otherwise , it is set as 0. The selected , i.e., predicted bounding box \nwould be tasked with predicting an object with the greatest IoU, as expressed  in (4): \n\ud835\udefe\ud835\udc50\ud835\udc5c\ud835\udc5c\ud835\udc5f\ud835\udc51 \u2211 \u2211 \ud835\udd5d\ud835\udc56\ud835\udc57\ud835\udc5c\ud835\udc4f\ud835\udc57[(\ud835\udc65\ud835\udc56\u2212\ud835\udc65\ud835\udc56\u0302)2+(\ud835\udc66\ud835\udc56\u2212\ud835\udc66\ud835\udc56\u0302)2]\ud835\udc35\n\ud835\udc57=0\ud835\udc462\n\ud835\udc56=0   (4) \nThe next component of the loss function computes the prediction error in width and \nheight of the bounding box, similar to the preceding component. However, the scale of \nerror in the large boxes has lesser impact compared to the small boxes.", "start_char_idx": 0, "end_char_idx": 3085, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "190dc89d-720f-4245-a8e7-b5a38479c977": {"__data__": {"id_": "190dc89d-720f-4245-a8e7-b5a38479c977", "embedding": null, "metadata": {"page_label": "6", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8ad2a569-cd4b-4305-84e7-073c6fd01aa8", "node_type": "4", "metadata": {"page_label": "6", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "1d72aeedc9de1e4ae623183fac6e826600b207936fd67b792283d7171b91e375", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "37f30d8e-eac0-46c2-a85f-fe48615e784f", "node_type": "1", "metadata": {"page_label": "6", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "291d914eddae357f53835d216616af77bcd1777fd0f44ecabf528212a5b9878d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f47f5810-f649-4fbd-b2f9-5e1330569c56", "node_type": "1", "metadata": {}, "hash": "a00288e61a03c923cf194af776ca25878a27cd6322651ba6e0f7ca54ff28b394", "class_name": "RelatedNodeInfo"}}, "text": "The selected , i.e., predicted bounding box \nwould be tasked with predicting an object with the greatest IoU, as expressed  in (4): \n\ud835\udefe\ud835\udc50\ud835\udc5c\ud835\udc5c\ud835\udc5f\ud835\udc51 \u2211 \u2211 \ud835\udd5d\ud835\udc56\ud835\udc57\ud835\udc5c\ud835\udc4f\ud835\udc57[(\ud835\udc65\ud835\udc56\u2212\ud835\udc65\ud835\udc56\u0302)2+(\ud835\udc66\ud835\udc56\u2212\ud835\udc66\ud835\udc56\u0302)2]\ud835\udc35\n\ud835\udc57=0\ud835\udc462\n\ud835\udc56=0   (4) \nThe next component of the loss function computes the prediction error in width and \nheight of the bounding box, similar to the preceding component. However, the scale of \nerror in the large boxes has lesser impact compared to the small boxes. The normalizati on \nof width and height between the range 0 and 1 indicates that  their square roots increase \nobj\nijis set to 1 when the object is present in the cell; otherwise, it is set\nas 0, whilst\nMachines 2023 , 11, x FOR PEER REVIEW  5 of 26 \n \n  \nFigure 3. YOLO-v1 preliminary architecture.  \nAs alluded to earlier, the input image is split into s  \u00d7 s grid cells (default = 7  \u00d7 7), with \neach cell predicting B bounding boxes, each containing five parameters and sharing pre-\ndiction probabilities of classes ( C). Therefore,  the parameter output would take the fol-\nlowing form, expressed in (2): \n\ud835\udc60\u00d7\ud835\udc60\u00d7(5\u2217\ud835\udc35+\ud835\udc36)  (2) \nConsidering  the example of YOLO network with each cell bounding box prediction \nset to 2 and evaluating the benchmark COCO dataset consisting of 80 classes, the param-\neter output would be given as  expressed in  (3): \n7\u00d77\u00d7(5\u22172+80)  (3) \nThe fundamental motive of YOLO and object detection in general is the object detec-\ntion and localization via bounding boxes. Therefore , two sets of bounding box vectors are \nrequired , i.e., vector y is the representative of ground truth  and vector \ud835\udc66\u0307 is the predicted \nvector. To address multiple bounding  boxes containing no  object or the same object, \nYOLO opts for non -maximum suppression (NMS). By defining a threshold value for \nNMS, all overlapping predicted bounding boxes with an IoU lower than the defined NM S \nvalue are eliminated.  \nThe original YOLO based on the Dark net framework consisted of two sub -variants. \nThe first architecture comprised of 24 convolutional layers with the final layer providing \na connection  into the first of the two fully connected layers. Whereas the \u2018Fast YOLO \u2019 var-\niant consisted of only nine convolutional layers hosting fewer filters each. Inspired by the \ninception module in GoogleNet, a sequence of 1\u00d71  convolution al layers w as imple-\nmented for reducing the resultant feature space from the preceding layers.  The prelimi-\nnary architecture for YOLO -v1 is presented in Figure 3.  \nTo address the issue of multiple bounding boxes for the same object or with a confi-\ndence score of zero , i.e., no object,  the authors decided to greatly penalize predictions from \nbounding boxes containing objects ( \ud835\udefe\ud835\udc50\ud835\udc5c\ud835\udc5c\ud835\udc5f\ud835\udc51 =5) and the lowest penalization for prediction \ncontaining no object ( \ud835\udefe\ud835\udc5b\ud835\udc5c\ud835\udc5c\ud835\udc4f\ud835\udc57 =0.5). The authors calculated the loss function by taking the \nsum of a ll bounding box parameters ( x, y, width, height, confidence score , and class prob-\nability). As a result,  the first part of the equation computes the loss of the bounding box \nprediction with respect to the ground truth bounding box based on the coordinates \n\ud835\udc65\ud835\udc50\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5f , \ud835\udc66\ud835\udc50\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5f.", "start_char_idx": 2653, "end_char_idx": 5757, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f47f5810-f649-4fbd-b2f9-5e1330569c56": {"__data__": {"id_": "f47f5810-f649-4fbd-b2f9-5e1330569c56", "embedding": null, "metadata": {"page_label": "6", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8ad2a569-cd4b-4305-84e7-073c6fd01aa8", "node_type": "4", "metadata": {"page_label": "6", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "1d72aeedc9de1e4ae623183fac6e826600b207936fd67b792283d7171b91e375", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "190dc89d-720f-4245-a8e7-b5a38479c977", "node_type": "1", "metadata": {"page_label": "6", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "1bb1746c9fc5193068bda03599ab2f189e1f677aea999c3d3b51057ec60a051b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0db7839f-7f06-4c25-be55-09aac901c7b4", "node_type": "1", "metadata": {}, "hash": "88601f5abf2b030bce8cb51a059d5f2f3a5d2914c9ce085f768aebafe4f965e9", "class_name": "RelatedNodeInfo"}}, "text": "The authors calculated the loss function by taking the \nsum of a ll bounding box parameters ( x, y, width, height, confidence score , and class prob-\nability). As a result,  the first part of the equation computes the loss of the bounding box \nprediction with respect to the ground truth bounding box based on the coordinates \n\ud835\udc65\ud835\udc50\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5f , \ud835\udc66\ud835\udc50\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5f. \ud835\udd5d\ud835\udc56\ud835\udc57\ud835\udc5c\ud835\udc4f\ud835\udc57 is set as 1 in the case of the object residing within  \ud835\udc57\ud835\udc61\u210e bounding box \nprediction in  \ud835\udc56\ud835\udc61\u210e cell; otherwise , it is set as 0. The selected , i.e., predicted bounding box \nwould be tasked with predicting an object with the greatest IoU, as expressed  in (4): \n\ud835\udefe\ud835\udc50\ud835\udc5c\ud835\udc5c\ud835\udc5f\ud835\udc51 \u2211 \u2211 \ud835\udd5d\ud835\udc56\ud835\udc57\ud835\udc5c\ud835\udc4f\ud835\udc57[(\ud835\udc65\ud835\udc56\u2212\ud835\udc65\ud835\udc56\u0302)2+(\ud835\udc66\ud835\udc56\u2212\ud835\udc66\ud835\udc56\u0302)2]\ud835\udc35\n\ud835\udc57=0\ud835\udc462\n\ud835\udc56=0   (4) \nThe next component of the loss function computes the prediction error in width and \nheight of the bounding box, similar to the preceding component. However, the scale of \nerror in the large boxes has lesser impact compared to the small boxes. The normalizati on \nof width and height between the range 0 and 1 indicates that  their square roots increase \nnoobj\nijworks in the opposite way, as shown in (6):\n\u2211S2\ni=0\u2211B\nj=0\nMachines 2023 , 11, x FOR PEER REVIEW  5 of 26 \n \n  \nFigure 3. YOLO-v1 preliminary architecture.  \nAs alluded to earlier, the input image is split into s  \u00d7 s grid cells (default = 7  \u00d7 7), with \neach cell predicting B bounding boxes, each containing five parameters and sharing pre-\ndiction probabilities of classes ( C). Therefore,  the parameter output would take the fol-\nlowing form, expressed in (2): \n\ud835\udc60\u00d7\ud835\udc60\u00d7(5\u2217\ud835\udc35+\ud835\udc36)  (2) \nConsidering  the example of YOLO network with each cell bounding box prediction \nset to 2 and evaluating the benchmark COCO dataset consisting of 80 classes, the param-\neter output would be given as  expressed in  (3): \n7\u00d77\u00d7(5\u22172+80)  (3) \nThe fundamental motive of YOLO and object detection in general is the object detec-\ntion and localization via bounding boxes. Therefore , two sets of bounding box vectors are \nrequired , i.e., vector y is the representative of ground truth  and vector \ud835\udc66\u0307 is the predicted \nvector. To address multiple bounding  boxes containing no  object or the same object, \nYOLO opts for non -maximum suppression (NMS). By defining a threshold value for \nNMS, all overlapping predicted bounding boxes with an IoU lower than the defined NM S \nvalue are eliminated.  \nThe original YOLO based on the Dark net framework consisted of two sub -variants. \nThe first architecture comprised of 24 convolutional layers with the final layer providing \na connection  into the first of the two fully connected layers. Whereas the \u2018Fast YOLO \u2019 var-\niant consisted of only nine convolutional layers hosting fewer filters each. Inspired by the \ninception module in GoogleNet, a sequence of 1\u00d71  convolution al layers w as imple-\nmented for reducing the resultant feature space from the preceding layers.  The prelimi-\nnary architecture for YOLO -v1 is presented in Figure 3.", "start_char_idx": 5412, "end_char_idx": 8310, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0db7839f-7f06-4c25-be55-09aac901c7b4": {"__data__": {"id_": "0db7839f-7f06-4c25-be55-09aac901c7b4", "embedding": null, "metadata": {"page_label": "6", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8ad2a569-cd4b-4305-84e7-073c6fd01aa8", "node_type": "4", "metadata": {"page_label": "6", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "1d72aeedc9de1e4ae623183fac6e826600b207936fd67b792283d7171b91e375", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f47f5810-f649-4fbd-b2f9-5e1330569c56", "node_type": "1", "metadata": {"page_label": "6", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "d0cb762f5580021c95fbded22c1e0ff605f6ddeff675e93f1e126180df8505fb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cd12dccb-a60b-45af-a183-436b82bdfb10", "node_type": "1", "metadata": {}, "hash": "f2b5930a7748aead12699d260f7633088ce80053b3e66e4c8e650fe37356f030", "class_name": "RelatedNodeInfo"}}, "text": "To address multiple bounding  boxes containing no  object or the same object, \nYOLO opts for non -maximum suppression (NMS). By defining a threshold value for \nNMS, all overlapping predicted bounding boxes with an IoU lower than the defined NM S \nvalue are eliminated.  \nThe original YOLO based on the Dark net framework consisted of two sub -variants. \nThe first architecture comprised of 24 convolutional layers with the final layer providing \na connection  into the first of the two fully connected layers. Whereas the \u2018Fast YOLO \u2019 var-\niant consisted of only nine convolutional layers hosting fewer filters each. Inspired by the \ninception module in GoogleNet, a sequence of 1\u00d71  convolution al layers w as imple-\nmented for reducing the resultant feature space from the preceding layers.  The prelimi-\nnary architecture for YOLO -v1 is presented in Figure 3.  \nTo address the issue of multiple bounding boxes for the same object or with a confi-\ndence score of zero , i.e., no object,  the authors decided to greatly penalize predictions from \nbounding boxes containing objects ( \ud835\udefe\ud835\udc50\ud835\udc5c\ud835\udc5c\ud835\udc5f\ud835\udc51 =5) and the lowest penalization for prediction \ncontaining no object ( \ud835\udefe\ud835\udc5b\ud835\udc5c\ud835\udc5c\ud835\udc4f\ud835\udc57 =0.5). The authors calculated the loss function by taking the \nsum of a ll bounding box parameters ( x, y, width, height, confidence score , and class prob-\nability). As a result,  the first part of the equation computes the loss of the bounding box \nprediction with respect to the ground truth bounding box based on the coordinates \n\ud835\udc65\ud835\udc50\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5f , \ud835\udc66\ud835\udc50\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5f. \ud835\udd5d\ud835\udc56\ud835\udc57\ud835\udc5c\ud835\udc4f\ud835\udc57 is set as 1 in the case of the object residing within  \ud835\udc57\ud835\udc61\u210e bounding box \nprediction in  \ud835\udc56\ud835\udc61\u210e cell; otherwise , it is set as 0. The selected , i.e., predicted bounding box \nwould be tasked with predicting an object with the greatest IoU, as expressed  in (4): \n\ud835\udefe\ud835\udc50\ud835\udc5c\ud835\udc5c\ud835\udc5f\ud835\udc51 \u2211 \u2211 \ud835\udd5d\ud835\udc56\ud835\udc57\ud835\udc5c\ud835\udc4f\ud835\udc57[(\ud835\udc65\ud835\udc56\u2212\ud835\udc65\ud835\udc56\u0302)2+(\ud835\udc66\ud835\udc56\u2212\ud835\udc66\ud835\udc56\u0302)2]\ud835\udc35\n\ud835\udc57=0\ud835\udc462\n\ud835\udc56=0   (4) \nThe next component of the loss function computes the prediction error in width and \nheight of the bounding box, similar to the preceding component. However, the scale of \nerror in the large boxes has lesser impact compared to the small boxes. The normalizati on \nof width and height between the range 0 and 1 indicates that  their square roots increase \nobj\nij(ci\u2212\u02c6ci)2+\u03b3noobj\u2211S2\ni=0\u2211B\nj=0\nMachines 2023 , 11, x FOR PEER REVIEW  5 of 26 \n \n  \nFigure 3. YOLO-v1 preliminary architecture.  \nAs alluded to earlier, the input image is split into s  \u00d7 s grid cells (default = 7  \u00d7 7), with \neach cell predicting B bounding boxes, each containing five parameters and sharing pre-\ndiction probabilities of classes ( C). Therefore,  the parameter output would take the fol-\nlowing form, expressed in (2): \n\ud835\udc60\u00d7\ud835\udc60\u00d7(5\u2217\ud835\udc35+\ud835\udc36)  (2) \nConsidering  the example of YOLO network with each cell bounding box prediction \nset to 2 and evaluating the benchmark COCO dataset consisting of 80 classes, the param-\neter output would be given as  expressed in  (3): \n7\u00d77\u00d7(5\u22172+80)  (3) \nThe fundamental motive of YOLO and object detection in general is the object detec-\ntion and localization via bounding boxes.", "start_char_idx": 7447, "end_char_idx": 10473, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cd12dccb-a60b-45af-a183-436b82bdfb10": {"__data__": {"id_": "cd12dccb-a60b-45af-a183-436b82bdfb10", "embedding": null, "metadata": {"page_label": "6", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8ad2a569-cd4b-4305-84e7-073c6fd01aa8", "node_type": "4", "metadata": {"page_label": "6", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "1d72aeedc9de1e4ae623183fac6e826600b207936fd67b792283d7171b91e375", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0db7839f-7f06-4c25-be55-09aac901c7b4", "node_type": "1", "metadata": {"page_label": "6", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "2fef59987d145cb7912c4fbfa4923502411711f30ae14799f2156567f18d216a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6ad428dc-65b0-4505-bda8-97106168845e", "node_type": "1", "metadata": {}, "hash": "b7ac90c6b9156ff4405cbae3d663092fcc53acf43f807e356bda45fb48fed5a1", "class_name": "RelatedNodeInfo"}}, "text": "As alluded to earlier, the input image is split into s  \u00d7 s grid cells (default = 7  \u00d7 7), with \neach cell predicting B bounding boxes, each containing five parameters and sharing pre-\ndiction probabilities of classes ( C). Therefore,  the parameter output would take the fol-\nlowing form, expressed in (2): \n\ud835\udc60\u00d7\ud835\udc60\u00d7(5\u2217\ud835\udc35+\ud835\udc36)  (2) \nConsidering  the example of YOLO network with each cell bounding box prediction \nset to 2 and evaluating the benchmark COCO dataset consisting of 80 classes, the param-\neter output would be given as  expressed in  (3): \n7\u00d77\u00d7(5\u22172+80)  (3) \nThe fundamental motive of YOLO and object detection in general is the object detec-\ntion and localization via bounding boxes. Therefore , two sets of bounding box vectors are \nrequired , i.e., vector y is the representative of ground truth  and vector \ud835\udc66\u0307 is the predicted \nvector. To address multiple bounding  boxes containing no  object or the same object, \nYOLO opts for non -maximum suppression (NMS). By defining a threshold value for \nNMS, all overlapping predicted bounding boxes with an IoU lower than the defined NM S \nvalue are eliminated.  \nThe original YOLO based on the Dark net framework consisted of two sub -variants. \nThe first architecture comprised of 24 convolutional layers with the final layer providing \na connection  into the first of the two fully connected layers. Whereas the \u2018Fast YOLO \u2019 var-\niant consisted of only nine convolutional layers hosting fewer filters each. Inspired by the \ninception module in GoogleNet, a sequence of 1\u00d71  convolution al layers w as imple-\nmented for reducing the resultant feature space from the preceding layers.  The prelimi-\nnary architecture for YOLO -v1 is presented in Figure 3.  \nTo address the issue of multiple bounding boxes for the same object or with a confi-\ndence score of zero , i.e., no object,  the authors decided to greatly penalize predictions from \nbounding boxes containing objects ( \ud835\udefe\ud835\udc50\ud835\udc5c\ud835\udc5c\ud835\udc5f\ud835\udc51 =5) and the lowest penalization for prediction \ncontaining no object ( \ud835\udefe\ud835\udc5b\ud835\udc5c\ud835\udc5c\ud835\udc4f\ud835\udc57 =0.5). The authors calculated the loss function by taking the \nsum of a ll bounding box parameters ( x, y, width, height, confidence score , and class prob-\nability). As a result,  the first part of the equation computes the loss of the bounding box \nprediction with respect to the ground truth bounding box based on the coordinates \n\ud835\udc65\ud835\udc50\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5f , \ud835\udc66\ud835\udc50\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5f. \ud835\udd5d\ud835\udc56\ud835\udc57\ud835\udc5c\ud835\udc4f\ud835\udc57 is set as 1 in the case of the object residing within  \ud835\udc57\ud835\udc61\u210e bounding box \nprediction in  \ud835\udc56\ud835\udc61\u210e cell; otherwise , it is set as 0. The selected , i.e., predicted bounding box \nwould be tasked with predicting an object with the greatest IoU, as expressed  in (4): \n\ud835\udefe\ud835\udc50\ud835\udc5c\ud835\udc5c\ud835\udc5f\ud835\udc51 \u2211 \u2211 \ud835\udd5d\ud835\udc56\ud835\udc57\ud835\udc5c\ud835\udc4f\ud835\udc57[(\ud835\udc65\ud835\udc56\u2212\ud835\udc65\ud835\udc56\u0302)2+(\ud835\udc66\ud835\udc56\u2212\ud835\udc66\ud835\udc56\u0302)2]\ud835\udc35\n\ud835\udc57=0\ud835\udc462\n\ud835\udc56=0   (4) \nThe next component of the loss function computes the prediction error in width and \nheight of the bounding box, similar to the preceding component. However, the scale of \nerror in the large boxes has lesser impact compared to the small boxes.", "start_char_idx": 150, "end_char_idx": 3085, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6ad428dc-65b0-4505-bda8-97106168845e": {"__data__": {"id_": "6ad428dc-65b0-4505-bda8-97106168845e", "embedding": null, "metadata": {"page_label": "6", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8ad2a569-cd4b-4305-84e7-073c6fd01aa8", "node_type": "4", "metadata": {"page_label": "6", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "1d72aeedc9de1e4ae623183fac6e826600b207936fd67b792283d7171b91e375", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cd12dccb-a60b-45af-a183-436b82bdfb10", "node_type": "1", "metadata": {"page_label": "6", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "ee4f1b0b5c718f6a987a86c0cd80f8c7f09a191a3d2af0479f4b05a4ab756c87", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b3b1a13f-927d-4c6b-b6fb-f79dbb543aed", "node_type": "1", "metadata": {}, "hash": "f41f25a771e89af8b3297c91e24a298084ba9857d71dfb6071847458570b82f6", "class_name": "RelatedNodeInfo"}}, "text": "The selected , i.e., predicted bounding box \nwould be tasked with predicting an object with the greatest IoU, as expressed  in (4): \n\ud835\udefe\ud835\udc50\ud835\udc5c\ud835\udc5c\ud835\udc5f\ud835\udc51 \u2211 \u2211 \ud835\udd5d\ud835\udc56\ud835\udc57\ud835\udc5c\ud835\udc4f\ud835\udc57[(\ud835\udc65\ud835\udc56\u2212\ud835\udc65\ud835\udc56\u0302)2+(\ud835\udc66\ud835\udc56\u2212\ud835\udc66\ud835\udc56\u0302)2]\ud835\udc35\n\ud835\udc57=0\ud835\udc462\n\ud835\udc56=0   (4) \nThe next component of the loss function computes the prediction error in width and \nheight of the bounding box, similar to the preceding component. However, the scale of \nerror in the large boxes has lesser impact compared to the small boxes. The normalizati on \nof width and height between the range 0 and 1 indicates that  their square roots increase \nnoobj\nij(xi\u2212\u02c6xi)2+(ci\u2212\u02c6ci)2(6)\nThe last component of the loss function, similar to the normal classi\ufb01cation loss,\ncalculates the class (c) probability loss, except for the\nMachines 2023 , 11, x FOR PEER REVIEW  5 of 26 \n \n  \nFigure 3. YOLO-v1 preliminary architecture.  \nAs alluded to earlier, the input image is split into s  \u00d7 s grid cells (default = 7  \u00d7 7), with \neach cell predicting B bounding boxes, each containing five parameters and sharing pre-\ndiction probabilities of classes ( C). Therefore,  the parameter output would take the fol-\nlowing form, expressed in (2): \n\ud835\udc60\u00d7\ud835\udc60\u00d7(5\u2217\ud835\udc35+\ud835\udc36)  (2) \nConsidering  the example of YOLO network with each cell bounding box prediction \nset to 2 and evaluating the benchmark COCO dataset consisting of 80 classes, the param-\neter output would be given as  expressed in  (3): \n7\u00d77\u00d7(5\u22172+80)  (3) \nThe fundamental motive of YOLO and object detection in general is the object detec-\ntion and localization via bounding boxes. Therefore , two sets of bounding box vectors are \nrequired , i.e., vector y is the representative of ground truth  and vector \ud835\udc66\u0307 is the predicted \nvector. To address multiple bounding  boxes containing no  object or the same object, \nYOLO opts for non -maximum suppression (NMS). By defining a threshold value for \nNMS, all overlapping predicted bounding boxes with an IoU lower than the defined NM S \nvalue are eliminated.  \nThe original YOLO based on the Dark net framework consisted of two sub -variants. \nThe first architecture comprised of 24 convolutional layers with the final layer providing \na connection  into the first of the two fully connected layers. Whereas the \u2018Fast YOLO \u2019 var-\niant consisted of only nine convolutional layers hosting fewer filters each. Inspired by the \ninception module in GoogleNet, a sequence of 1\u00d71  convolution al layers w as imple-\nmented for reducing the resultant feature space from the preceding layers.  The prelimi-\nnary architecture for YOLO -v1 is presented in Figure 3.  \nTo address the issue of multiple bounding boxes for the same object or with a confi-\ndence score of zero , i.e., no object,  the authors decided to greatly penalize predictions from \nbounding boxes containing objects ( \ud835\udefe\ud835\udc50\ud835\udc5c\ud835\udc5c\ud835\udc5f\ud835\udc51 =5) and the lowest penalization for prediction \ncontaining no object ( \ud835\udefe\ud835\udc5b\ud835\udc5c\ud835\udc5c\ud835\udc4f\ud835\udc57 =0.5). The authors calculated the loss function by taking the \nsum of a ll bounding box parameters ( x, y, width, height, confidence score , and class prob-\nability). As a result,  the first part of the equation computes the loss of the bounding box \nprediction with respect to the ground truth bounding box based on the coordinates \n\ud835\udc65\ud835\udc50\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5f , \ud835\udc66\ud835\udc50\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5f.", "start_char_idx": 12285, "end_char_idx": 15469, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b3b1a13f-927d-4c6b-b6fb-f79dbb543aed": {"__data__": {"id_": "b3b1a13f-927d-4c6b-b6fb-f79dbb543aed", "embedding": null, "metadata": {"page_label": "6", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8ad2a569-cd4b-4305-84e7-073c6fd01aa8", "node_type": "4", "metadata": {"page_label": "6", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "1d72aeedc9de1e4ae623183fac6e826600b207936fd67b792283d7171b91e375", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6ad428dc-65b0-4505-bda8-97106168845e", "node_type": "1", "metadata": {"page_label": "6", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "dc5f6801847351e50014f9bbf1b27b926b1e957c9f8fc293e55def0c4ad0ee8f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "62356069-fe6e-40f6-84f7-383d57128a4d", "node_type": "1", "metadata": {}, "hash": "456286042aaf299c1354b62c720916c131dbed0be11a31689afa31471307d7e7", "class_name": "RelatedNodeInfo"}}, "text": "The authors calculated the loss function by taking the \nsum of a ll bounding box parameters ( x, y, width, height, confidence score , and class prob-\nability). As a result,  the first part of the equation computes the loss of the bounding box \nprediction with respect to the ground truth bounding box based on the coordinates \n\ud835\udc65\ud835\udc50\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5f , \ud835\udc66\ud835\udc50\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5f. \ud835\udd5d\ud835\udc56\ud835\udc57\ud835\udc5c\ud835\udc4f\ud835\udc57 is set as 1 in the case of the object residing within  \ud835\udc57\ud835\udc61\u210e bounding box \nprediction in  \ud835\udc56\ud835\udc61\u210e cell; otherwise , it is set as 0. The selected , i.e., predicted bounding box \nwould be tasked with predicting an object with the greatest IoU, as expressed  in (4): \n\ud835\udefe\ud835\udc50\ud835\udc5c\ud835\udc5c\ud835\udc5f\ud835\udc51 \u2211 \u2211 \ud835\udd5d\ud835\udc56\ud835\udc57\ud835\udc5c\ud835\udc4f\ud835\udc57[(\ud835\udc65\ud835\udc56\u2212\ud835\udc65\ud835\udc56\u0302)2+(\ud835\udc66\ud835\udc56\u2212\ud835\udc66\ud835\udc56\u0302)2]\ud835\udc35\n\ud835\udc57=0\ud835\udc462\n\ud835\udc56=0   (4) \nThe next component of the loss function computes the prediction error in width and \nheight of the bounding box, similar to the preceding component. However, the scale of \nerror in the large boxes has lesser impact compared to the small boxes. The normalizati on \nof width and height between the range 0 and 1 indicates that  their square roots increase \nobj\nijpart, expressed in (7):\n\u2211S2\ni=0\nMachines 2023 , 11, x FOR PEER REVIEW  5 of 26 \n \n  \nFigure 3. YOLO-v1 preliminary architecture.  \nAs alluded to earlier, the input image is split into s  \u00d7 s grid cells (default = 7  \u00d7 7), with \neach cell predicting B bounding boxes, each containing five parameters and sharing pre-\ndiction probabilities of classes ( C). Therefore,  the parameter output would take the fol-\nlowing form, expressed in (2): \n\ud835\udc60\u00d7\ud835\udc60\u00d7(5\u2217\ud835\udc35+\ud835\udc36)  (2) \nConsidering  the example of YOLO network with each cell bounding box prediction \nset to 2 and evaluating the benchmark COCO dataset consisting of 80 classes, the param-\neter output would be given as  expressed in  (3): \n7\u00d77\u00d7(5\u22172+80)  (3) \nThe fundamental motive of YOLO and object detection in general is the object detec-\ntion and localization via bounding boxes. Therefore , two sets of bounding box vectors are \nrequired , i.e., vector y is the representative of ground truth  and vector \ud835\udc66\u0307 is the predicted \nvector. To address multiple bounding  boxes containing no  object or the same object, \nYOLO opts for non -maximum suppression (NMS). By defining a threshold value for \nNMS, all overlapping predicted bounding boxes with an IoU lower than the defined NM S \nvalue are eliminated.  \nThe original YOLO based on the Dark net framework consisted of two sub -variants. \nThe first architecture comprised of 24 convolutional layers with the final layer providing \na connection  into the first of the two fully connected layers. Whereas the \u2018Fast YOLO \u2019 var-\niant consisted of only nine convolutional layers hosting fewer filters each. Inspired by the \ninception module in GoogleNet, a sequence of 1\u00d71  convolution al layers w as imple-\nmented for reducing the resultant feature space from the preceding layers.  The prelimi-\nnary architecture for YOLO -v1 is presented in Figure 3.  \nTo address the issue of multiple bounding boxes for the same object or with a confi-\ndence score of zero , i.e., no object,  the authors decided to greatly penalize predictions from \nbounding boxes containing objects ( \ud835\udefe\ud835\udc50\ud835\udc5c\ud835\udc5c\ud835\udc5f\ud835\udc51 =5) and the lowest penalization for prediction \ncontaining no object ( \ud835\udefe\ud835\udc5b\ud835\udc5c\ud835\udc5c\ud835\udc4f\ud835\udc57 =0.5).", "start_char_idx": 15124, "end_char_idx": 18307, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "62356069-fe6e-40f6-84f7-383d57128a4d": {"__data__": {"id_": "62356069-fe6e-40f6-84f7-383d57128a4d", "embedding": null, "metadata": {"page_label": "6", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8ad2a569-cd4b-4305-84e7-073c6fd01aa8", "node_type": "4", "metadata": {"page_label": "6", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "1d72aeedc9de1e4ae623183fac6e826600b207936fd67b792283d7171b91e375", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b3b1a13f-927d-4c6b-b6fb-f79dbb543aed", "node_type": "1", "metadata": {"page_label": "6", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "23cb3c5c9c075b6a9b009c55b3731a75e203eef546a4b13a0f9d804d1ef8e9c0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "10c6d2b7-9e6f-43c5-848c-ad53aa7f6455", "node_type": "1", "metadata": {}, "hash": "3b186af7a05fe1aa014572f4781d44cd198880e1e61ca54cdde7fb16f0e5ca58", "class_name": "RelatedNodeInfo"}}, "text": "Whereas the \u2018Fast YOLO \u2019 var-\niant consisted of only nine convolutional layers hosting fewer filters each. Inspired by the \ninception module in GoogleNet, a sequence of 1\u00d71  convolution al layers w as imple-\nmented for reducing the resultant feature space from the preceding layers.  The prelimi-\nnary architecture for YOLO -v1 is presented in Figure 3.  \nTo address the issue of multiple bounding boxes for the same object or with a confi-\ndence score of zero , i.e., no object,  the authors decided to greatly penalize predictions from \nbounding boxes containing objects ( \ud835\udefe\ud835\udc50\ud835\udc5c\ud835\udc5c\ud835\udc5f\ud835\udc51 =5) and the lowest penalization for prediction \ncontaining no object ( \ud835\udefe\ud835\udc5b\ud835\udc5c\ud835\udc5c\ud835\udc4f\ud835\udc57 =0.5). The authors calculated the loss function by taking the \nsum of a ll bounding box parameters ( x, y, width, height, confidence score , and class prob-\nability). As a result,  the first part of the equation computes the loss of the bounding box \nprediction with respect to the ground truth bounding box based on the coordinates \n\ud835\udc65\ud835\udc50\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5f , \ud835\udc66\ud835\udc50\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5f. \ud835\udd5d\ud835\udc56\ud835\udc57\ud835\udc5c\ud835\udc4f\ud835\udc57 is set as 1 in the case of the object residing within  \ud835\udc57\ud835\udc61\u210e bounding box \nprediction in  \ud835\udc56\ud835\udc61\u210e cell; otherwise , it is set as 0. The selected , i.e., predicted bounding box \nwould be tasked with predicting an object with the greatest IoU, as expressed  in (4): \n\ud835\udefe\ud835\udc50\ud835\udc5c\ud835\udc5c\ud835\udc5f\ud835\udc51 \u2211 \u2211 \ud835\udd5d\ud835\udc56\ud835\udc57\ud835\udc5c\ud835\udc4f\ud835\udc57[(\ud835\udc65\ud835\udc56\u2212\ud835\udc65\ud835\udc56\u0302)2+(\ud835\udc66\ud835\udc56\u2212\ud835\udc66\ud835\udc56\u0302)2]\ud835\udc35\n\ud835\udc57=0\ud835\udc462\n\ud835\udc56=0   (4) \nThe next component of the loss function computes the prediction error in width and \nheight of the bounding box, similar to the preceding component. However, the scale of \nerror in the large boxes has lesser impact compared to the small boxes. The normalizati on \nof width and height between the range 0 and 1 indicates that  their square roots increase \nobj\nij\u2211c\u2208classes\n.(pi(c)\u2212\nMachines 2023 , 11, x FOR PEER REVIEW  6 of 26 \n \n the differences for smaller values to a higher degree compared to that of larger values, \nexpressed as (5):  \n\ud835\udefe\ud835\udc50\ud835\udc5c\ud835\udc5c\ud835\udc5f\ud835\udc51 \u2211 \u2211 \ud835\udd5d\ud835\udc56\ud835\udc57\ud835\udc5c\ud835\udc4f\ud835\udc57[(\u221a\ud835\udc64\ud835\udc56\u2212\u221a\ud835\udc64\ud835\udc56\u0302)2+(\u221a\u210e\ud835\udc56\u2212\u221a\u210e\ud835\udc56\u0302)2\n]\ud835\udc35\n\ud835\udc57=0\ud835\udc462\n\ud835\udc56=0   (5) \nNext, the loss of the confidence score is computed based on whether the object is \npresent or absent with respect to the bounding box. Penalization of the object confidence \nerror is only executed by the loss function if that predictor was responsible for the ground \ntruth bounding box.", "start_char_idx": 17641, "end_char_idx": 19867, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "10c6d2b7-9e6f-43c5-848c-ad53aa7f6455": {"__data__": {"id_": "10c6d2b7-9e6f-43c5-848c-ad53aa7f6455", "embedding": null, "metadata": {"page_label": "6", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8ad2a569-cd4b-4305-84e7-073c6fd01aa8", "node_type": "4", "metadata": {"page_label": "6", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "1d72aeedc9de1e4ae623183fac6e826600b207936fd67b792283d7171b91e375", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "62356069-fe6e-40f6-84f7-383d57128a4d", "node_type": "1", "metadata": {"page_label": "6", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "fc71816377c225785fe492740376500e4f61553fb6348619fb0ad06ebd50f488", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "605490da-c024-48e5-91cd-803d434f2701", "node_type": "1", "metadata": {}, "hash": "dc7a382562fff5a0d82a6ae083b1bf1c36d81b4237788f2734f16dc66ce55be3", "class_name": "RelatedNodeInfo"}}, "text": "Penalization of the object confidence \nerror is only executed by the loss function if that predictor was responsible for the ground \ntruth bounding box. \ud835\udd5d\ud835\udc56\ud835\udc57\ud835\udc5c\ud835\udc4f\ud835\udc57 is set to 1 when the object is present in the cell ; otherwise , it is \nset as 0, whilst \ud835\udd5dijnoobj works in the opposite way, as shown in (6):  \n \n\u2211 \u2211 \ud835\udd5dijobj(ci\u2212ci\u0302)2+\u03b3noobj \u2211 \u2211 \ud835\udd5dijnoobj(xi\u2212xi\u0302)2+(ci\u2212ci\u0302)2 B\nj=0S2\ni=0B\nj=0S2\ni=0   (6) \n  \nThe last component of the loss function, similar to the normal classification loss, cal-\nculates the class ( c) probability loss, except for the \ud835\udd5d\ud835\udc56\ud835\udc57\ud835\udc5c\ud835\udc4f\ud835\udc57 part, expressed in (7):  \n \n\u2211 \ud835\udd5d\ud835\udc56\ud835\udc57\ud835\udc5c\ud835\udc4f\ud835\udc57 \ud835\udc462\n\ud835\udc56=0 \u2211 (\ud835\udc5d\ud835\udc56(\ud835\udc50)\u2212 \ud835\udc50\u2208\ud835\udc50\ud835\udc59\ud835\udc4e\ud835\udc60\ud835\udc60\ud835\udc52\ud835\udc60.\ud835\udc5d\ud835\udc56(\ud835\udc50)\u0302 )2  (7) \n  \nPerformance wise, the simple YOLO (24 conv olutional  layers) when trained on the \nPASCAL VOC dataset (2007 and 2012) [39,40] achieved a mean average precision (refer-\nring to cross -class performance) (mAP) of 63.4% at 45 FPS, whilst Fast YOLO achieved \n52.7% mAP at an impressive 155 FPS. Although the performance was better than real -time \ndetectors , such as DPM -v5 [41] (33% mAP), it was lower than the state -of-the-art (SOTA) \nat the time , i.e., Faster R -CNN (71% mAP).  \nThere were some clear loopholes that required attention , such as the architecture hav-\ning comparatively low recall and higher localization error compared to Faster R -CNN. \nAdditionally, the architecture struggled to detect close proximity objec ts due to the fact \nthat each grid cell was capped to two bounding box proposals.  The loopholes attributed \nto the original YOLO provided inspiration for the following variants of YOLO.  \n2.2. YOLO-v2/9000  \nYOLO-v2/9000 was introduced by Joseph Redmon in 2016 [ 42]. The motive was to \nremove or at least mitigate the inefficiencies observed with the original YOLO whil e main-\ntaining the impressive speed factor. Several enhancements were claimed through the im-\nplementation of various techniques. Batch normalization [43 ] was introduced with the in-\nternal architecture to improve model convergence, leading to faster training. This intro-\nduction eliminated the need for other regularization techniques , such as dropout [44] \naimed at reducing overfitting [45]. Its effectiveness can be gauged by the fact that simply \nintroducing batch normalization improved the mAP by 2% compared to the original \nYOLO.  \nThe original YOLO worked with an input image size of 224  \u00d7 224 pixels during the \ntraining stage, whilst for the detection phase , input images could be scaled up to 448  \u00d7 448 \npixels, enforcing the architecture to adjust to the varying image resolution , which in turn \ndecrease the mAP. To address this, the authors trained the architecture on 448  \u00d7 448 pixel \nimages for 10 epochs on the  ImageNet [46] dataset, providing the architecture with the \ncapacity to adjust the internal filters when dealing with higher resolution images, result-\ning in an increased mAP of 4%.", "start_char_idx": 19715, "end_char_idx": 22573, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "605490da-c024-48e5-91cd-803d434f2701": {"__data__": {"id_": "605490da-c024-48e5-91cd-803d434f2701", "embedding": null, "metadata": {"page_label": "6", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8ad2a569-cd4b-4305-84e7-073c6fd01aa8", "node_type": "4", "metadata": {"page_label": "6", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "1d72aeedc9de1e4ae623183fac6e826600b207936fd67b792283d7171b91e375", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "10c6d2b7-9e6f-43c5-848c-ad53aa7f6455", "node_type": "1", "metadata": {"page_label": "6", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "39b8f0d45e0c1f65a0e51c62c4fc6bccdeda896a248d4cba8869eaf21252ce08", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b1b2e1a6-b0ca-4205-b2b3-4d6e393d1363", "node_type": "1", "metadata": {}, "hash": "45ba7dc2946d972444c6dc87a905ae915e5478a1a159258368d2cc6f05f145dc", "class_name": "RelatedNodeInfo"}}, "text": "This intro-\nduction eliminated the need for other regularization techniques , such as dropout [44] \naimed at reducing overfitting [45]. Its effectiveness can be gauged by the fact that simply \nintroducing batch normalization improved the mAP by 2% compared to the original \nYOLO.  \nThe original YOLO worked with an input image size of 224  \u00d7 224 pixels during the \ntraining stage, whilst for the detection phase , input images could be scaled up to 448  \u00d7 448 \npixels, enforcing the architecture to adjust to the varying image resolution , which in turn \ndecrease the mAP. To address this, the authors trained the architecture on 448  \u00d7 448 pixel \nimages for 10 epochs on the  ImageNet [46] dataset, providing the architecture with the \ncapacity to adjust the internal filters when dealing with higher resolution images, result-\ning in an increased mAP of 4%. Whilst architectures , such as Fast and Faster R -CNN pre-\ndict coordinates dir ectly from the convolutional network, the original YOLO utilized fully  (7)\nPerformance wise, the simple YOLO (24 convolutional layers) when trained on the\nPASCAL VOC dataset (2007 and 2012) [ 39,40] achieved a mean average precision (referring\nto cross-class performance) (mAP) of 63.4% at 45 FPS, whilst Fast YOLO achieved 52.7%\nmAP at an impressive 155 FPS. Although the performance was better than real-time\ndetectors, such as DPM-v5 [ 41] (33% mAP), it was lower than the state-of-the-art (SOTA) at\nthe time, i.e., Faster R-CNN (71% mAP).\nThere were some clear loopholes that required attention, such as the architecture\nhaving comparatively low recall and higher localization error compared to Faster R-CNN.\nAdditionally, the architecture struggled to detect close proximity objects due to the fact that\neach grid cell was capped to two bounding box proposals. The loopholes attributed to the\noriginal YOLO provided inspiration for the following variants of YOLO.\n2.2. YOLO-v2/9000\nYOLO-v2/9000 was introduced by Joseph Redmon in 2016 [ 42]. The motive was\nto remove or at least mitigate the inef\ufb01ciencies observed with the original YOLO while\nmaintaining the impressive speed factor. Several enhancements were claimed through\nthe implementation of various techniques. Batch normalization [ 43] was introduced with\nthe internal architecture to improve model convergence, leading to faster training. This\nintroduction eliminated the need for other regularization techniques, such as dropout [ 44]\naimed at reducing over\ufb01tting [ 45]. Its effectiveness can be gauged by the fact that simply\nintroducing batch normalization improved the mAP by 2% compared to the original YOLO.\nThe original YOLO worked with an input image size of 224 \u00d7224 pixels during\nthe training stage, whilst for the detection phase, input images could be scaled up to\n448\u00d7448 pixels, enforcing the architecture to adjust to the varying image resolution,\nwhich in turn decrease the mAP . To address this, the authors trained the architecture on\n448\u00d7448 pixel images for 10 epochs on the ImageNet [ 46] dataset, providing the architec-\nture with the capacity to adjust the internal \ufb01lters when dealing with higher resolution\nimages, resulting in an increased mAP of 4%. Whilst architectures, such as Fast and Faster\nR-CNN predict coordinates directly from the convolutional network, the original YOLO\nutilized fully connected layers to serve this purpose. YOLO-v2 replaced the fully connected\nlayer responsible for predicting bounding boxes by adding anchor boxes for bounding\nbox predictions. Anchor boxes [ 47] are essentially a list of prede\ufb01ned dimensions (boxes)\naimed at best matching the objects of interest. Rather than manual determination of best-\ufb01t\nanchor boxes, the authors utilized k-means clustering [ 48] on the training set bounding\nboxes, inclusive of the ground truth bounding boxes, grouping similar shapes and plotting\naverage IoU with respect to the closest centroid as shown in Figure 4.", "start_char_idx": 21714, "end_char_idx": 25636, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b1b2e1a6-b0ca-4205-b2b3-4d6e393d1363": {"__data__": {"id_": "b1b2e1a6-b0ca-4205-b2b3-4d6e393d1363", "embedding": null, "metadata": {"page_label": "6", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8ad2a569-cd4b-4305-84e7-073c6fd01aa8", "node_type": "4", "metadata": {"page_label": "6", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "1d72aeedc9de1e4ae623183fac6e826600b207936fd67b792283d7171b91e375", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "605490da-c024-48e5-91cd-803d434f2701", "node_type": "1", "metadata": {"page_label": "6", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "8d0cc7250bcd4be46704cb966532cebecfed10ec5f6fe7b55fc8448948308c45", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "04ca27d0-7812-42de-bbc6-24a6d2f4acdc", "node_type": "1", "metadata": {}, "hash": "dc6296a9e2a203e3b0458b95e0c9071560496e090790dc50da17f8331773f5a8", "class_name": "RelatedNodeInfo"}}, "text": "Whilst architectures, such as Fast and Faster\nR-CNN predict coordinates directly from the convolutional network, the original YOLO\nutilized fully connected layers to serve this purpose. YOLO-v2 replaced the fully connected\nlayer responsible for predicting bounding boxes by adding anchor boxes for bounding\nbox predictions. Anchor boxes [ 47] are essentially a list of prede\ufb01ned dimensions (boxes)\naimed at best matching the objects of interest. Rather than manual determination of best-\ufb01t\nanchor boxes, the authors utilized k-means clustering [ 48] on the training set bounding\nboxes, inclusive of the ground truth bounding boxes, grouping similar shapes and plotting\naverage IoU with respect to the closest centroid as shown in Figure 4. YOLO-v2 was trained\non different architectures, namely, VGG-16 and GoogleNet, in addition to the authors\nproposing the Darknet-19 [ 49] architecture due to characteristics, such as reduced process-\ning requirements, i.e., 5.58 FLOPs compared to 30.69 FLOPs and 8.52 FLOPs on VGG-16\nand GoogleNet, respectively. In terms of performance, YOLO-v2 provided 76.8 mAP at\n67 FPS and 78.6 mAP at 40 FPS. The results demonstrated the architectures\u2019 superiority\nover SOTA architectures of that time, such as SSD and Faster R-CNN. YOLO-9000 utilized", "start_char_idx": 24897, "end_char_idx": 26175, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "04ca27d0-7812-42de-bbc6-24a6d2f4acdc": {"__data__": {"id_": "04ca27d0-7812-42de-bbc6-24a6d2f4acdc", "embedding": null, "metadata": {"page_label": "7", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "27a7f5ca-a0ad-4c4e-bc7a-e7ba1ddddd65", "node_type": "4", "metadata": {"page_label": "7", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "1ab2201f5b043e411e1fdd424502ec064865e1d5a63291b31195a18ac0158af6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b1b2e1a6-b0ca-4205-b2b3-4d6e393d1363", "node_type": "1", "metadata": {"page_label": "6", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "886aee12b49923efbf51446c6e1f86c46159c2b3a8fb62a1352268ba3c7dc08f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6822cb04-b0a8-412a-8aeb-69b3d02ed83d", "node_type": "1", "metadata": {}, "hash": "c5dd6968b29b1aa9973392671e0fcfac63e5d3fcb2b4285020300f38ed9fec8a", "class_name": "RelatedNodeInfo"}}, "text": "Machines 2023 ,11, 677 7 of 25\nYOLO-v2 architecture, aimed at real-time detection of more than 9000 different objects;\nhowever, at a signi\ufb01cantly reduced mAP of 19.7%.\nMachines 2023 , 11, x FOR PEER REVIEW 7 of 26 \n \n connected layers to serve this purpose. YO LO-v2 replaced the fully connected layer re-\nsponsible for predicting bounding boxes by adding anchor boxes for bounding box pre-\ndictions. Anchor boxes [47] are essentially a list of prede \ufb01ned dimensions (boxes) aimed \nat best matching the objects of interest. Rather than manual determination of best- \ufb01t an-\nchor boxes, the authors utilized k-means clus tering [48] on the training set bounding \nboxes, inclusive of the ground truth boundi ng boxes, grouping similar shapes and plo tting \naverage IoU with respect to the closest cent roid as shown in Figure 4. YOLO-v2 was \ntrained on di \ufb00erent architectures, namely, VGG-16 and GoogleNet, in addition to the au-\nthors proposing the Darknet-19 [49] architectu re due to characteristics, such as reduced \nprocessing requirements, i.e., 5.58 FLOPs compared to 30.69 FLOPs and 8.52 FLOPs on \nVGG-16 and GoogleNet, respectively. In term s of performance, YOLO-v2 provided 76.8 \nmAP at 67 FPS and 78.6 mAP at 40 FPS. The results demonstrated the architectures\u2019 supe-\nriority over SOTA architectures of that time, such as SSD and Faster R-CNN. YOLO-9000 \nutilized YOLO-v2 architecture, aimed at real-time detection of more than 9000 di \ufb00erent \nobjects; however, at a signi \ufb01cantly reduced mAP of 19.7%. \n \nFigure 4. Dimension cluste rs vs. mAP. \n2.3. YOLO-v3 \nArchitectures, such as VGG, focused their development work around the concept \nthat deeper networks, i.e., more internal laye rs, equated to higher accuracy. YOLO-v2 also \nhad higher number of convolutional layers compared to its predecessor. \nHowever, as the image progressed throug h the network, the progressive down sam-\npling resulted in the loss of \ufb01ne-grained features; therefore, YOLO-v2 often struggled with \ndetecting smaller objects. At the time research was active in addressing this issue, as evi-\ndent by the deployment of skip connection s [50] embedded within the proposed ResNet \narchitecture, the focus was on addressing the vanishing gradient issue by facilitating in-\nformation propagation via skip connection, as presented in Figure 5. \nFigure 4. Dimension clusters vs. mAP .\n2.3. YOLO-v3\nArchitectures, such as VGG, focused their development work around the concept that\ndeeper networks, i.e., more internal layers, equated to higher accuracy. YOLO-v2 also had\nhigher number of convolutional layers compared to its predecessor.\nHowever, as the image progressed through the network, the progressive down sam-\npling resulted in the loss of \ufb01ne-grained features; therefore, YOLO-v2 often struggled\nwith detecting smaller objects. At the time research was active in addressing this issue,\nas evident by the deployment of skip connections [ 50] embedded within the proposed\nResNet architecture, the focus was on addressing the vanishing gradient issue by facilitating\ninformation propagation via skip connection, as presented in Figure 5.\nMachines 2023 , 11, x FOR PEER REVIEW 8 of 26 \n \n  \nFigure 5. Skip-connection con \ufb01guration. \nYOLO-v3 proposed a hybrid architecture fa ctoring in aspects of YOLO-v2, Darknet-\n53 [51], and the ResNet concept of residual networks. This enabled the preservation of \n\ufb01ne-grained features by a llowing for the gradient \ufb02ow from shallow layers to deeper lay-\ners. \nOn top of the existing 53 layers of Darknet-53 for feature extraction, a stack of 53 \nadditional layers was added for the detection head, totaling 106 convolutional layers for \nthe YOLO-v3.", "start_char_idx": 0, "end_char_idx": 3674, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6822cb04-b0a8-412a-8aeb-69b3d02ed83d": {"__data__": {"id_": "6822cb04-b0a8-412a-8aeb-69b3d02ed83d", "embedding": null, "metadata": {"page_label": "7", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "27a7f5ca-a0ad-4c4e-bc7a-e7ba1ddddd65", "node_type": "4", "metadata": {"page_label": "7", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "1ab2201f5b043e411e1fdd424502ec064865e1d5a63291b31195a18ac0158af6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "04ca27d0-7812-42de-bbc6-24a6d2f4acdc", "node_type": "1", "metadata": {"page_label": "7", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "8916cec4203095cc01c2068c2dd4452d17fc21ec64b1b8ad5f83b51c61961273", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2aab20ce-7dba-468d-b904-64998792342b", "node_type": "1", "metadata": {}, "hash": "43c0eedd6d2095ee5e319605432fec5c4eba928b377338c0f20fd5ccaae94024", "class_name": "RelatedNodeInfo"}}, "text": "Machines 2023 , 11, x FOR PEER REVIEW 8 of 26 \n \n  \nFigure 5. Skip-connection con \ufb01guration. \nYOLO-v3 proposed a hybrid architecture fa ctoring in aspects of YOLO-v2, Darknet-\n53 [51], and the ResNet concept of residual networks. This enabled the preservation of \n\ufb01ne-grained features by a llowing for the gradient \ufb02ow from shallow layers to deeper lay-\ners. \nOn top of the existing 53 layers of Darknet-53 for feature extraction, a stack of 53 \nadditional layers was added for the detection head, totaling 106 convolutional layers for \nthe YOLO-v3. Additionally, YOLO-v3 facilitated multi-scale detection, namely, the archi-\ntecture made predictions at three di \ufb00erent scales of granularity for outpu tting better per-\nformance, increasing the probability of small object detection. \n2.4. YOLO-v4 \nYOLO-v4 was the \ufb01rst variant of the YOLO family after the original author discon-\ntinued further work that was introduced to the computer vi sion community in April 2020 \nby Alexey Bochkovsky et al. [52]. YOLO-v4 was essentially the distillation of a large suite \nof object detection techniques, tested and en hanced for providing a real-time, lightweight \nobject detector. \nThe backbone of an object detector has a critical role in the quality of features ex-\ntracted. In-line with the experimental spirit, the authors experimented with three di \ufb00erent \nbackbones: CSPResNext-50, CSPDarknet-53, and E \ufb03cientNet-B3 [53]. The \ufb01rst was based \non DenseNet [54] aimed at alleviating the vanishing gradient problem and bolstering fea-ture propagation and reuse, resulting in reduced number of network parameters. E \ufb03-\ncientNet was proposed by Google Brain. The paper posits that an optima selection for \nparameters when scaling CNNs can be asce rtained through a search mechanism. After \nexperimenting with the above feature extractors, the authors based on their intuition and \nbacked by their experimental result s selected CSPDarknet-53 as the o \ufb03cial backbone for \nYOLO-v4. \nFor feature aggregation, the authors experi mented with several techniques for inte-\ngration at the neck level including feature pyramid network (FPN) [55] and path aggrega-tion network (PANet) [56]. Ultimately, the authors opted for PANet as the feature aggre-\ngator. The modi \ufb01ed PANet, as shown in Figure 6, utilized the concatenation mechanism. \nPANet can be seen as an advanced version of FPN, namely, PANet proposed a bo ttom-up \naugmentation path along with the top-down path (FPN), adding a \u2018shortcut\u2019 connection \nfor linking \ufb01ne-grained features from high- and low-level layers. Additionally, the authors \nintroduced a SPP [57] block post CSPDarknet-53 aimed at increasing the receptive \ufb01eld \nand separation of the important features arriving from the backbone. \nFigure 5. Skip-connection con\ufb01guration.", "start_char_idx": 3125, "end_char_idx": 5909, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2aab20ce-7dba-468d-b904-64998792342b": {"__data__": {"id_": "2aab20ce-7dba-468d-b904-64998792342b", "embedding": null, "metadata": {"page_label": "8", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "da46a3dd-d186-4e85-96d2-4ce903ed0f84", "node_type": "4", "metadata": {"page_label": "8", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "ddcbbcb03a663dad97334eb0249362ee1533711914f7df4e50a7dacb8e992b4c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6822cb04-b0a8-412a-8aeb-69b3d02ed83d", "node_type": "1", "metadata": {"page_label": "7", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "b5dca853c132412195f71bffde61cc81afbdf5d7cc14a3e6e5bffe1266f481a0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0ac8ead8-2866-4dbc-8d68-af957df18bf7", "node_type": "1", "metadata": {}, "hash": "f14bcb601fc48749254f498f64c8cbf3190b769f005149765254a20294c264f9", "class_name": "RelatedNodeInfo"}}, "text": "Machines 2023 ,11, 677 8 of 25\nYOLO-v3 proposed a hybrid architecture factoring in aspects of YOLO-v2, Darknet-\n53 [51], and the ResNet concept of residual networks. This enabled the preservation of\n\ufb01ne-grained features by allowing for the gradient \ufb02ow from shallow layers to deeper layers.\nOn top of the existing 53 layers of Darknet-53 for feature extraction, a stack of 53 addi-\ntional layers was added for the detection head, totaling 106 convolutional layers for the\nYOLO-v3. Additionally, YOLO-v3 facilitated multi-scale detection, namely, the architecture\nmade predictions at three different scales of granularity for outputting better performance,\nincreasing the probability of small object detection.\n2.4. YOLO-v4\nYOLO-v4 was the \ufb01rst variant of the YOLO family after the original author discon-\ntinued further work that was introduced to the computer vision community in April 2020\nby Alexey Bochkovsky et al. [52]. YOLO-v4 was essentially the distillation of a large suite\nof object detection techniques, tested and enhanced for providing a real-time, lightweight\nobject detector.\nThe backbone of an object detector has a critical role in the quality of features extracted.\nIn-line with the experimental spirit, the authors experimented with three different back-\nbones: CSPResNext-50, CSPDarknet-53, and Ef\ufb01cientNet-B3 [ 53]. The \ufb01rst was based on\nDenseNet [ 54] aimed at alleviating the vanishing gradient problem and bolstering feature\npropagation and reuse, resulting in reduced number of network parameters. Ef\ufb01cientNet\nwas proposed by Google Brain. The paper posits that an optima selection for parameters\nwhen scaling CNNs can be ascertained through a search mechanism. After experimenting\nwith the above feature extractors, the authors based on their intuition and backed by their\nexperimental results selected CSPDarknet-53 as the of\ufb01cial backbone for YOLO-v4.\nFor feature aggregation, the authors experimented with several techniques for integra-\ntion at the neck level including feature pyramid network (FPN) [ 55] and path aggregation\nnetwork (PANet) [ 56]. Ultimately, the authors opted for PANet as the feature aggregator.\nThe modi\ufb01ed PANet, as shown in Figure 6, utilized the concatenation mechanism. PANet\ncan be seen as an advanced version of FPN, namely, PANet proposed a bottom-up augmen-\ntation path along with the top-down path (FPN), adding a \u2018shortcut\u2019 connection for linking\n\ufb01ne-grained features from high- and low-level layers. Additionally, the authors introduced\na SPP [ 57] block post CSPDarknet-53 aimed at increasing the receptive \ufb01eld and separation\nof the important features arriving from the backbone.\nMachines 2023 , 11, x FOR PEER REVIEW 9 of 26 \n \n  \nFigure 6. Path aggregation. ( a) Original PAN, ( b) modi \ufb01ed PAN. \nThe authors also introduced a bag-of-freebie s, presented in Figure 7, primarily con-\nsisting of augmentations, such as Mosaic aimed at improving performance without intro-\nducing additional baggage onto the inference time. CIoU loss [58] was also introduced as \na freebie, focused on the overlap of the predic ted and ground truth bounding box. In the \ncase of no overlap, the idea was to observe the closeness of the two boxes and encourage \noverlap if in close proximity. \nIn addition to the bag-of-freebies, the auth ors introduced \u2018bag-of-specials\u2019, with the \nauthors claiming that although this set of op timization techniques presented in Figure 7 \nwould marginally impact the inference time, they would signi \ufb01cantly improve the overall \nperformance. One of the components within th e \u2018bag-of-specials\u2019 was the Mish [59] acti-\nvation function aimed at moving feature creations toward their respective optimal points. Cross mini-batch normalization [60] was also  presented facilitating the running on any \nGPU as many batch normalization techniques involve multiple GPUs operating in tan-\ndem. \n \nFigure 7. State-of-the-art optimization methodologies experimented in YOLO-v4 via bag-of-spe-\ncials. \n2.5.", "start_char_idx": 0, "end_char_idx": 3976, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0ac8ead8-2866-4dbc-8d68-af957df18bf7": {"__data__": {"id_": "0ac8ead8-2866-4dbc-8d68-af957df18bf7", "embedding": null, "metadata": {"page_label": "8", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "da46a3dd-d186-4e85-96d2-4ce903ed0f84", "node_type": "4", "metadata": {"page_label": "8", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "ddcbbcb03a663dad97334eb0249362ee1533711914f7df4e50a7dacb8e992b4c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2aab20ce-7dba-468d-b904-64998792342b", "node_type": "1", "metadata": {"page_label": "8", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "16c84d7d1777e8f7b868118935fe9b119eeefa555aeac9a959aa354ef0891520", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a9155316-bdd4-43a4-a71e-8cafc54c5c05", "node_type": "1", "metadata": {}, "hash": "7833d289bbaa1447bee1f8e348c6d2aa6fe0a0b08e3b54e853244ed80f7f852a", "class_name": "RelatedNodeInfo"}}, "text": "In the \ncase of no overlap, the idea was to observe the closeness of the two boxes and encourage \noverlap if in close proximity. \nIn addition to the bag-of-freebies, the auth ors introduced \u2018bag-of-specials\u2019, with the \nauthors claiming that although this set of op timization techniques presented in Figure 7 \nwould marginally impact the inference time, they would signi \ufb01cantly improve the overall \nperformance. One of the components within th e \u2018bag-of-specials\u2019 was the Mish [59] acti-\nvation function aimed at moving feature creations toward their respective optimal points. Cross mini-batch normalization [60] was also  presented facilitating the running on any \nGPU as many batch normalization techniques involve multiple GPUs operating in tan-\ndem. \n \nFigure 7. State-of-the-art optimization methodologies experimented in YOLO-v4 via bag-of-spe-\ncials. \n2.5. YOLO-v5 \nThe YOLO network in essence consists of th ree key pillars, namely, backbone for fea-\nture extraction, neck focused on feature aggregation, and the head for consuming output \nFigure 6. Path aggregation. ( a) Original PAN, ( b) modi\ufb01ed PAN.", "start_char_idx": 3111, "end_char_idx": 4225, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a9155316-bdd4-43a4-a71e-8cafc54c5c05": {"__data__": {"id_": "a9155316-bdd4-43a4-a71e-8cafc54c5c05", "embedding": null, "metadata": {"page_label": "9", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1c4b5901-ea90-4603-9e64-f3e49de73a06", "node_type": "4", "metadata": {"page_label": "9", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "8374cd17b828ae83e891a976be0fa4e79917868c55d534659cd4ed2aaf05dcc7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0ac8ead8-2866-4dbc-8d68-af957df18bf7", "node_type": "1", "metadata": {"page_label": "8", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "95952352b66f552f063e72ca200dc475c541bd21c92567f9120d80c41fd71c7b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ad1a9dda-5747-44f8-a302-6b3b9ac20cdf", "node_type": "1", "metadata": {}, "hash": "f0684797aa90209d1cc89be9438b93cb7e2dbb4adc9d882b9510fabd902bfb57", "class_name": "RelatedNodeInfo"}}, "text": "Machines 2023 ,11, 677 9 of 25\nThe authors also introduced a bag-of-freebies, presented in Figure 7, primarily consist-\ning of augmentations, such as Mosaic aimed at improving performance without introducing\nadditional baggage onto the inference time. CIoU loss [ 58] was also introduced as a freebie,\nfocused on the overlap of the predicted and ground truth bounding box. In the case of no\noverlap, the idea was to observe the closeness of the two boxes and encourage overlap if in\nclose proximity.\nMachines 2023 , 11, x FOR PEER REVIEW 9 of 26 \n \n  \nFigure 6. Path aggregation. ( a) Original PAN, ( b) modi \ufb01ed PAN. \nThe authors also introduced a bag-of-freebie s, presented in Figure 7, primarily con-\nsisting of augmentations, such as Mosaic aimed at improving performance without intro-\nducing additional baggage onto the inference time. CIoU loss [58] was also introduced as \na freebie, focused on the overlap of the predic ted and ground truth bounding box. In the \ncase of no overlap, the idea was to observe the closeness of the two boxes and encourage \noverlap if in close proximity. \nIn addition to the bag-of-freebies, the auth ors introduced \u2018bag-of-specials\u2019, with the \nauthors claiming that although this set of op timization techniques presented in Figure 7 \nwould marginally impact the inference time, they would signi \ufb01cantly improve the overall \nperformance. One of the components within th e \u2018bag-of-specials\u2019 was the Mish [59] acti-\nvation function aimed at moving feature creations toward their respective optimal points. Cross mini-batch normalization [60] was also  presented facilitating the running on any \nGPU as many batch normalization techniques involve multiple GPUs operating in tan-\ndem. \n \nFigure 7. State-of-the-art optimization methodologies experimented in YOLO-v4 via bag-of-spe-\ncials. \n2.5. YOLO-v5 \nThe YOLO network in essence consists of th ree key pillars, namely, backbone for fea-\nture extraction, neck focused on feature aggregation, and the head for consuming output \nFigure 7. State-of-the-art optimization methodologies experimented in YOLO-v4 via bag-of-specials.\nIn addition to the bag-of-freebies, the authors introduced \u2018bag-of-specials\u2019, with the\nauthors claiming that although this set of optimization techniques presented in Figure 7\nwould marginally impact the inference time, they would signi\ufb01cantly improve the overall\nperformance. One of the components within the \u2018bag-of-specials\u2019 was the Mish [ 59] acti-\nvation function aimed at moving feature creations toward their respective optimal points.\nCross mini-batch normalization [ 60] was also presented facilitating the running on any\nGPU as many batch normalization techniques involve multiple GPUs operating in tandem.\n2.5. YOLO-v5\nThe YOLO network in essence consists of three key pillars, namely, backbone for\nfeature extraction, neck focused on feature aggregation, and the head for consuming\noutput features from the neck as input and generating detections. YOLO-v5 [ 61] similar to\nYOLO-v4, with respect to contributions, focus on the conglomeration and re\ufb01nement of\nvarious computer vision techniques for enhancing performance. In addition, in less than\n2 months after the release of YOLO-v4, Glenn Jocher open-sourced an implementation of\nYOLO-v5 [61].\nA notable mention is that YOLO-v5 was the \ufb01rst native release of architectures be-\nlonging to the YOLO clan, to be written in PyTorch [ 62] rather than Darknet. Although\nDarknet is considered as a \ufb02exible low-level research framework, it was not purpose built\nfor production environments with a signi\ufb01cantly smaller number of subscribers due to\ncon\ufb01gurability challenges. PyTorch, on the other hand, provided an established eco-system,\nwith a wider subscription base among the computer vision community and provided the\nsupporting infrastructure for facilitating mobile device deployment.", "start_char_idx": 0, "end_char_idx": 3860, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ad1a9dda-5747-44f8-a302-6b3b9ac20cdf": {"__data__": {"id_": "ad1a9dda-5747-44f8-a302-6b3b9ac20cdf", "embedding": null, "metadata": {"page_label": "10", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d8c32baf-e976-4c41-9e49-c5fb6c01e93c", "node_type": "4", "metadata": {"page_label": "10", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "8bf396675eb4c728f2afa57b29ce433761b3abf678e4863100cf8dc550e0054f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a9155316-bdd4-43a4-a71e-8cafc54c5c05", "node_type": "1", "metadata": {"page_label": "9", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "8374cd17b828ae83e891a976be0fa4e79917868c55d534659cd4ed2aaf05dcc7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a3dc245f-7b97-4133-9223-110012d6ce3e", "node_type": "1", "metadata": {}, "hash": "7b65e459b8d68f499f221a45fa0ae8e34e4473b03e3b87a5227f6f2f3ae1e3d6", "class_name": "RelatedNodeInfo"}}, "text": "Machines 2023 ,11, 677 10 of 25\nIn addition, another notable proposal was the \u2018automated anchor box learning\u2019 concept.\nIn YOLO-v2, the anchor box mechanism was introduced based on selecting anchor boxes\nthat closely resemble the dimensions of the ground truth boxes in the training set via\nk-means. The authors select the \ufb01ve close-\ufb01t anchor boxes based on the COCO dataset [ 63]\nand implement them as default boxes. However, the application of this methodology to a\nunique dataset with signi\ufb01cant object differentials compared to those present in the COCO\ndataset can quickly expose the inability of the prede\ufb01ned boxes to adapt quickly to the\nunique dataset. Therefore, authors in YOLO-v5 integrated the anchor box selection process\ninto the YOLO-v5 pipeline. As a result, the network would automatically learn the best-\ufb01t\nanchor boxes for the particular dataset and utilize them during training to accelerate the\nprocess. YOLO-v5 comes in several variants with respect to the computational parameters\nas presented in Table 1.\nTable 1. YOLO-v5 internal variant comparison.\nModel Average Precision (@50) Parameters FLOPs\nYOLO-v5s 55.8% 7.5 M 13.2B\nYOLO-v5m 62.4% 21.8 M 39.4B\nYOLO-v5l 65.4% 47.8 M 88.1B\nYOLO-v5x 66.9% 86.7 M 205.7B\nYOLO-v5 comprised of a weight \ufb01le equating to 27 MB compared to YOLO-v5l at 192\nMB. Figure 8 demonstrates the superiority of YOLO-v5 over Ef\ufb01cientDet [64].\nMachines 2023 , 11, x FOR PEER REVIEW 10 of 26 \n \n features from the neck as input and generating detections. YOLO-v5 [61] similar to YOLO-\nv4, with respect to contributions, focus on the conglomeration and re \ufb01nement of various \ncomputer vision techniques for enhancing perfor mance. In addition, in less than 2 months \nafter the release of YOLO-v4, Glenn Jocher open-sourced an implementation of YOLO-v5 \n[61]. \nA notable mention is that YOLO-v5 was the \ufb01rst native release of architectures be-\nlonging to the YOLO clan, to be wri tten in PyTorch [62] rather than Darknet. Although \nDarknet is considered as a \ufb02exible low-level research framework, it was not purpose built \nfor production environments with a signi \ufb01cantly smaller number of subscribers due to \ncon\ufb01gurability challenges. PyTorch, on the othe r hand, provided an established eco-sys-\ntem, with a wider subscription base among the computer vision community and provided \nthe supporting infrastructure for facilitating mobile device deployment. \nIn addition, another notable proposal was the \u2018automated anchor box learning\u2019 con-\ncept. In YOLO-v2, the anchor box mechanism was introduced based on selecting anchor \nboxes that closely resemble the dimensions of  the ground truth boxes in the training set \nvia k-means. The authors select the \ufb01ve close- \ufb01t anchor boxes based on the COCO dataset \n[63] and implement them as default boxes. However, the application of this methodology \nto a unique dataset with signi \ufb01cant object di \ufb00erentials compared to those present in the \nCOCO dataset can quickly expose the inability of the prede \ufb01ned boxes to adapt quickly \nto the unique dataset. Therefore, authors in  YOLO-v5 integrated the anchor box selection \nprocess into the YOLO-v5 pipeline. As a result, the network would automatically learn the best- \ufb01t anchor boxes for the particular dataset and utilize them during training to ac-\ncelerate the process. YOLO-v5 comes in severa l variants with respect to the computational \nparameters as presented in Table 1. \nTable 1. YOLO-v5 internal variant comparison.", "start_char_idx": 0, "end_char_idx": 3463, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a3dc245f-7b97-4133-9223-110012d6ce3e": {"__data__": {"id_": "a3dc245f-7b97-4133-9223-110012d6ce3e", "embedding": null, "metadata": {"page_label": "10", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d8c32baf-e976-4c41-9e49-c5fb6c01e93c", "node_type": "4", "metadata": {"page_label": "10", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "8bf396675eb4c728f2afa57b29ce433761b3abf678e4863100cf8dc550e0054f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ad1a9dda-5747-44f8-a302-6b3b9ac20cdf", "node_type": "1", "metadata": {"page_label": "10", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "a3c5fbc7146643ba831efa439874e53f81166af63f5d80a832d04c78e578e665", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "655ecc22-74fb-4999-8586-e93b4beb56d0", "node_type": "1", "metadata": {}, "hash": "c2c58a264abd3bad92238372d927335027add2bf6620e32f251efb281401dfcf", "class_name": "RelatedNodeInfo"}}, "text": "The authors select the \ufb01ve close- \ufb01t anchor boxes based on the COCO dataset \n[63] and implement them as default boxes. However, the application of this methodology \nto a unique dataset with signi \ufb01cant object di \ufb00erentials compared to those present in the \nCOCO dataset can quickly expose the inability of the prede \ufb01ned boxes to adapt quickly \nto the unique dataset. Therefore, authors in  YOLO-v5 integrated the anchor box selection \nprocess into the YOLO-v5 pipeline. As a result, the network would automatically learn the best- \ufb01t anchor boxes for the particular dataset and utilize them during training to ac-\ncelerate the process. YOLO-v5 comes in severa l variants with respect to the computational \nparameters as presented in Table 1. \nTable 1. YOLO-v5 internal variant comparison. \nModel Average Precision (@50) Parameters FLOPs \nYOLO-v5s 55.8% 7.5 M 13.2B \nYOLO-v5m 62.4% 21.8 M 39.4B \nYOLO-v5l 65.4% 47.8 M 88.1B \nYOLO-v5x 66.9% 86.7 M 205.7B \nYOLO-v5 comprised of a weight \ufb01le equating to 27 MB compared to YOLO-v5l at 192 \nMB. Figure 8 demonstrates the superiority of YOLO-v5 over E \ufb03cientDet [64]. \n \nFigure 8. YOLO-v5 variant comparison vs. E \ufb03cientDet [61]. \nFigure 8. YOLO-v5 variant comparison vs. Ef\ufb01cientDet [61].\n2.6. YOLO-v6\nThe initial codebase for YOLO-v6 [ 65] was released in June 2022 by the Meituan\nTechnical Team based in China. The authors focused their design strategy on producing an\nindustry-orientated object detector.\nTo meet industrial application requirements, the architecture would need to be highly\nperformant on a range of hardware options, maintaining high speed and accuracy. To\nconform with the diverse set of industrial applications, YOLO-v6 comes in several variants\nstarting with YOLO-v6-nano as the fastest with the least number of parameters and reaching\nYOLO-v6-large with high accuracy at the expense of speed, as shown in Table 2.", "start_char_idx": 2674, "end_char_idx": 4556, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "655ecc22-74fb-4999-8586-e93b4beb56d0": {"__data__": {"id_": "655ecc22-74fb-4999-8586-e93b4beb56d0", "embedding": null, "metadata": {"page_label": "11", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b709ba6e-cc70-4811-902c-a513de62c8ac", "node_type": "4", "metadata": {"page_label": "11", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "2626cf4d38ea8f4ce1543961e1d8f68a8d191a3339ac25e80b4888c3bc33c759", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a3dc245f-7b97-4133-9223-110012d6ce3e", "node_type": "1", "metadata": {"page_label": "10", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "ddf0ef11c975f777b06911ce518c67bdad62801208dd2d270436651bb3a316fd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "128a3420-93d6-4152-be00-bd6b56d67b01", "node_type": "1", "metadata": {}, "hash": "760322d7372e3def57b1fc354bdac57e507d047c963c2a5500b708343424f9e4", "class_name": "RelatedNodeInfo"}}, "text": "Machines 2023 ,11, 677 11 of 25\nTable 2. YOLO-v6 variant comparison.\nVariantmAP 0.5:0.95\n(COCO-val)FPS Tesla T4 Parameters (Million)\nYOLO-v6-N 35.9 (300 epochs) 802 4.3\nYOLO-v6-T 40.3 (300 epochs) 449 15.0\nYOLO-v6-RepOpt 43.3 (300 epochs) 596 17.2\nYOLO-v6-S 43.5 (300 epochs) 495 17.2\nYOLO-v6-M 49.7 233 34.3\nYOLO-v6-L-ReLU 51.7 149 58.5\nThe impressive performance presented in Table 2 is a result of several innovations\nintegrated into the YOLO-v6 architecture. The key contributions can be summed into four\npoints. First, in contrast to its predecessors, YOLO-v6 opts for an anchor-free approach,\nmaking it 51% faster when compared to anchor-based approaches.\nSecond, the authors introduced a revised reparametrized backbone and neck, proposed\nas Ef\ufb01cientRep backbone and Rep-PAN neck [ 66], namely, up to and including YOLO-v5,\nthe regression and classi\ufb01cation heads shared the same features. Breaking the convention,\nYOLO-v6 implements the decoupled head as shown in Figure 9. As a result, the architecture\nhas additional layers separating features from the \ufb01nal head, as empirically shown to\nimprove the performance. Third, YOLO-v6 mandates a two-loss function. Varifocal loss\n(VFL) [ 67] is used as the classi\ufb01cation loss and distribution focal loss (DFL) [ 68], along with\nSIoU/GIoU [ 69] as regression loss. VFL being a derivative of focal loss, treats positive\nand negative samples at varying degrees of importance, helping in balancing the learning\nsignals from both sample types. DFL is deployed for box regression in YOLO-v6 medium\nand large variants, treating the continuous distribution of the box locations as discretized\nprobability distribution, which is shown to be particularly ef\ufb01cient when the ground truth\nbox boundaries are blurred.\nMachines 2023 , 11, x FOR PEER REVIEW 11 of 26 \n \n 2.6. YOLO-v6 \nThe initial codebase for YOLO-v6 [65] was released in June 2022 by the Meituan Tech-\nnical Team based in China. The authors focu sed their design strategy on producing an \nindustry-orientated object detector. \nTo meet industrial application requirements, the architecture would need to be \nhighly performant on a range of hardware op tions, maintaining high speed and accuracy. \nTo conform with the diverse set of industrial applications, YOLO-v6 comes in several var-\niants starting with YOLO-v6-nano as the fastest with the least number of parameters and reaching YOLO-v6-large with high accuracy at the expense of speed, as shown in Table 2. \nTable 2. YOLO-v6 variant comparison. \nVariant mAP 0.5:0.95 \n(COCO-val) FPS Tesla T4 Parameters (Million) \nYOLO-v6-N 35.9 (300 epochs) 802 4.3 \nYOLO-v6-T 40.3 (300 epochs) 449 15.0 \nYOLO-v6-RepOpt 43.3 (300 epochs) 596 17.2 \nYOLO-v6-S 43.5 (300 epochs) 495 17.2 \nYOLO-v6-M 49.7 233 34.3 \nYOLO-v6-L-ReLU 51.7 149 58.5 \nThe impressive performance presented in Ta ble 2 is a result of several innovations \nintegrated into the YOLO-v6 ar chitecture. The key contributi ons can be summed into four \npoints. First, in contrast to its predecesso rs, YOLO-v6 opts for an anchor-free approach, \nmaking it 51% faster when compared to anchor-based approaches.", "start_char_idx": 0, "end_char_idx": 3114, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "128a3420-93d6-4152-be00-bd6b56d67b01": {"__data__": {"id_": "128a3420-93d6-4152-be00-bd6b56d67b01", "embedding": null, "metadata": {"page_label": "11", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b709ba6e-cc70-4811-902c-a513de62c8ac", "node_type": "4", "metadata": {"page_label": "11", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "2626cf4d38ea8f4ce1543961e1d8f68a8d191a3339ac25e80b4888c3bc33c759", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "655ecc22-74fb-4999-8586-e93b4beb56d0", "node_type": "1", "metadata": {"page_label": "11", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "00375b9f35ce3e6d9894a6b625d71f47d4d894ec12da6c975eec939a5459099f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "204329bd-79eb-4787-a511-f017b6c322e5", "node_type": "1", "metadata": {}, "hash": "6e149c02e9f5883695d69905a90206b518719b66e492019449aa4ff7e33b2dcb", "class_name": "RelatedNodeInfo"}}, "text": "The key contributi ons can be summed into four \npoints. First, in contrast to its predecesso rs, YOLO-v6 opts for an anchor-free approach, \nmaking it 51% faster when compared to anchor-based approaches. \nSecond, the authors introduced a revised reparametrized backbone and neck, pro-\nposed as E \ufb03cientRep backbone and Rep-PAN neck [66], namely, up to and including \nYOLO-v5, the regression and classi \ufb01cation heads shared the same features. Breaking the \nconvention, YOLO-v6 implements the decoupled head as shown in Figure 9. As a result, \nthe architecture has additional layers separating features from the \ufb01nal head, as empiri-\ncally shown to improve the performance. Thir d, YOLO-v6 mandates a two-loss function. \nVarifocal loss (VFL) [67] is used as the classi \ufb01cation loss and distribution focal loss (DFL) \n[68], along with SIoU/GIoU [69] as regression loss. VFL being a derivative of focal loss, \ntreats positive and negative samples at varying degrees of importance, helping in balanc-\ning the learning signals from both sample types. DFL is deployed for box regression in YOLO-v6 medium and large variants, treating the continuous distribution of the box lo-\ncations as discretized probability distribution, which is shown to be particularly e \ufb03cient \nwhen the ground truth box boundaries are blurred. \n \nFigure 9. YOLO-v6 model base architecture.\nAdditional improvements focused on industrial applications include the use of knowl-\nedge distillation [70], involving a teacher model used for training a student model, where\nthe predictions of the teacher are used as soft labels along with the ground truth for training\nthe student. This comes without fueling the computational cost as essentially the aim is\nto train a smaller (student) model to replicate the high performance of the larger (teacher)\nmodel. Comparing the performance of YOLO-v6 with its predecessors, including YOLO-v5\non the benchmark COCO dataset in Figure 10, it is clear that YOLO-v6 achieves a higher\nmAP at various FPS.", "start_char_idx": 2912, "end_char_idx": 4915, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "204329bd-79eb-4787-a511-f017b6c322e5": {"__data__": {"id_": "204329bd-79eb-4787-a511-f017b6c322e5", "embedding": null, "metadata": {"page_label": "12", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5046b664-6d35-4b4d-9529-de82cf07730c", "node_type": "4", "metadata": {"page_label": "12", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "41ab4cac59398a3512fff2942a483b2e2b4985c6b7fcdb7d6eabdd64c4862a7f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "128a3420-93d6-4152-be00-bd6b56d67b01", "node_type": "1", "metadata": {"page_label": "11", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "2ec05d542d71ec1ba528a6173828b8f4fbc49bf2ebcbaf31aab1974f875a4657", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b53883d1-11f9-4df9-9cde-9cbd73e45c7e", "node_type": "1", "metadata": {}, "hash": "23dc08f1fc4a295636ec6bb431e4ac56c4f152435929dfeea51e219490f5c56c", "class_name": "RelatedNodeInfo"}}, "text": "Machines 2023 ,11, 677 12 of 25\nMachines 2023 , 11, x FOR PEER REVIEW 12 of 26 \n \n Figure 9. YOLO-v6 model base architecture. \nAdditional improvements focused on indu strial applications include the use of \nknowledge distillation [70], involving a teacher model used for training a student model, \nwhere the predictions of the teacher are used as soft labels along with the ground truth \nfor training the student. This comes without fueling the computational cost as essentially the aim is to train a smaller (student) model to replicate the high performance of the larger \n(teacher) model. Comparing the performance of YOLO-v6 with its predecessors, includ-\ning YOLO-v5 on the benchmark COCO dataset in Figure 10, it is clear that YOLO-v6 \nachieves a higher mAP at various FPS. \n \nFigure 10. Relative evaluation of YOLO-v6 vs. YOLO-v5 [71]. \n2.7. YOLO-v7 \nThe following month after the release of YOLO-v6, the YOLO-v7 was released [72]. \nAlthough other variants have been releas ed in between, including YOLO-X [73] and \nYOLO-R [74], these focused more on GPU speed  enhancements with respect to inferenc-\ning. YOLO-v7 proposes several architectural reforms for improving the accuracy and \nmaintaining high detection speeds. The proposed reforms can be split into two categories: \nArchitectural reforms and Trainable BoF (bag-o f-freebies). Architectural reforms included \nthe implementation of the E-ELAN (extended e \ufb03cient layer aggregation network) [75] in \nthe YOLO-v7 backbone, taking inspiration from research advancements in network e \ufb03-\nciency. The design of the E-ELAN was guided by the analysis of factors that impact accu-\nracy and speed, such as memory access cost , input/output channel ratio, and gradient \npath. \nThe second architectural reform was presented as compound model scaling, as \nshown in Figure 11. The aim was to cater for a wider scope of application requirements, \ni.e., certain applications require accuracy to be prioritized, whilst others may prioritize speed. Although NAS (network architecture se arch) [76] can be used for parameter-spe-\nci\ufb01c scaling to \ufb01nd the best factors, the scaling factors are independent [77]. Whereas the \ncompound-scaling mechanism a llows for the width and depth to be scaled in coherence \nfor concatenation-based networks, maintaining optimal network architecture while scal-\ning for di \ufb00erent sizes. \nFigure 10. Relative evaluation of YOLO-v6 vs. YOLO-v5 [71].\n2.7. YOLO-v7\nThe following month after the release of YOLO-v6, the YOLO-v7 was released [ 72].\nAlthough other variants have been released in between, including YOLO-X [ 73] and YOLO-\nR [74], these focused more on GPU speed enhancements with respect to inferencing. YOLO-\nv7 proposes several architectural reforms for improving the accuracy and maintaining high\ndetection speeds. The proposed reforms can be split into two categories: Architectural\nreforms and Trainable BoF (bag-of-freebies). Architectural reforms included the implemen-\ntation of the E-ELAN (extended ef\ufb01cient layer aggregation network) [ 75] in the YOLO-v7\nbackbone, taking inspiration from research advancements in network ef\ufb01ciency. The design\nof the E-ELAN was guided by the analysis of factors that impact accuracy and speed, such\nas memory access cost, input/output channel ratio, and gradient path.\nThe second architectural reform was presented as compound model scaling, as shown\nin Figure 11. The aim was to cater for a wider scope of application requirements, i.e., certain\napplications require accuracy to be prioritized, whilst others may prioritize speed. Although\nNAS (network architecture search) [ 76] can be used for parameter-speci\ufb01c scaling to \ufb01nd\nthe best factors, the scaling factors are independent [ 77]. Whereas the compound-scaling\nmechanism allows for the width and depth to be scaled in coherence for concatenation-\nbased networks, maintaining optimal network architecture while scaling for different sizes.", "start_char_idx": 0, "end_char_idx": 3928, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b53883d1-11f9-4df9-9cde-9cbd73e45c7e": {"__data__": {"id_": "b53883d1-11f9-4df9-9cde-9cbd73e45c7e", "embedding": null, "metadata": {"page_label": "12", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5046b664-6d35-4b4d-9529-de82cf07730c", "node_type": "4", "metadata": {"page_label": "12", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "41ab4cac59398a3512fff2942a483b2e2b4985c6b7fcdb7d6eabdd64c4862a7f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "204329bd-79eb-4787-a511-f017b6c322e5", "node_type": "1", "metadata": {"page_label": "12", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "93436786d442663a487bd2aa3be8824954a07e80db4ecdc96dd175fb47e5160d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2c892c7b-3dc6-4ae9-9787-bec7737985ee", "node_type": "1", "metadata": {}, "hash": "a353da9bbb168ae1763b28a2ae48c7d0ef8a7b1487cc5e6bd453944b9ab2704c", "class_name": "RelatedNodeInfo"}}, "text": "The design\nof the E-ELAN was guided by the analysis of factors that impact accuracy and speed, such\nas memory access cost, input/output channel ratio, and gradient path.\nThe second architectural reform was presented as compound model scaling, as shown\nin Figure 11. The aim was to cater for a wider scope of application requirements, i.e., certain\napplications require accuracy to be prioritized, whilst others may prioritize speed. Although\nNAS (network architecture search) [ 76] can be used for parameter-speci\ufb01c scaling to \ufb01nd\nthe best factors, the scaling factors are independent [ 77]. Whereas the compound-scaling\nmechanism allows for the width and depth to be scaled in coherence for concatenation-\nbased networks, maintaining optimal network architecture while scaling for different sizes.\nMachines 2023 , 11, x FOR PEER REVIEW 13 of 26 \n \n  \nFigure 11. YOLO-v7 compound scaling. \nRe-parameterization planning is based on averaging a set of model weights to obtain \na more robust network [78,79]. Expanding further, module level re-parameterization ena-\nbles segments of the network to regulate th eir own parameterization strategies. YOLO-v7 \nutilizes gradient \ufb02ow propagation paths with the aim to observe which internal network \nmodules should deploy re-parameterization strategies. \nThe auxiliary head coarse-to- \ufb01ne concept is proposed on  the premise that the net-\nwork head is quite far downstream; therefore, the auxiliary head is deployed at the middle \nlayers to assist in the training proces s. However, this would not train as e \ufb03ciently as the \nlead head, due to the former not having access to the complete network. \nFigure 12 presents a performance comparis on of YOLO-v7 with the preceding YOLO \nvariants on the MS COCO dataset. It is clear from Figure 12 that all YOLO-v7 variants \nsurpassed the compared object detectors in a ccuracy and speed in the range of 5\u2013160 FPS. \nIt is, however, important to note, as mentione d by the authors of YOLO-v7, that none of \nthe YOLO-v7 variants are designed for CPU-based mobile device deployment. YOLO-v7-tiny/v7/W6 variants are designed for edge GPU, consumer GPU, an d cloud GPU, respec-\ntively. Whilst YOLO-v7-E6/D6/E6E are designed for high-end cloud GPUs only. \n \nFigure 12. YOLO-v7 comparison vs. other object detectors [72]. \nFigure 11. YOLO-v7 compound scaling.", "start_char_idx": 3130, "end_char_idx": 5465, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2c892c7b-3dc6-4ae9-9787-bec7737985ee": {"__data__": {"id_": "2c892c7b-3dc6-4ae9-9787-bec7737985ee", "embedding": null, "metadata": {"page_label": "13", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "801abaf9-b805-4411-8488-efee2556f926", "node_type": "4", "metadata": {"page_label": "13", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "5145f1a2e2a375b2a97152caee718fe92bc84a15cef1ee9cbea6d8ff2d38dfbf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b53883d1-11f9-4df9-9cde-9cbd73e45c7e", "node_type": "1", "metadata": {"page_label": "12", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "d76c8578600c2393afaafa1c06046bd32c9e3b99ba2331769592edbdd5948e49", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3564a4b7-270f-4311-80e5-71a0933efb0f", "node_type": "1", "metadata": {}, "hash": "65419115ab699c9db5a1a858b6cef9698b4a02e2c63e678e4fb39f7fc0572024", "class_name": "RelatedNodeInfo"}}, "text": "Machines 2023 ,11, 677 13 of 25\nRe-parameterization planning is based on averaging a set of model weights to obtain a\nmore robust network [ 78,79]. Expanding further, module level re-parameterization enables\nsegments of the network to regulate their own parameterization strategies. YOLO-v7\nutilizes gradient \ufb02ow propagation paths with the aim to observe which internal network\nmodules should deploy re-parameterization strategies.\nThe auxiliary head coarse-to-\ufb01ne concept is proposed on the premise that the network\nhead is quite far downstream; therefore, the auxiliary head is deployed at the middle layers\nto assist in the training process. However, this would not train as ef\ufb01ciently as the lead\nhead, due to the former not having access to the complete network.\nFigure 12 presents a performance comparison of YOLO-v7 with the preceding YOLO\nvariants on the MS COCO dataset. It is clear from Figure 12 that all YOLO-v7 variants\nsurpassed the compared object detectors in accuracy and speed in the range of 5\u2013160 FPS .\nIt is, however, important to note, as mentioned by the authors of YOLO-v7, that none of\nthe YOLO-v7 variants are designed for CPU-based mobile device deployment. YOLO-\nv7-tiny/v7/W6 variants are designed for edge GPU, consumer GPU, and cloud GPU,\nrespectively. Whilst YOLO-v7-E6/D6/E6E are designed for high-end cloud GPUs only.\nMachines 2023 , 11, x FOR PEER REVIEW 13 of 26 \n \n  \nFigure 11. YOLO-v7 compound scaling. \nRe-parameterization planning is based on averaging a set of model weights to obtain \na more robust network [78,79]. Expanding further, module level re-parameterization ena-\nbles segments of the network to regulate th eir own parameterization strategies. YOLO-v7 \nutilizes gradient \ufb02ow propagation paths with the aim to observe which internal network \nmodules should deploy re-parameterization strategies. \nThe auxiliary head coarse-to- \ufb01ne concept is proposed on  the premise that the net-\nwork head is quite far downstream; therefore, the auxiliary head is deployed at the middle \nlayers to assist in the training proces s. However, this would not train as e \ufb03ciently as the \nlead head, due to the former not having access to the complete network. \nFigure 12 presents a performance comparis on of YOLO-v7 with the preceding YOLO \nvariants on the MS COCO dataset. It is clear from Figure 12 that all YOLO-v7 variants \nsurpassed the compared object detectors in a ccuracy and speed in the range of 5\u2013160 FPS. \nIt is, however, important to note, as mentione d by the authors of YOLO-v7, that none of \nthe YOLO-v7 variants are designed for CPU-based mobile device deployment. YOLO-v7-tiny/v7/W6 variants are designed for edge GPU, consumer GPU, an d cloud GPU, respec-\ntively. Whilst YOLO-v7-E6/D6/E6E are designed for high-end cloud GPUs only. \n \nFigure 12. YOLO-v7 comparison vs. other object detectors [72]. \nFigure 12. YOLO-v7 comparison vs. other object detectors [72].\nInternal variant comparison of YOLO-v7 is presented in Table 3. As evident, there is a\nsigni\ufb01cant performance gap with respect to mAP when comparing YOLO-v7-tiny with the\ncomputationally demanding YOLO-v7-D6. However, the latter would not be suitable for\nedge deployment onto a computationally constrained device.\nTable 3. Variant comparison of YOLO-v7.\nModel Size (Pixels) mAP (@50) Parameters FLOPs\nYOLO-v7-tiny 640 52.8% 6.2 M 5.8G\nYOLO-v7 640 69.7% 36.9 M 104.7G\nYOLO-v7-X 640 71.1% 71.3 M 189.9G\nYOLO-v7-E6 1280 73.5% 97.2 M 515.2G\nYOLO-v7-D6 1280 73.8% 154.7 M 806.8G", "start_char_idx": 0, "end_char_idx": 3488, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3564a4b7-270f-4311-80e5-71a0933efb0f": {"__data__": {"id_": "3564a4b7-270f-4311-80e5-71a0933efb0f", "embedding": null, "metadata": {"page_label": "14", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "12073eb4-59a6-476f-896b-94c1a65bf9ab", "node_type": "4", "metadata": {"page_label": "14", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "01698d3b0e187b4c5ab458f90ed73f0c75e58969c5c0ca031fc795b3c8462044", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2c892c7b-3dc6-4ae9-9787-bec7737985ee", "node_type": "1", "metadata": {"page_label": "13", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "5145f1a2e2a375b2a97152caee718fe92bc84a15cef1ee9cbea6d8ff2d38dfbf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ec564054-994c-448b-91a9-7d2184f7b475", "node_type": "1", "metadata": {}, "hash": "f028e68f80d6b386e06debb7dae4b11e1f7a4fea4faac25d218220c5a5f972d8", "class_name": "RelatedNodeInfo"}}, "text": "Machines 2023 ,11, 677 14 of 25\n2.8. YOLO-v8\nThe latest addition to the family of YOLO was con\ufb01rmed in January 2023 with the\nrelease of YOLO-v8 [ 80] by Ultralytics (also released YOLO-v5). Although a paper release\nis impending and many features are yet to be added to the YOLO-v8 repository, initial\ncomparisons of the newcomer against its predecessors demonstrate its superiority as the\nnew YOLO state-of-the-art.\nFigure 13 demonstrates that when comparing YOLO-v8 against YOLO-v5 and YOLO-\nv6 trained on 640 image resolution, all YOLO-v8 variants output better throughput with a\nsimilar number of parameters, indicating toward hardware-ef\ufb01cient, architectural reforms.\nThe fact that YOLO-v8 and YOLO-v5 are presented by Ultralytics with YOLO-v5 providing\nimpressive real-time performance and based on the initial benchmarking results released\nby Ultralytics, it is strongly assumed that the YOLO-v8 will be focusing on constrained\nedge device deployment at high-inference speed.\nMachines 2023 , 11, x FOR PEER REVIEW 15 of 26 \n \n  \nFigure 13. YOLO-v8 comparison with predecessors [80]. \n3. Industrial Defect Detection Via YOLO \nThe previous section demonstrates the rapid evolution of the YOLO \u2018clan\u2019 of object \ndetectors amongst the computer vision community. This section of the review focuses on \nthe implementation of YOLO variants for the detection of surface defects within the in-dustrial se tting. The selection of \u2018industrial se tting\u2019 is due to its varying and stringent re-\nquirements alternating between accuracy and speed, a theme which is found through \nDNA of the YOLO variants. \n3.1. Industrial Fabric Defect Detection \nRui Jin et al. [81] in their premise state the ine \ufb03ciencies of manual inspection in the \ntextile manufacturing domain as high cost of  labor, human-related fatigue, and reduced \ndetection speed (less than 20 m/min). The authors aim to address these ine \ufb03ciencies by \nproposing a YOLO-v5-based architecture, coupled with a spatial a ttention mechanism for \naccentuation of smaller defective regions. Th e proposed approach involved a teacher net-\nwork trained on the fabric dataset. Post tr aining of the teacher network, the learned \nweights were distilled to the student network, which was compatible for deployment onto \na Jetson TX2 [82] via TensorRT [83]. The results presented by the authors show, as ex-\npected, that the teacher network reported hi gher performance with an AUC of 98.1% com-\npared to 95.2% (student network). However, as the student network was computationally \nsmaller, the inference time was signi \ufb01cantly less at 16 ms for the student network in con-\ntrast to the teacher network at 35 ms on th e Jetson TX2. Based on the performance, the \nFigure 13. YOLO-v8 comparison with predecessors [80].\n3. Industrial Defect Detection via YOLO\nThe previous section demonstrates the rapid evolution of the YOLO \u2018clan\u2019 of object\ndetectors amongst the computer vision community. This section of the review focuses\non the implementation of YOLO variants for the detection of surface defects within the\nindustrial setting. The selection of \u2018industrial setting\u2019 is due to its varying and stringent\nrequirements alternating between accuracy and speed, a theme which is found through\nDNA of the YOLO variants.", "start_char_idx": 0, "end_char_idx": 3257, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ec564054-994c-448b-91a9-7d2184f7b475": {"__data__": {"id_": "ec564054-994c-448b-91a9-7d2184f7b475", "embedding": null, "metadata": {"page_label": "15", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "09b6b1c1-a608-47bb-978d-70d9e5a34236", "node_type": "4", "metadata": {"page_label": "15", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "716b477782a907d28fc2e5d32238097f1616c607bd933ae6a66594fd173e56c8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3564a4b7-270f-4311-80e5-71a0933efb0f", "node_type": "1", "metadata": {"page_label": "14", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "01698d3b0e187b4c5ab458f90ed73f0c75e58969c5c0ca031fc795b3c8462044", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fc0ef815-7688-4082-a740-dfb5782eb18c", "node_type": "1", "metadata": {}, "hash": "fc8eef985ff2ceb5543018e9c8fb12f7470206d8dc7c0673bcddea69ad62c3ec", "class_name": "RelatedNodeInfo"}}, "text": "Machines 2023 ,11, 677 15 of 25\n3.1. Industrial Fabric Defect Detection\nRui Jin et al. [ 81] in their premise state the inef\ufb01ciencies of manual inspection in the\ntextile manufacturing domain as high cost of labor, human-related fatigue, and reduced\ndetection speed (less than 20 m/min). The authors aim to address these inef\ufb01ciencies by\nproposing a YOLO-v5-based architecture, coupled with a spatial attention mechanism\nfor accentuation of smaller defective regions. The proposed approach involved a teacher\nnetwork trained on the fabric dataset. Post training of the teacher network, the learned\nweights were distilled to the student network, which was compatible for deployment onto\na Jetson TX2 [ 82] via TensorRT [ 83]. The results presented by the authors show, as expected,\nthat the teacher network reported higher performance with an AUC of 98.1% compared to\n95.2% (student network). However, as the student network was computationally smaller,\nthe inference time was signi\ufb01cantly less at 16 ms for the student network in contrast to the\nteacher network at 35 ms on the Jetson TX2. Based on the performance, the authors claim\nthat the proposed solution provides high accuracy and real-time inference speed, making it\ncompatible for deployment via the edge device.\nSifundvoleshile Dlamini et al. [ 84] propose a production environment fabric defect\ndetection framework focused on real-time detection and accurate classi\ufb01cation on-site,\nas shown in Figure 14. The authors embed conventional image processing at the onset\nof their data enhancement strategy, i.e., \ufb01ltering to denoise feature enhancement. Post\naugmentations and data scaling, the authors train the YOLO-v4 architecture based on\npretrained weights. The reported performance was respectable with an F1-score of 93.6%,\nat an impressive detection speed of 34 fps and prediction speed of 21.4 ms. The authors\nclaim that the performance is evident to the effectiveness of the selected architecture for the\ngiven domain.\nMachines 2023 , 11, x FOR PEER REVIEW 16 of 26 \n \n authors claim that the proposed solution prov ides high accuracy and real-time inference \nspeed, making it compatible for deployment via the edge device. \nSifundvoleshile Dlamini et al. [84] propos e a production environment fabric defect \ndetection framework focused on real-time detection and accurate classi \ufb01cation on-site, as \nshown in Figure 14. The authors embed conventional image processing at the onset of \ntheir data enhancement strategy, i.e., \ufb01ltering to denoise feature enhancement. Post aug-\nmentations and data scaling, the authors tr ain the YOLO-v4 architecture based on pre-\ntrained weights. The reported performance was respectable with an F1-score of 93.6%, at \nan impressive detection speed of 34 fps and pr ediction speed of 21.4 ms. The authors claim \nthat the performance is evident to the e \ufb00ectiveness of the selected architecture for the \ngiven domain. \n \nFigure 14. Inspection machine integration [84]. \nRestricted by the available computing reso urces for edge deploy ment, Guijuan Lin et \nal. [85] state problems with quality inspection  in the fabric production domain, including \nminute scale of defects, extreme unbalance wi th the aspect ratio of certain defects, and \nslow defect detection speeds. To address thes e issues, the authors proposed a sliding-win-\ndow, self-a ttention (multihead) mechanism calibrated  for small defect targets. Addition-\nally, the Swin Transformer [86] module as depicted in Figure 15 was integrated into the \noriginal YOLO-v5 architecture for the extraction  of hierarchical features. Furthermore, the \ngeneralized focal loss is implemented with the architecture aimed at improving the learn-ing process for positive target instances, whilst lowering the rate of missed detections. The \nauthors report the accuracy of the proposed so lution on a real-world fabric dataset, reach-\ning 76.5% mAP at 58.8 FPS, making it compatible with the real-time detection require-ments for detection via embedded devices. \nFigure 14. Inspection machine integration [84].\nRestricted by the available computing resources for edge deployment, Guijuan Lin et al.", "start_char_idx": 0, "end_char_idx": 4145, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fc0ef815-7688-4082-a740-dfb5782eb18c": {"__data__": {"id_": "fc0ef815-7688-4082-a740-dfb5782eb18c", "embedding": null, "metadata": {"page_label": "15", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "09b6b1c1-a608-47bb-978d-70d9e5a34236", "node_type": "4", "metadata": {"page_label": "15", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "716b477782a907d28fc2e5d32238097f1616c607bd933ae6a66594fd173e56c8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ec564054-994c-448b-91a9-7d2184f7b475", "node_type": "1", "metadata": {"page_label": "15", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "b33610628cddcdb93bf465f32223fc35cd16ef9f839a43869bbf57e22d10edc9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c2b13b35-b269-4f53-b4c4-599b82d2c7ca", "node_type": "1", "metadata": {}, "hash": "8a5bea46d389eba3e9171f7f0b752af28d6670ea953ae774c9b37e4c23309152", "class_name": "RelatedNodeInfo"}}, "text": "To address thes e issues, the authors proposed a sliding-win-\ndow, self-a ttention (multihead) mechanism calibrated  for small defect targets. Addition-\nally, the Swin Transformer [86] module as depicted in Figure 15 was integrated into the \noriginal YOLO-v5 architecture for the extraction  of hierarchical features. Furthermore, the \ngeneralized focal loss is implemented with the architecture aimed at improving the learn-ing process for positive target instances, whilst lowering the rate of missed detections. The \nauthors report the accuracy of the proposed so lution on a real-world fabric dataset, reach-\ning 76.5% mAP at 58.8 FPS, making it compatible with the real-time detection require-ments for detection via embedded devices. \nFigure 14. Inspection machine integration [84].\nRestricted by the available computing resources for edge deployment, Guijuan Lin et al. [ 85]\nstate problems with quality inspection in the fabric production domain, including minute\nscale of defects, extreme unbalance with the aspect ratio of certain defects, and slow defect\ndetection speeds. To address these issues, the authors proposed a sliding-window, self-\nattention (multihead) mechanism calibrated for small defect targets. Additionally, the Swin\nTransformer [ 86] module as depicted in Figure 15 was integrated into the original YOLO-v5\narchitecture for the extraction of hierarchical features. Furthermore, the generalized focal\nloss is implemented with the architecture aimed at improving the learning process for\npositive target instances, whilst lowering the rate of missed detections. The authors report\nthe accuracy of the proposed solution on a real-world fabric dataset, reaching 76.5% mAP", "start_char_idx": 3269, "end_char_idx": 4966, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c2b13b35-b269-4f53-b4c4-599b82d2c7ca": {"__data__": {"id_": "c2b13b35-b269-4f53-b4c4-599b82d2c7ca", "embedding": null, "metadata": {"page_label": "16", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "883382e7-f13e-4195-99d7-08fc8c05a2de", "node_type": "4", "metadata": {"page_label": "16", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "d2389c2321900bdb7cba804be19c5d839afcb3c078e3962818e0ce39e2a67ec5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fc0ef815-7688-4082-a740-dfb5782eb18c", "node_type": "1", "metadata": {"page_label": "15", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "a3975e8b6227dda3b2aea28ba6501f17174cfbb1642651992ae8e448bad935ad", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7a8fa370-76a5-48b2-84b9-f22e8077bfe1", "node_type": "1", "metadata": {}, "hash": "8ba0b09c9c0f1428f70680ee97c698be4430f4c35cf13c168c785680ae8002a5", "class_name": "RelatedNodeInfo"}}, "text": "Machines 2023 ,11, 677 16 of 25\nat 58.8 FPS, making it compatible with the real-time detection requirements for detection\nvia embedded devices.\nMachines 2023 , 11, x FOR PEER REVIEW 17 of 26 \n \n  \nFigure 15. Backbone for Swin Transformer network [85]. \n3.2. Solar Cell Surface Defect Detection \nSetting their premise, the authors [87] state that human-led Photovoltaic (PV) inspec-\ntion has many drawbacks including the re quirement of operation and maintenance \n(O&M) engineers, cell-by-cell inspec tion, high workload, and reduced e \ufb03ciency. The au-\nthors propose an improved architecture based on YOLO-v5 for the characterization of complex solar cell surface textures and defect ive regions. The proposal is based on the \nintegration of deformable convolution within  the CSP module with the aim of achieving \nan adaptive learning scale. Additionally, an a ttention mechanism is incorporated for en-\nhanced feature extraction. Moreover, the authors optimize the original YOLO-v5 architec-\nture further via K-means++ clustering for an chor box determination algorithm. Based on \nthe presented results, the improved architectu re achieved a respectable mAP of 89.64% on \nan EL-based solar cell image dataset, 7.85% higher compared to mAP for the original ar-\nchitecture, with detection speed reaching 36.24 FPS, which can be translated as a more accurate detection while remaining compatible with the real-time requirements. \nAmran Binomairah et al. [88]  highlight two frequent defects encountered during the \nmanufacturing process of crystalline solar cell s as dark spot/region and microcracks. The \nlatter can have a detrimental impact on the performance of the module, which is a major \ncause for PV module failures. The authors su bscribe to the YOLO architecture, comparing \nthe performance of their methodology on YOLO-v4 and an improved YOLO-v4-tiny inte-\ngrated with a spatial pyramid pooling mechanism. Based on the presented results, YOLO-\nv4 achieved 98.8% mAP at 62.9 ms, whilst the improved YOLO-v4-tiny lagged with 91% \nmAP at 28.2 ms. The authors claim that although the la tter is less accurate, it is notably \nfaster than the former. \nTianyi Sun et al. [89] focus on automated hot-spot detection within PV cells based a \nmodi \ufb01ed version of the YOLO-v5 architecture. The \ufb01rst improvement comes in the form \nof enhanced anchors and detection heads for the respective architecture. To improve the \ndetection precision at varying scales, k-means clustering [48] is deployed for clustering \nthe length\u2013width ratio with respect to the data  annotation frame. Additionally, a set of the \nanchors consisting of smaller values were adde d to cater for the detection of small defects \nby optimizing the cluster numb er. The reported performance of the improved architecture \nwas reported as 87.8% mAP, with the average recall rate of 89.0% and F1-score reaching \n88.9%. The reported FPS was impressive reaching  98.6 FPS, with the authors claiming that \nthe proposed solution would provide intellige nt monitoring at PV power stations. Infer-\nencing output presented in Figure 16 sh ows the proposed AP-YOLO-v5 architecture, \nproviding inferences at a higher con \ufb01dence level compared to the original YOLO-v5. \nFigure 15. Backbone for Swin Transformer network [85].\n3.2. Solar Cell Surface Defect Detection\nSetting their premise, the authors [ 87] state that human-led Photovoltaic (PV) inspec-\ntion has many drawbacks including the requirement of operation and maintenance (O&M)\nengineers, cell-by-cell inspection, high workload, and reduced ef\ufb01ciency. The authors\npropose an improved architecture based on YOLO-v5 for the characterization of complex\nsolar cell surface textures and defective regions. The proposal is based on the integration\nof deformable convolution within the CSP module with the aim of achieving an adaptive\nlearning scale. Additionally, an attention mechanism is incorporated for enhanced feature\nextraction.", "start_char_idx": 0, "end_char_idx": 3943, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7a8fa370-76a5-48b2-84b9-f22e8077bfe1": {"__data__": {"id_": "7a8fa370-76a5-48b2-84b9-f22e8077bfe1", "embedding": null, "metadata": {"page_label": "16", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "883382e7-f13e-4195-99d7-08fc8c05a2de", "node_type": "4", "metadata": {"page_label": "16", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "d2389c2321900bdb7cba804be19c5d839afcb3c078e3962818e0ce39e2a67ec5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c2b13b35-b269-4f53-b4c4-599b82d2c7ca", "node_type": "1", "metadata": {"page_label": "16", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "56cf6d197fb94609838713ff8a1394b3bc2bad4424bfb807f8c2b0f7e8ce6a70", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1e10b9a7-5cf8-444b-9c18-cee7ba2db12a", "node_type": "1", "metadata": {}, "hash": "d9095ad2624688b23833dd5322e687ff641b35203b7b05badacd825b9cca2b9d", "class_name": "RelatedNodeInfo"}}, "text": "Figure 15. Backbone for Swin Transformer network [85].\n3.2. Solar Cell Surface Defect Detection\nSetting their premise, the authors [ 87] state that human-led Photovoltaic (PV) inspec-\ntion has many drawbacks including the requirement of operation and maintenance (O&M)\nengineers, cell-by-cell inspection, high workload, and reduced ef\ufb01ciency. The authors\npropose an improved architecture based on YOLO-v5 for the characterization of complex\nsolar cell surface textures and defective regions. The proposal is based on the integration\nof deformable convolution within the CSP module with the aim of achieving an adaptive\nlearning scale. Additionally, an attention mechanism is incorporated for enhanced feature\nextraction. Moreover, the authors optimize the original YOLO-v5 architecture further via\nK-means++ clustering for anchor box determination algorithm. Based on the presented\nresults, the improved architecture achieved a respectable mAP of 89.64% on an EL-based\nsolar cell image dataset, 7.85% higher compared to mAP for the original architecture, with\ndetection speed reaching 36.24 FPS, which can be translated as a more accurate detection\nwhile remaining compatible with the real-time requirements.\nAmran Binomairah et al. [ 88] highlight two frequent defects encountered during\nthe manufacturing process of crystalline solar cells as dark spot/region and microcracks.\nThe latter can have a detrimental impact on the performance of the module, which is\na major cause for PV module failures. The authors subscribe to the YOLO architecture,\ncomparing the performance of their methodology on YOLO-v4 and an improved YOLO-v4-\ntiny integrated with a spatial pyramid pooling mechanism. Based on the presented results,\nYOLO-v4 achieved 98.8% mAP at 62.9 ms, whilst the improved YOLO-v4-tiny lagged with\n91% mAP at 28.2 ms. The authors claim that although the latter is less accurate, it is notably\nfaster than the former.\nTianyi Sun et al. [ 89] focus on automated hot-spot detection within PV cells based a\nmodi\ufb01ed version of the YOLO-v5 architecture. The \ufb01rst improvement comes in the form\nof enhanced anchors and detection heads for the respective architecture. To improve the\ndetection precision at varying scales, k-means clustering [ 48] is deployed for clustering the\nlength\u2013width ratio with respect to the data annotation frame. Additionally, a set of the\nanchors consisting of smaller values were added to cater for the detection of small defects\nby optimizing the cluster number. The reported performance of the improved architecture\nwas reported as 87.8% mAP , with the average recall rate of 89.0% and F1-score reaching", "start_char_idx": 3223, "end_char_idx": 5857, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1e10b9a7-5cf8-444b-9c18-cee7ba2db12a": {"__data__": {"id_": "1e10b9a7-5cf8-444b-9c18-cee7ba2db12a", "embedding": null, "metadata": {"page_label": "17", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2aed3d7f-8075-4833-a35e-7b41d917fbdd", "node_type": "4", "metadata": {"page_label": "17", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "8eb3a08f7d5aef1f33b8845b7f443ea283ed3d87fe0ee0082e7f49a161653295", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7a8fa370-76a5-48b2-84b9-f22e8077bfe1", "node_type": "1", "metadata": {"page_label": "16", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "2bb35f838b372741dd1cd860f5de86aec2e7568c40f5858ab2cc7e5b433f1e83", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dbb96f28-a554-41b5-9791-74ee7bdc13ef", "node_type": "1", "metadata": {}, "hash": "a06ca1da70242abfb532f90f3276cf5f983f6f00be5d18a2d3dd622f16f178f5", "class_name": "RelatedNodeInfo"}}, "text": "Machines 2023 ,11, 677 17 of 25\n88.9%. The reported FPS was impressive reaching 98.6 FPS, with the authors claiming\nthat the proposed solution would provide intelligent monitoring at PV power stations.\nInferencing output presented in Figure 16 shows the proposed AP-YOLO-v5 architecture,\nproviding inferences at a higher con\ufb01dence level compared to the original YOLO-v5.\nMachines 2023 , 11, x FOR PEER REVIEW 18 of 26 \n \n  \nFigure 16. inference/con \ufb01dence comparison [89]. \n3.3. Steel Surface Defect Detection \nDinming Yang et al. [90] set the premise of  their research by stating the importance \nof steel pipe quality inspection, citing the growing demand in countries, such as China. \nAlthough X-ray testing is utilized as one of the key methods for industrial nondestructive \ntesting (NDT), the authors state that it still requires human assistance for the determina-\ntion, classi \ufb01cation, and localization of the defect s. The authors propose the implementa-\ntion of YOLO-v5 for production-based weld st eel defect detection based on X-ray images \nof the weld pipe. The authors claim that the trained YOLO-v5 reached a mAP of 98.7% \n(IoU-0.5), whilst meeting the real-time detect ion requirements of steel pipe production \nwith a single image detection rate of 0.12 s. \nZhuxi MA et al. [91] address the issu e of large-scale computation and speci \ufb01c hard-\nware requirements for automated defect detection in aluminum strips. The authors select \nYOLO-v4 as the architecture, whilst the back bone is constructed to make use of depth-\nwise separable convolutions along with a parallel dual a ttention mechanism for feature \nenhancement, as shown in Figure 17. The propos ed network is tested on real data from a \ncold-rolling workshop, providing impressive results on real data achieving an mAP of \n96.28%. Compared to the original YOLO-v4, the authors claim that the proposed architec-\nture volume is reduced by 83.38%, whilst the inference speed is increased by a factor of \nthree. The increase in performance was pa rtly due to the custom anchor approach, \nwhereby due to the maximum aspect ratio of the custom dataset, the defect was set to 1:20 \nwhich is in-line with the defect characteristics, such as scratch marks. \nFigure 16. Inference/con\ufb01dence comparison [89].\n3.3. Steel Surface Defect Detection\nDinming Yang et al. [ 90] set the premise of their research by stating the importance\nof steel pipe quality inspection, citing the growing demand in countries, such as China.\nAlthough X-ray testing is utilized as one of the key methods for industrial nondestructive\ntesting (NDT), the authors state that it still requires human assistance for the determination,\nclassi\ufb01cation, and localization of the defects. The authors propose the implementation of\nYOLO-v5 for production-based weld steel defect detection based on X-ray images of the\nweld pipe. The authors claim that the trained YOLO-v5 reached a mAP of 98.7% (IoU-0.5),\nwhilst meeting the real-time detection requirements of steel pipe production with a single\nimage detection rate of 0.12 s.\nZhuxi MA et al. [ 91] address the issue of large-scale computation and speci\ufb01c hard-\nware requirements for automated defect detection in aluminum strips. The authors select\nYOLO-v4 as the architecture, whilst the backbone is constructed to make use of depth-\nwise separable convolutions along with a parallel dual attention mechanism for feature\nenhancement, as shown in Figure 17. The proposed network is tested on real data from\na cold-rolling workshop, providing impressive results on real data achieving an mAP of\n96.28%. Compared to the original YOLO-v4, the authors claim that the proposed architec-\nture volume is reduced by 83.38%, whilst the inference speed is increased by a factor of\nthree. The increase in performance was partly due to the custom anchor approach, whereby\ndue to the maximum aspect ratio of the custom dataset, the defect was set to 1:20 which is\nin-line with the defect characteristics, such as scratch marks.", "start_char_idx": 0, "end_char_idx": 3998, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dbb96f28-a554-41b5-9791-74ee7bdc13ef": {"__data__": {"id_": "dbb96f28-a554-41b5-9791-74ee7bdc13ef", "embedding": null, "metadata": {"page_label": "18", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "95fa3d9e-798b-4cce-90e9-16caeecf2d2c", "node_type": "4", "metadata": {"page_label": "18", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "1d81436903451d21354cdc076fe1167ee5e936d53240bc609a41e5c8a655e71e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1e10b9a7-5cf8-444b-9c18-cee7ba2db12a", "node_type": "1", "metadata": {"page_label": "17", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "8eb3a08f7d5aef1f33b8845b7f443ea283ed3d87fe0ee0082e7f49a161653295", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4be0ac67-5599-4c5e-9d1f-13cc13249d39", "node_type": "1", "metadata": {}, "hash": "d2da122b683ef86dc81431e3962c2f2c3195bcf3541f877c461447488453cbd7", "class_name": "RelatedNodeInfo"}}, "text": "Machines 2023 ,11, 677 18 of 25\nMachines 2023 , 11, x FOR PEER REVIEW 19 of 26 \n \n  \nFigure 17. Proposed parallel network structure [91]. \nJianting Shi et al. [92] cite the manufacturin g process of steel production as the reason \nfor various defects originating on the steel surface, such as rolling scale and patches. The \nauthors state that the small dimensions of the defects as well as the stringent detection \nrequirements make the quality inspection process a challenging task. Therefore, the au-\nthors present an improved version of YOLO-v5 by incorporating an a ttention mechanism \nfor facilitating the transmission of shallow features from the backbone to the neck, pre-\nserving the defective regions, in addition to k-means clustering of anchor boxes for ad-\ndressing the extreme aspect ratios of defectiv e targets within the dataset. The authors state \nthat the improved architecture achieved 86.35% mAP reaching 45 FPS detection speed, whilst the original architecture achieved 81.78% mAP at 52 FPS. \n3.4. Pallet Racking Defect Inspection \nA promising application with signi \ufb01cant deployment scope in the warehousing and \ngeneral industrial storage centers is automated pallet racking inspection. Warehouses and \ndistribution centers host a crit ical infrastructure known as racking for stock storage. Un-\nnoticed damage to pallet racking can pave the way for signi \ufb01cant losses initiated by rack-\ning collapse leading to wasted/damaged stock, \ufb01nancial implications, operational losses, \ninjured employees, and worst-case, loss of lives [93]. Due to the ine \ufb03ciencies of the con-\nventional racking inspection mechanisms, such  as human-led annual inspection resulting \nin labor costs, bias, fatigue, and mechanical pr oducts, such as rackguar ds [94] lacking clas-\nsi\ufb01cation intelligence, CNN-based automated de tection seems to be a promising alterna-\ntive. \nRealizing the potential, Hussain et al. [95] inaugurated research into automated pallet \nracking detection via computer vision. After presenting their initial research based on the \nMobileNet-V2 architecture, the authors recent ly proposed the implementation of YOLO-\nv7 for automated pallet racking inspection [96]. The selection of the architecture was in-\nline with the stringent requirements of production \ufb02oor deployment, i.e., edge device de-\nployment, placed onto an oper ating forklift, requiring real-time detection as the forklift \napproaches the racking. Evaluating the perfor mance of the proposed solution on a real \ndataset, the authors claimed an impressive pe rformance of 91.1% mAP running at 19 FPS. \nFigure 17. Proposed parallel network structure [91].\nJianting Shi et al. [ 92] cite the manufacturing process of steel production as the reason\nfor various defects originating on the steel surface, such as rolling scale and patches. The\nauthors state that the small dimensions of the defects as well as the stringent detection\nrequirements make the quality inspection process a challenging task. Therefore, the authors\npresent an improved version of YOLO-v5 by incorporating an attention mechanism for\nfacilitating the transmission of shallow features from the backbone to the neck, preserving\nthe defective regions, in addition to k-means clustering of anchor boxes for addressing\nthe extreme aspect ratios of defective targets within the dataset. The authors state that the\nimproved architecture achieved 86.35% mAP reaching 45 FPS detection speed, whilst the\noriginal architecture achieved 81.78% mAP at 52 FPS.\n3.4. Pallet Racking Defect Inspection\nA promising application with significant deployment scope in the warehousing\nand general industrial storage centers is automated pallet racking inspection. Ware-\nhouses and distribution centers host a critical infrastructure known as racking for stock\nstorage. Unnoticed damage to pallet racking can pave the way for significant losses\ninitiated by racking collapse leading to wasted/damaged stock, financial implications,\noperational losses, injured employees, and worst-case, loss of lives [ 93]. Due to the\ninefficiencies of the conventional racking inspection mechanisms, such as human-led\nannual inspection resulting in labor costs, bias, fatigue, and mechanical products, such\nas rackguards [ 94] lacking classification intelligence, CNN-based automated detection\nseems to be a promising alternative.", "start_char_idx": 0, "end_char_idx": 4352, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4be0ac67-5599-4c5e-9d1f-13cc13249d39": {"__data__": {"id_": "4be0ac67-5599-4c5e-9d1f-13cc13249d39", "embedding": null, "metadata": {"page_label": "18", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "95fa3d9e-798b-4cce-90e9-16caeecf2d2c", "node_type": "4", "metadata": {"page_label": "18", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "1d81436903451d21354cdc076fe1167ee5e936d53240bc609a41e5c8a655e71e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dbb96f28-a554-41b5-9791-74ee7bdc13ef", "node_type": "1", "metadata": {"page_label": "18", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "2dba12628f82b6252dbf4da3165b6384cba7c304e2381ad33cb27629fd444ece", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2b1cfe90-f9d0-41ac-a129-46d147ad209d", "node_type": "1", "metadata": {}, "hash": "235f4190c73b122e6cfa42cec8935c6810af6b92dea2ddcf2686f2162dfbc1fd", "class_name": "RelatedNodeInfo"}}, "text": "3.4. Pallet Racking Defect Inspection\nA promising application with significant deployment scope in the warehousing\nand general industrial storage centers is automated pallet racking inspection. Ware-\nhouses and distribution centers host a critical infrastructure known as racking for stock\nstorage. Unnoticed damage to pallet racking can pave the way for significant losses\ninitiated by racking collapse leading to wasted/damaged stock, financial implications,\noperational losses, injured employees, and worst-case, loss of lives [ 93]. Due to the\ninefficiencies of the conventional racking inspection mechanisms, such as human-led\nannual inspection resulting in labor costs, bias, fatigue, and mechanical products, such\nas rackguards [ 94] lacking classification intelligence, CNN-based automated detection\nseems to be a promising alternative.\nRealizing the potential, Hussain et al. [ 95] inaugurated research into automated pallet\nracking detection via computer vision. After presenting their initial research based on the\nMobileNet-V2 architecture, the authors recently proposed the implementation of YOLO-\nv7 for automated pallet racking inspection [ 96]. The selection of the architecture was\nin-line with the stringent requirements of production \ufb02oor deployment, i.e., edge device\ndeployment, placed onto an operating forklift, requiring real-time detection as the forklift", "start_char_idx": 3508, "end_char_idx": 4888, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2b1cfe90-f9d0-41ac-a129-46d147ad209d": {"__data__": {"id_": "2b1cfe90-f9d0-41ac-a129-46d147ad209d", "embedding": null, "metadata": {"page_label": "19", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d8877978-9e4a-4524-9da1-dde2d09bb59e", "node_type": "4", "metadata": {"page_label": "19", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "198b8ef76d69b467480802ac7dff43ca9f9334cabbb2d8bc51ebe07839b4f3d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4be0ac67-5599-4c5e-9d1f-13cc13249d39", "node_type": "1", "metadata": {"page_label": "18", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "4e05115d6d401255138e6c42ae584a177ab916418e0d4949f03d3e7f45a9f616", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "094ef49a-395e-4575-8d39-13c15a1c34d7", "node_type": "1", "metadata": {}, "hash": "913f77e061827f22dfc1f771e4a6eed56228f8d6420ee142307bf1ef8e7d3539", "class_name": "RelatedNodeInfo"}}, "text": "Machines 2023 ,11, 677 19 of 25\napproaches the racking. Evaluating the performance of the proposed solution on a real\ndataset, the authors claimed an impressive performance of 91.1% mAP running at 19 FPS.\nTable 4 presents a comparison of the present research in this emerging \ufb01eld. Although\nmask R-CNN presents the highest accuracy, which is a derivative of the segmentation\nfamily of architectures with signi\ufb01cant computational load, this makes it an infeasible\noption for deployment. Whereas the proposed approach utilizing YOLO-v7 achieved\nsimilar accuracy compared to MobileNet-V2, whilst requiring signi\ufb01cantly less training\ndata along with inferencing at 19 FPS.\nTable 4. Racking domain research comparison.\nResearch Architecture Dataset Size Accuracy FPS\n[95] MobileNet-V2 19,717 92.7% -----\n[96] YOLO-v7 2095 91.1% 19\n[97] Mask-RCNN 75 93.45% -----\n4. Discussion\nThe YOLO family of object detectors has had a signi\ufb01cant impact on improving the\npotential of computer vision applications. Right from the onset, i.e., the release of the\nYOLO-v1 in 2015, signi\ufb01cant breakthroughs were introduced. YOLO-v1 became the \ufb01rst\narchitecture combining the two conventionally separate tasks of bounding box prediction\nand classi\ufb01cation into one. YOLO-v2 was released in the following year, introducing archi-\ntectural improvements and iterative improvements, such as batch normalization, higher\nresolution, and anchor boxes. In 2018, YOLO-v3 was released, an extension of previous\nvariants with enhancements including the introduction of objectness scores for bounding\nbox predictions added connections for the backbone layers and the ability to generate\npredictions at three different levels of granularity, leading to improved performance on\nsmaller object targets.\nAfter a short delay, YOLO-v4 was released in April 2020, becoming the \ufb01rst variant of\nthe YOLO family not to be authored by the original author Joseph Redmon. Enhancements\nincluded improved feature aggregation, gifting of the \u2018bag of freebies\u2019, and the mish\nactivation. In a matter of months, YOLO-v5 entered the computer vision territory, becoming\nthe \ufb01rst variant to be released without being accompanied by a paper release. YOLO-v5\nbased on PyTorch, with an active GitHub repo further delineated the implementation\nprocess, make it accessible to a wider audience. Focused on internal architectural reforms,\nYOLO-v6 authors redesigned the backbone (Ef\ufb01cientRep) and neck (Rep-PAN) modules,\nwith an inclination toward hardware ef\ufb01ciency. Additionally, anchor-free and the concept\nof decoupled head was introduced, implying additional layers for feature separation from\nthe \ufb01nal head, which is empirically shown to improve the overall performance. The authors\nof YOLO-v7 also focused on architectural reforms, considering the amount of memory\nrequired to keep layers within memory and the distance required for gradients to back-\npropagate, i.e., shorter gradients, resulting in enhanced learning capacity. For the ultimate\nlayer aggregation, the authors implemented E-ELAN, which is an extension of the ELAN\ncomputational block. The advent of 2023 introduced the latest version of the YOLO family,\nYOLO-v8, which was released by Ultralytics. With an impending paper release, initial\ncomparisons of the latest version against predecessors have shown promising performance\nwith respect to throughput when compared to similar computational parameters.\n4.1. Reason for Rising Popularity\nTable 5 presents a summary of the reviewed YOLO variants based on the underlying\nframework, backbone, average-precision (AP), and key contributions. It can be observed\nfrom Table 3 that as the variants evolved there was a shift from the conservative Darknet\nframework to a more accessible one, i.e., PyTorch. The AP presented here is based on\nCOCO-2017 [ 63] with the exception of YOLO-v1/v2, which are based on VOC-2017 [ 39].", "start_char_idx": 0, "end_char_idx": 3874, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "094ef49a-395e-4575-8d39-13c15a1c34d7": {"__data__": {"id_": "094ef49a-395e-4575-8d39-13c15a1c34d7", "embedding": null, "metadata": {"page_label": "20", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "66961e48-a891-4a3a-b1da-5b410e742284", "node_type": "4", "metadata": {"page_label": "20", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "1ee182610232113ad32760323748d95671b1e578fc75afa233de82a9e13a8ced", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2b1cfe90-f9d0-41ac-a129-46d147ad209d", "node_type": "1", "metadata": {"page_label": "19", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "198b8ef76d69b467480802ac7dff43ca9f9334cabbb2d8bc51ebe07839b4f3d5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "04a7ab6a-e449-49c2-88ff-551eaf09a7e5", "node_type": "1", "metadata": {}, "hash": "46d55a1ed13e282dd24df51a1289a58e848ec867fd28fd05112a1d425ffbe000", "class_name": "RelatedNodeInfo"}}, "text": "Machines 2023 ,11, 677 20 of 25\nCOCO-2017 [ 63] consists of over 80 objects designed to represent a vast array of regularly\nseen object. It contains 121,408 images resulting in 883,331 object annotations with median\nimage ratio of 640 \u00d7480 pixels. It is important to note that the overall accuracy along with\ninference capacity depends on the deployed design/training strategies, as demonstrated in\nthe industrial surface detection section.\nTable 5. Abstract variant comparison.\nVariant Framework Backbone AP (%) Comments\nV1 Darknet Darknet-24 63.4 Only detect a maximum of two objects in the same grid.\nV2 Darknet Darknet-24 63.4Introduced batch norm, k-means clustering for anchor boxes.\nCapable of detecting > 9000 categories.\nV3 Darknet Darknet-53 36.2Utilized multi-scale predictions and spatial pyramid pooling\nleading to larger receptive \ufb01eld.\nV4 Darknet CSPDarknet-53 43.5 Presented bag-of-freebies including the use of CIoU loss.\nV5 PyTorch Modi\ufb01ed CSPv7 55.8First variant based in PyTorch, making it available to a wider\naudience. Incorporated the anchor selection processes into\nthe YOLO-v5 pipeline.\nV6 PyTorch Ef\ufb01cientRep 52.5Focused on industrial settings, presented an anchor-free\npipeline. Presented new loss determination mechanisms\n(VFL, DFL, and SIoU/GIoU).\nV7 PyTorch RepConvN 56.8Architectural introductions included E-ELAN for faster\nconvergence along with a bag-of-freebies including\nRepConvN and reparameterization-planning.\nV8 PyTorch YOLO-v8 53.9Anchor-free reducing the number of prediction boxes whilst\nspeeding up non-maximum suppression. Pending paper for\nfurther architectural insights.\nThe AP metric consists of precision-recall (PR) metrics, de\ufb01ning of a positive prediction\nusing Intersection over Union, and the handling of multiple object categories. AP provides\na balanced overview of PR based on the area under the PR curve. IoU facilitates the\nquanti\ufb01cation of similarity between predicted kpand ground truth kgbounding boxes as\nexpressed in (8):\nIoU =area(\nkp\u2229kg)\narea(\nkp\u222akg) (8)\nThe rise of YOLO can be attributed to two factors. First, the fact that the architectural\ncomposition of YOLO variants is compatible for one-stage detection and classi\ufb01cation\nmakes it computationally lightweight with respect to other detectors. However, we feel\nthat ef\ufb01cient architectural composition by itself did not drive the popularity of the YOLO\nvariants, as other single-stage detectors, such as MobileNets, also serve a similar purpose.\nThe second reason is the accessibility factor, which was introduced as the YOLO\nvariants progressed, with YOLO-v5 being the turning point. Expanding further on this\npoint, the \ufb01rst two variants were based on the Darknet framework. Although this pro-\nvided a degree of \ufb02exibility, accessibility was limited to a smaller user base due to the\nrequired expertise. Ultralytics, introduced YOLO-v5 based on the PyTorch framework,\nmaking the architecture available for a wider audience and increasing the potential domain\nof applications.\nAs evident from Table 6, the migration to a more accessible framework coupled with\narchitectural reforms for improved real-time performance sky-rocketed. At present, YOLO-\nv5 has 34.7 k stars, a signi\ufb01cant lead compared to its predecessors. From implementation,\nYOLO-v5 only required the installation of lightweight python libraries. The architectural\nreforms indicated that the model training time was reduced, which in turn reduced the ex-\nperimentation cost attributed to the training process, i.e., GPU utilization. For deployment\nand testing purposes, researchers have several routes, such as individual/batch images,\nvideo/webcam feeds, in addition to simple weight conversion to ONXX weights for edge\ndevice deployment.", "start_char_idx": 0, "end_char_idx": 3725, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "04a7ab6a-e449-49c2-88ff-551eaf09a7e5": {"__data__": {"id_": "04a7ab6a-e449-49c2-88ff-551eaf09a7e5", "embedding": null, "metadata": {"page_label": "21", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "35a17c4d-a332-4238-894a-aa4d925b0281", "node_type": "4", "metadata": {"page_label": "21", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "326c08fcb87972494fbc221349dfae83e337857816290013da18993b048fc9bf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "094ef49a-395e-4575-8d39-13c15a1c34d7", "node_type": "1", "metadata": {"page_label": "20", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "1ee182610232113ad32760323748d95671b1e578fc75afa233de82a9e13a8ced", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "084b76d2-9d47-49fe-a704-b3176e748bac", "node_type": "1", "metadata": {}, "hash": "c8b7d60cd4e4198045ad7bb1c854ebea67f2b125f7bf310a1eb309f0c2cb5059", "class_name": "RelatedNodeInfo"}}, "text": "Machines 2023 ,11, 677 21 of 25\nTable 6. GitHub popularity comparison.\nYOLO Variant Stars (K)\nV3 9.3\nV4 20.2\nV5 34.7\nV6 4.6\nV7 8.4\nV8 2.9\n4.2. YOLO and Industrial Defect Detection\nManifestations of the fourth industrial revolution can be observed at present in an\nad-hoc manner, spanning across various industries. With respect to the manufacturing\nindustry, this revolution can be targeted at the quality inspection processes, which are\nvital for assuring ef\ufb01ciency and retaining client satisfaction. When focusing on surface\ndefect detection, as alluded to earlier, the inspection requirements can be more stringent\nas compared to other applications. This is due to many factors, such as the fact that the\ndefects may be extremely small, requiring external spectral imaging to expose defects prior\nto classi\ufb01cation and due to the fact that the operational setting of the production line may\nonly provide a small-time window within which inference must be carried out.\nConsidering the stringent requirements outlined above and benchmarking against the\nprinciples of YOLO family of variants, forms the conclusion that the YOLO variants have the\npotential to address both real-time, constrained deployment and small-scale defect detec-\ntion requirements of industrial-based surface defect detection. YOLO variants have proven\nreal-time compliance in several industrial environments as shown in [81,84,85,90,95] . An\ninteresting observation arising from the industrial literature reviewed is the ability for users\nto modify the internal modules of YOLO variants in order to take care of their speci\ufb01c ap-\nplication needs without compromising on real-time compliance, for example [81,87,91,92] ,\nintroducing attention-mechanisms for accentuation of defective regions.\nAn additional factor, found within the later YOLO variants is sub-variants for each\nbase architecture, i.e., for YOLO-v5 variants including YOLO-v5-S/M/L, this corresponds\nto different computational loads with respect to the number of parameters. This \ufb02exibility\nenables researchers to consider a more \ufb02exible approach with the architecture selection\ncriteria based on the industrial requirements, i.e., if real-time inference is required with less\nemphasis on optimal mAP , a lightweight variant can be selected, such as YOLO-v5-small\nrather than YOLO-v5-large.\n5. Conclusions\nIn conclusion, this work is the \ufb01rst of its type focused on documenting and reviewing\nthe evolution of the most prevalent single-stage object detector within the computer vision\ndomain. The review presents the key advancements of each variant, followed by imple-\nmentation of YOLO architectures within various industrial settings focused on surface\nautomated real-time surface defect detection.\nFrom the review, it is clear as the YOLO variants have progressed, latter versions in\nparticular, YOLO-v5 has focused on constrained edge deployment, a key requirement for\nmany manufacturing applications. Due to the fact that there is no copyright and patent\nrestrictions, research anchored around the YOLO architecture, i.e., real-time, lightweight,\naccurate detection, can be conducted by any individual or research organization, which has\nalso contributed to the prevalence of this variant.\nWith YOLO-v8 released in January 2023, showing promising performance with respect\nto throughput and computational load requirements, it is envisioned that 2023 will see\nmore variants released by previous or new authors focused on improving the deployment\ncapacity of the architectures with respect to constrained deployment environments.\nWith research organizations, such as Ultralytics and Meituan Technical Team taking\na keen interest in the development of YOLO architectures with a focus on edge-friendly", "start_char_idx": 0, "end_char_idx": 3739, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "084b76d2-9d47-49fe-a704-b3176e748bac": {"__data__": {"id_": "084b76d2-9d47-49fe-a704-b3176e748bac", "embedding": null, "metadata": {"page_label": "22", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d3aec5d3-5d75-4830-90fd-a7ac97c2f698", "node_type": "4", "metadata": {"page_label": "22", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "867cfc2154096e43d158c622debe1a67a06232c507b5ed5f8481cf4a38cb1b70", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "04a7ab6a-e449-49c2-88ff-551eaf09a7e5", "node_type": "1", "metadata": {"page_label": "21", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "326c08fcb87972494fbc221349dfae83e337857816290013da18993b048fc9bf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "344844a4-ac2d-4848-ba1f-ad29e005d00b", "node_type": "1", "metadata": {}, "hash": "ee1812e88401f1a27fdefa5b8a5e4ebd32449533133dc44f38f163e731e0cb59", "class_name": "RelatedNodeInfo"}}, "text": "Machines 2023 ,11, 677 22 of 25\ndeployment, we anticipate further technological advancements in the architectural footprint\nof YOLO. To cater for constrained deployment, these advancements will need to focus on\nenergy conservation whilst maintaining high inference rates. Furthermore, we envision\nthe proliferation of YOLO architectures into production facilities to help with quality\ninspection pipelines as well as providing stimulus for innovative products as demonstrated\nby [96] with an automated pallet racking inspection solution. Along with integration\ninto a diverse set of hardware and IoT devices, YOLO has the potential to tap into new\ndomains where computer vision can assist in enhancing existing processes whilst requiring\nlimited resources.\nFunding: This research received no external funding.\nData Availability Statement: Not applicable.\nCon\ufb02icts of Interest: The authors declare no con\ufb02ict of interest.\nReferences\n1. Zhang, B.; Quan, C.; Ren, F. Study on CNN in the recognition of emotion in audio and images. In Proceedings of the 2016\nIEEE/ACIS 15th International Conference on Computer and Information Science (ICIS), Okayama, Japan, 26\u201329 June 2016.\n[CrossRef]\n2. Pollen, D.A. Explicit neural representations, recursive neural networks and conscious visual perception. Cereb. Cortex 2003 ,13,\n807\u2013814. [CrossRef] [PubMed]\n3. Using arti\ufb01cial neural networks to understand the human brain. Res. Featur. 2022 . [CrossRef]\n4. Improvement of Neural Networks Arti\ufb01cial Output. Int. J. Sci. Res. (IJSR) 2017 ,6, 352\u2013361. [CrossRef]\n5. Dodia, S.; Annappa, B.; Mahesh, P .A. Recent advancements in deep learning based lung cancer detection: A systematic review.\nEng. Appl. Artif. Intell. 2022 ,116, 105490. [CrossRef]\n6. Ojo, M.O.; Zahid, A. Deep Learning in Controlled Environment Agriculture: A Review of Recent Advancements, Challenges and\nProspects. Sensors 2022 ,22, 7965. [CrossRef] [PubMed]\n7. Jarvis, R.A. A Perspective on Range Finding Techniques for Computer Vision. IEEE Trans. Pattern Anal. Mach. Intell. 1983 ,P AMI-5 ,\n122\u2013139. [CrossRef]\n8. Hussain, M.; Bird, J.; Faria, D.R. A Study on CNN Transfer Learning for Image Classi\ufb01cation. 11 August 2018. Available online:\nhttps://research.aston.ac.uk/en/publications/a-study-on-cnn-transfer-learning-for-image-classi\ufb01cation (accessed on 1 January\n2023).\n9. Yang, R.; Yu, Y. Arti\ufb01cial Convolutional Neural Network in Object Detection and Semantic Segmentation for Medical Imaging\nAnalysis. Front. Oncol. 2021 ,11, 638182. [CrossRef]\n10. Haupt, J.; Nowak, R. Compressive Sampling vs. Conventional Imaging. In Proceedings of the 2006 International Conference on\nImage Processing, Las Vegas, NV , USA, 26\u201329 June 2006; pp. 1269\u20131272. [CrossRef]\n11. Gu, J.; Wang, Z.; Kuen, J.; Ma, L.; Shahroudy, A.; Shuai, B.; Liu, T.; Wang, X.; Wang, G.; Cai, J.; et al. Recent advances in\nconvolutional neural networks. Pattern Recognit. 2018 ,77, 354\u2013377. [CrossRef]\n12. Perez, H.; Tah, J.H.M.; Mosavi, A. Deep Learning for Detecting Building Defects Using Convolutional Neural Networks. Sensors\n2019 ,19, 3556. [CrossRef]\n13. Hussain, M.; Al-Aqrabi, H.; Hill, R. PV-CrackNet Architecture for Filter Induced Augmentation and Micro-Cracks Detection\nwithin a Photovoltaic Manufacturing Facility. Energies 2022 ,15, 8667.", "start_char_idx": 0, "end_char_idx": 3275, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "344844a4-ac2d-4848-ba1f-ad29e005d00b": {"__data__": {"id_": "344844a4-ac2d-4848-ba1f-ad29e005d00b", "embedding": null, "metadata": {"page_label": "22", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d3aec5d3-5d75-4830-90fd-a7ac97c2f698", "node_type": "4", "metadata": {"page_label": "22", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "867cfc2154096e43d158c622debe1a67a06232c507b5ed5f8481cf4a38cb1b70", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "084b76d2-9d47-49fe-a704-b3176e748bac", "node_type": "1", "metadata": {"page_label": "22", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "b10ccb5ffe7a0fcbbeb190ddceec3737e4b0af6525f2650c03bf3ca9b3a6c227", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dac45c93-a982-4297-a161-7ffde6437e8d", "node_type": "1", "metadata": {}, "hash": "2121e55d23e62348ad10f6bc8e5cbe72678ff6b17543d15e9cfbb5fd4d9e4674", "class_name": "RelatedNodeInfo"}}, "text": "Gu, J.; Wang, Z.; Kuen, J.; Ma, L.; Shahroudy, A.; Shuai, B.; Liu, T.; Wang, X.; Wang, G.; Cai, J.; et al. Recent advances in\nconvolutional neural networks. Pattern Recognit. 2018 ,77, 354\u2013377. [CrossRef]\n12. Perez, H.; Tah, J.H.M.; Mosavi, A. Deep Learning for Detecting Building Defects Using Convolutional Neural Networks. Sensors\n2019 ,19, 3556. [CrossRef]\n13. Hussain, M.; Al-Aqrabi, H.; Hill, R. PV-CrackNet Architecture for Filter Induced Augmentation and Micro-Cracks Detection\nwithin a Photovoltaic Manufacturing Facility. Energies 2022 ,15, 8667. [CrossRef]\n14. Hussain, M.; Dhimish, M.; Holmes, V .; Mather, P . Deployment of AI-based RBF network for photovoltaics fault detection\nprocedure. AIMS Electron. Electr. Eng. 2020 ,4, 1\u201318. [CrossRef]\n15. Hussain, M.; Al-Aqrabi, H.; Munawar, M.; Hill, R.; Parkinson, S. Exudate Regeneration for Automated Exudate Detection in\nRetinal Fundus Images. IEEE Access 2022 . [CrossRef]\n16. Hussain, M.; Al-Aqrabi, H.; Hill, R. Statistical Analysis and Development of an Ensemble-Based Machine Learning Model for\nPhotovoltaic Fault Detection. Energies 2022 ,15, 5492. [CrossRef]\n17. Singh, S.A.; Desai, K.A. Automated surface defect detection framework using machine vision and convolutional neural networks.\nJ. Intell. Manuf. 2022 ,34, 1995\u20132011. [CrossRef]\n18. Weichert, D.; Link, P .; Stoll, A.; R\u00fcping, S.; Ihlenfeldt, S.; Wrobel, S. A review of machine learning for the optimization of\nproduction processes. Int. J. Adv. Manuf. Technol. 2019 ,104, 1889\u20131902. [CrossRef]\n19. Wang, J.; Ma, Y.; Zhang, L.; Gao, R.X.; Wu, D. Deep learning for smart manufacturing: Methods and applications. J. Manuf. Syst.\n2018 ,48, 144\u2013156. [CrossRef]\n20. Weimer, D.; Scholz-Reiter, B.; Shpitalni, M. Design of deep convolutional neural network architectures for automated feature\nextraction in industrial inspection. CIRP Ann. 2016 ,65, 417\u2013420. [CrossRef]\n21. Kusiak, A. Smart manufacturing. Int. J. Prod. Res. 2017 ,56, 508\u2013517. [CrossRef]", "start_char_idx": 2719, "end_char_idx": 4694, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dac45c93-a982-4297-a161-7ffde6437e8d": {"__data__": {"id_": "dac45c93-a982-4297-a161-7ffde6437e8d", "embedding": null, "metadata": {"page_label": "23", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1e556286-3fcd-4921-a794-873084fd4807", "node_type": "4", "metadata": {"page_label": "23", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "7ad738638b47d27372c080724c9867f34480ad2aba720dc38f48c50639e0c7ab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "344844a4-ac2d-4848-ba1f-ad29e005d00b", "node_type": "1", "metadata": {"page_label": "22", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "497058a5ebe59d7dda39bb234f0c750152b94343008a6cf68b52fd0a7efef966", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f81b47a7-9715-484a-b708-285119d6544b", "node_type": "1", "metadata": {}, "hash": "bee6d0642df5f53f3322f156b376e6578978e3f1823e7c0f570398ae651043d1", "class_name": "RelatedNodeInfo"}}, "text": "Machines 2023 ,11, 677 23 of 25\n22. Yang, J.; Li, S.; Wang, Z.; Dong, H.; Wang, J.; Tang, S. Using Deep Learning to Detect Defects in Manufacturing: A Comprehensive\nSurvey and Current Challenges. Materials 2020 ,13, 5755. [CrossRef]\n23. Soviany, P .; Ionescu, R.T. Optimizing the Trade-Off between Single-Stage and Two-Stage Deep Object Detectors using Image\nDif\ufb01culty Prediction. In Proceedings of the 2018 20th International Symposium on Symbolic and Numeric Algorithms for\nScienti\ufb01c Computing (SYNASC), Timisoara, Romania, 20\u201323 September 2018. [CrossRef]\n24. Du, L.; Zhang, R.; Wang, X. Overview of two-stage object detection algorithms. J. Phys. Conf. Ser. 2020 ,1544 , 012033. [CrossRef]\n25. Sultana, F.; Su\ufb01an, A.; Dutta, P . A Review of Object Detection Models Based on Convolutional Neural Network. In Advances in\nIntelligent Systems and Computing ; Springer: Singapore, 2020; pp. 1\u201316. [CrossRef]\n26. Liu, W.; Anguelov, D.; Erhan, D.; Szegedy, C.; Reed, S.; Fu, C.Y.; Berg, A.C. SSD: Single shot multibox detector. In Proceedings of\nthe Computer Vision\u2014ECCV 2016, Amsterdam, The Netherlands, 11\u201314 October 2016; pp. 21\u201337. [CrossRef]\n27. Fu, C.Y.; Liu, W.; Ranga, A.; Tyagi, A.; Berg, A.C. DSSD: Deconvolutional Single Shot Detector. arXiv 2017 , arXiv:1701.06659.\n28. Cheng, X.; Yu, J. RetinaNet with Difference Channel Attention and Adaptively Spatial Feature Fusion for Steel Surface Defect\nDetection. IEEE Trans. Instrum. Meas. 2020 ,70, 2503911. [CrossRef]\n29. Redmon, J.; Divvala, S.; Girshick, R.; Farhadi, A. You Only Look Once: Uni\ufb01ed, Real-Time Object Detection. In Proceedings of the\n2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Las Vegas, NV , USA, 27\u201330 June 2016; pp. 779\u2013788.\n[CrossRef]\n30. Wang, Z.J.; Turko, R.; Shaikh, O.; Park, H.; Das, N.; Hohman, F.; Kahng, M.; Chau, D.H.P . CNN Explainer: Learning Convolutional\nNeural Networks with Interactive Visualization. IEEE Trans. Vis. Comput. Graph. 2020 ,27, 1396\u20131406. [CrossRef] [PubMed]\n31. Krizhevsky, A.; Sutskever, I.; Hinton, G.E. Imagenet classi\ufb01cation with deep convolutional neural networks. Commun. ACM 2017 ,\n60, 84\u201390. [CrossRef]\n32. Simonyan, K.; Zisserman, A. Very Deep Convolutional Networks for Large-Scale Image Recognition. arXiv 2014 , arXiv:1409.1556.\n33. Szegedy, C.; Liu, W.; Jia, Y.; Sermanet, P .; Reed, S.; Anguelov, D.; Rabinovich, A. Going deeper with convolutions. In Proceedings\nof the Conference on Computer Vision and Pattern Recognition, Boston, MA, USA, 12 June 2015.\n34. He, K.; Zhang, X.; Ren, S.; Sun, J. Deep residual learning for image recognition. In Proceedings of the Conference on Computer\nVision and Pattern Recognition, Las Vegas, NV , USA, 30 June 2016.\n35.", "start_char_idx": 0, "end_char_idx": 2706, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f81b47a7-9715-484a-b708-285119d6544b": {"__data__": {"id_": "f81b47a7-9715-484a-b708-285119d6544b", "embedding": null, "metadata": {"page_label": "23", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1e556286-3fcd-4921-a794-873084fd4807", "node_type": "4", "metadata": {"page_label": "23", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "7ad738638b47d27372c080724c9867f34480ad2aba720dc38f48c50639e0c7ab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dac45c93-a982-4297-a161-7ffde6437e8d", "node_type": "1", "metadata": {"page_label": "23", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "0ad51989327aaffdcd08fff8cdf3f08a9c462ef51036da0d7add49f92369581d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "91640ef9-5917-42ca-9b8c-d7b1f107157e", "node_type": "1", "metadata": {}, "hash": "bf3794ef59f72b50046e93660fe7e362c52427a4fa37ad7f34ccc11303675bc9", "class_name": "RelatedNodeInfo"}}, "text": "Commun. ACM 2017 ,\n60, 84\u201390. [CrossRef]\n32. Simonyan, K.; Zisserman, A. Very Deep Convolutional Networks for Large-Scale Image Recognition. arXiv 2014 , arXiv:1409.1556.\n33. Szegedy, C.; Liu, W.; Jia, Y.; Sermanet, P .; Reed, S.; Anguelov, D.; Rabinovich, A. Going deeper with convolutions. In Proceedings\nof the Conference on Computer Vision and Pattern Recognition, Boston, MA, USA, 12 June 2015.\n34. He, K.; Zhang, X.; Ren, S.; Sun, J. Deep residual learning for image recognition. In Proceedings of the Conference on Computer\nVision and Pattern Recognition, Las Vegas, NV , USA, 30 June 2016.\n35. Girshick, R.; Donahue, J.; Darrell, T.; Malik, J. Region-Based Convolutional Networks for Accurate Object Detection and\nSegmentation. IEEE Trans. Pattern Anal. Mach. Intell. 2015 ,38, 142\u2013158. [CrossRef]\n36. Girshick, R. Fast R-CNN. In Proceedings of the International Conference on Computer Vision, Santiago, Chile, 7\u201313 December\n2015.\n37. Ren, S.; He, K.; Girshick, R.; Sun, J. Faster R-CNN: Towards real-time object detection with region proposal networks. Trans.\nPattern Anal. Mach. Intell. 2017 ,39, 1137\u20131149. [CrossRef]\n38. Vidyavani, A.; Dheeraj, K.; Reddy, M.R.M.; Kumar, K.N. Object Detection Method Based on YOLOv3 using Deep Learning\nNetworks. Int. J. Innov. Technol. Explor. Eng. 2019 ,9, 1414\u20131417. [CrossRef]\n39. Everingham, M.; Van Gool, L.; Williams, C.K.I.; Winn, J.; Zisserman, A. The Pascal Visual Object Classes (VOC) Challenge. Int. J.\nComput. Vis. 2009 ,88, 303\u2013338. [CrossRef]\n40. Shetty, S. Application of Convolutional Neural Network for Image Classi\ufb01cation on Pascal VOC Challenge 2012 dataset.\narXiv 2016 , arXiv:1607.03785.\n41. Felzenszwalb, P .F.; Girshick, R.B.; McAllester, D.; Ramanan, D. Object Detection with Discriminatively Trained Part-Based Models.\nIEEE Trans. Pattern Anal. Mach. Intell. 2009 ,32, 1627\u20131645. [CrossRef] [PubMed]\n42. Chang, Y.-L.; Anagaw, A.; Chang, L.; Wang, Y.C.; Hsiao, C.-Y.; Lee, W.-H. Ship Detection Based on YOLOv2 for SAR Imagery.\nRemote Sens. 2019 ,11, 786. [CrossRef]\n43. Liao, Z.; Carneiro, G. On the importance of normalisation layers in deep learning with piecewise linear activation units. In\nProceedings of the 2016 IEEE Winter Conference on Applications of Computer Vision (WACV), New York, NY, USA, 7\u201310 March\n2016. [CrossRef]\n44. Garbin, C.; Zhu, X.; Marques, O. Dropout vs. batch normalization: An empirical study of their impact to deep learning. Multimed.\nTools Appl. 2020 ,79, 12777\u201312815. [CrossRef]\n45. Li, G.; Jian, X.; Wen, Z.; AlSultan, J. Algorithm of over\ufb01tting avoidance in CNN based on maximum pooled and weight decay.\nAppl. Math. Nonlinear Sci. 2022 ,7, 965\u2013974. [CrossRef]\n46. Deng, J.; Dong, W.; Socher, R.; Li, L.J.", "start_char_idx": 2105, "end_char_idx": 4813, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "91640ef9-5917-42ca-9b8c-d7b1f107157e": {"__data__": {"id_": "91640ef9-5917-42ca-9b8c-d7b1f107157e", "embedding": null, "metadata": {"page_label": "23", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1e556286-3fcd-4921-a794-873084fd4807", "node_type": "4", "metadata": {"page_label": "23", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "7ad738638b47d27372c080724c9867f34480ad2aba720dc38f48c50639e0c7ab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f81b47a7-9715-484a-b708-285119d6544b", "node_type": "1", "metadata": {"page_label": "23", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "ce393d2647de4fae037b22723501b8d781eb2d573fdc6e5dffb77dfad98fb0fc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "96acc44e-3d51-402d-ae65-1743756780a8", "node_type": "1", "metadata": {}, "hash": "13baedf383f6f07e293c3eed7a5b24500d58d8ea3cc20e5b4833ac954656e5ae", "class_name": "RelatedNodeInfo"}}, "text": "In\nProceedings of the 2016 IEEE Winter Conference on Applications of Computer Vision (WACV), New York, NY, USA, 7\u201310 March\n2016. [CrossRef]\n44. Garbin, C.; Zhu, X.; Marques, O. Dropout vs. batch normalization: An empirical study of their impact to deep learning. Multimed.\nTools Appl. 2020 ,79, 12777\u201312815. [CrossRef]\n45. Li, G.; Jian, X.; Wen, Z.; AlSultan, J. Algorithm of over\ufb01tting avoidance in CNN based on maximum pooled and weight decay.\nAppl. Math. Nonlinear Sci. 2022 ,7, 965\u2013974. [CrossRef]\n46. Deng, J.; Dong, W.; Socher, R.; Li, L.J.; Li, K.; Fei-Fei, L. Imagenet: A large-scale hierarchical image database. In Proceedings of the\n2009 IEEE Conference on Computer Vision and Pattern Recognition, Miami, FL, USA, 20\u201325 June 2009.\n47. Xue, J.; Cheng, F.; Li, Y.; Song, Y.; Mao, T. Detection of Farmland Obstacles Based on an Improved YOLOv5s Algorithm by Using\nCIoU and Anchor Box Scale Clustering. Sensors 2022 ,22, 1790. [CrossRef]\n48. Ahmed, M.; Seraj, R.; Islam, S.M.S. The k-means Algorithm: A Comprehensive Survey and Performance Evaluation. Electronics\n2020 ,9, 1295. [CrossRef]\n49. Redmon, J. Darknet: Open Source Neural Networks in C. 2013. Available online: https://pjreddie.com/darknet (accessed on\n1 January 2023).\n50. Furusho, Y.; Ikeda, K. Theoretical analysis of skip connections and batch normalization from generalization and optimization\nperspectives. APSIP A Trans. Signal Inf. Process. 2020 ,9, e9. [CrossRef]", "start_char_idx": 4267, "end_char_idx": 5706, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "96acc44e-3d51-402d-ae65-1743756780a8": {"__data__": {"id_": "96acc44e-3d51-402d-ae65-1743756780a8", "embedding": null, "metadata": {"page_label": "24", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d34d3d5f-b1d2-41f9-820e-9c007640f7a4", "node_type": "4", "metadata": {"page_label": "24", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "d47ac87cc1a1d01897b7464f7bdd4dc9c68c4650ebdd1e9ccfe16300743529fd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "91640ef9-5917-42ca-9b8c-d7b1f107157e", "node_type": "1", "metadata": {"page_label": "23", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "b02350bfe6c45b69b3a131a3abad49bb4939cb99fbd1171563b34fdc58f497a8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3448e5cd-a0d7-4de7-a954-26dfef75c13a", "node_type": "1", "metadata": {}, "hash": "f6e2a22610346e50c4d912087d7384eac18c7ed55b9a35a9b8d6c9378cf7b38a", "class_name": "RelatedNodeInfo"}}, "text": "Machines 2023 ,11, 677 24 of 25\n51. Machine-Learning System Tackles Speech and Object Recognition. Available online: https://news.mit.edu/machine-learning-\nimage-object-recognition-918 (accessed on 1 January 2023).\n52. Bochkovskiy, A.; Wang, C.Y.; Liao HY, M. YOLOv4: Optimal Speed and Accuracy of Object Detection. arXiv 2020 ,\narXiv:2004.10934.\n53. Tan, M.; Le, Q. Ef\ufb01cientNet: Rethinking model scaling for convolutional neural networks. In Proceedings of the International\nConference on Machine Learning (ICML), Long Beach, CA, USA, 9\u201315 June 2019.\n54. Huang, G.; Liu, Z.; Van Der Maaten, L.; Weinberger, K.Q. Densely connected convolutional networks. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition (CVPR), Honolulu, HI, USA, 21\u201326 July 2017; pp. 4700\u20134708.\n55. Lin, T.Y.; Doll \u00e1r, P .; Girshick, R.; He, K.; Hariharan, B.; Belongie, S. Feature pyramid networks for object detection. In Proceedings\nof the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Honolulu, HI, USA, 21\u201326 July 2017; pp. 2117\u20132125.\n56. Liu, S.; Qi, L.; Qin, H.; Shi, J.; Jia, J. Path aggregation network for instance segmentation. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), Salt Lake City, UT, USA, 18\u201323 June 2018; pp. 8759\u20138768.\n57. He, K.; Zhang, X.; Ren, S.; Sun, J. Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition. IEEE Trans.\nPattern Anal. Mach. Intell. 2015 ,37, 1904\u20131916. [CrossRef]\n58. Zheng, Z.; Wang, P .; Liu, W.; Li, J.; Ye, R.; Ren, D. Distance-IoU Loss: Faster and better learning for bounding box regression. In\nProceedings of the AAAI Conference on Arti\ufb01cial Intelligence (AAAI), New York, NY, USA, 7\u201312 February 2020.\n59. Misra, D. Mish: A self regularized nonmonotonic neural activation function. arXiv 2019 , arXiv:1908.08681.\n60. Yao, Z.; Cao, Y.; Zheng, S.; Huang, G.; Lin, S. Cross-Iteration Batch Normalization. arXiv 2020 , arXiv:2002.05712.\n61. Ultralytics. YOLOv5 2020. Available online: https://github.com/ultralytics/yolov5 (accessed on 1 January 2023).\n62. Jocher, G.; Stoken, A.; Borovec, J.; Christopher, S.T.A.N.; Laughing, L.C. Ultralytics/yolov5: v4.0-nn.SiLU() Activations, Weights\n& Biases Logging, PyTorch Hub Integration. Zenodo 2021 . Available online: https://zenodo.org/record/4418161 (accessed on\n5 January 2023).\n63. Lin, T.Y.; Maire, M.; Belongie, S.; Hays, J.; Perona, P .; Ramanan, D.; Zitnick, C.L. Microsoft coco: Common objects in context. In\nProceedings of the European Conference on Computer Vision, Zurich, Switzerland, 6\u201312 September 2014.\n64. Tan, M.; Pang, R.; Le, Q.V . Ef\ufb01cientDet: Scalable and Ef\ufb01cient Object Detection. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, Seattle, WA, USA, 13\u201319 June 2020.\n65.", "start_char_idx": 0, "end_char_idx": 2810, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3448e5cd-a0d7-4de7-a954-26dfef75c13a": {"__data__": {"id_": "3448e5cd-a0d7-4de7-a954-26dfef75c13a", "embedding": null, "metadata": {"page_label": "24", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d34d3d5f-b1d2-41f9-820e-9c007640f7a4", "node_type": "4", "metadata": {"page_label": "24", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "d47ac87cc1a1d01897b7464f7bdd4dc9c68c4650ebdd1e9ccfe16300743529fd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "96acc44e-3d51-402d-ae65-1743756780a8", "node_type": "1", "metadata": {"page_label": "24", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "5dac32248638e56398e86e5b7d048ad1e61c3c79cfc7d22d0148fc949df4ec28", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "562bcee7-4979-4141-af4b-061ac126fe93", "node_type": "1", "metadata": {}, "hash": "032931739f3f7977e56ad74d7e09df8d23f4bd42ffdbc0f165182b9fb93b5c0a", "class_name": "RelatedNodeInfo"}}, "text": "Zenodo 2021 . Available online: https://zenodo.org/record/4418161 (accessed on\n5 January 2023).\n63. Lin, T.Y.; Maire, M.; Belongie, S.; Hays, J.; Perona, P .; Ramanan, D.; Zitnick, C.L. Microsoft coco: Common objects in context. In\nProceedings of the European Conference on Computer Vision, Zurich, Switzerland, 6\u201312 September 2014.\n64. Tan, M.; Pang, R.; Le, Q.V . Ef\ufb01cientDet: Scalable and Ef\ufb01cient Object Detection. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, Seattle, WA, USA, 13\u201319 June 2020.\n65. Li, C.; Li, L.; Jiang, H.; Weng, K.; Geng, Y.; Li, L.; Wei, X. YOLOv6: A Single-Stage Object Detection Framework for Industrial\nApplications. arXiv 2022 , arXiv:2209.02976.\n66. Ding, X.; Zhang, X.; Ma, N.; Han, J.; Ding, G.; Sun, J. Repvgg: Making vgg-style convnets great again. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, Nashville, TN, USA, 20\u201325 June 2021; pp. 13733\u201313742.\n67. Zhang, H.; Wang, Y.; Dayoub, F.; Sunderhauf, N. Varifocalnet: An iou-aware dense object detector. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, Nashville, TN, USA, 20\u201325 June 2021; pp. 8514\u20138523.\n68. Li, X.; Wang, W.; Wu, L.; Chen, S.; Hu, X.; Li, J.; Yang, J. Generalized focal loss: Learning quali\ufb01ed and distributed bounding\nboxes for dense object detection. Adv. Neural Inf. Process. Syst. 2020 ,33, 21002\u201321012.\n69. Gevorgyan, Z. Siou loss: More powerful learning for bounding box regression. arXiv 2022 , arXiv:2205.12740.\n70. Shu, C.; Liu, Y.; Gao, J.; Yan, Z.; Shen, C. Channel-wise knowledge distillation for dense prediction. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision, Montreal, BC, Canada, 11\u201317 October 20221; pp. 5311\u20135320.\n71. Solawetz, J.; Nelson, J. What\u2019s New in YOLOv6? 4 July 2022. Available online: https://blog.robo\ufb02ow.com/yolov6/ (accessed on\n1 January 2023).\n72. Wang, C.Y.; Bochkovskiy, A.; Liao HY, M. YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors.\narXiv 2022 , arXiv:2207.02696.\n73. Ge, Z.; Liu, S.; Wang, F.; Li, Z.; Sun, J. YOLOX: Exceeding YOLO series in 2021. arXiv 2021 , arXiv:2107.08430.\n74. Wang, C.-Y.; Yeh, I.-H.; Liao, H.-Y.M. You only learn one representation: Uni\ufb01ed network for multiple tasks. arXiv 2021 ,\narXiv:2105.04206.\n75. Wu, W.; Zhao, Y.; Xu, Y.; Tan, X.; He, D.; Zou, Z.; Ye, J.; Li, Y.; Yao, M.; Dong, Z.; et al. DSANet: Dynamic Segment AggrDSANet:\nDynamic Segment Aggregation Network for Video-Level Representation Learning. In Proceedings of the MM \u201921\u201429th ACM\nInternational Conference on Multimedia, Virtual, 20\u201324 October 2021. [CrossRef]\n76.", "start_char_idx": 2267, "end_char_idx": 4950, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "562bcee7-4979-4141-af4b-061ac126fe93": {"__data__": {"id_": "562bcee7-4979-4141-af4b-061ac126fe93", "embedding": null, "metadata": {"page_label": "24", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d34d3d5f-b1d2-41f9-820e-9c007640f7a4", "node_type": "4", "metadata": {"page_label": "24", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "d47ac87cc1a1d01897b7464f7bdd4dc9c68c4650ebdd1e9ccfe16300743529fd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3448e5cd-a0d7-4de7-a954-26dfef75c13a", "node_type": "1", "metadata": {"page_label": "24", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "fc32ffd267c526b51c3029b1e922ae61542ad2be8f5180be849a68064fe98866", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "873df3c6-ad2d-4813-abc2-aae7e5f93997", "node_type": "1", "metadata": {}, "hash": "7631918cbcfd8d3b8f9638c0e4b719913cd6d8cd6f7548a957a6195fdafb72b6", "class_name": "RelatedNodeInfo"}}, "text": "74. Wang, C.-Y.; Yeh, I.-H.; Liao, H.-Y.M. You only learn one representation: Uni\ufb01ed network for multiple tasks. arXiv 2021 ,\narXiv:2105.04206.\n75. Wu, W.; Zhao, Y.; Xu, Y.; Tan, X.; He, D.; Zou, Z.; Ye, J.; Li, Y.; Yao, M.; Dong, Z.; et al. DSANet: Dynamic Segment AggrDSANet:\nDynamic Segment Aggregation Network for Video-Level Representation Learning. In Proceedings of the MM \u201921\u201429th ACM\nInternational Conference on Multimedia, Virtual, 20\u201324 October 2021. [CrossRef]\n76. Li, C.; Tang, T.; Wang, G.; Peng, J.; Wang, B.; Liang, X.; Chang, X. BossNAS: Exploring Hybrid CNN-transformers with Block-\nwisely Self-supervised Neural Architecture Search. In Proceedings of the IEEE/CVF International Conference on Computer\nVision, Online, 11\u201317 October 2021. [CrossRef]\n77. Dollar, P .; Singh, M.; Girshick, R. Fast and accurate model scaling. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), Nashville, TN, USA, 20\u201325 June 2021; pp. 924\u2013932.\n78. Guo, S.; Alvarez, J.M.; Salzmann, M. ExpandNets: Linear over-parameterization to train compact convolutional networks. Adv.\nNeural Inf. Process. Syst. (NeurIPS) 2020 ,33, 1298\u20131310.\n79. Ding, X.; Zhang, X.; Zhou, Y.; Han, J.; Ding, G.; Sun, J. Scaling up your kernels to 31 \u00d731: Revisiting large kernel design in CNNs.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), New Orleans, LA, USA, 18\u201324\nJune 2022.\n80. Jocher, G.; Chaurasia, A.; Qiu, J. YOLO by Ultralytics. GitHub. 1 January 2023. Available online: https://github.com/ultralytics/\nultralytics (accessed on 12 January 2023).", "start_char_idx": 4474, "end_char_idx": 6089, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "873df3c6-ad2d-4813-abc2-aae7e5f93997": {"__data__": {"id_": "873df3c6-ad2d-4813-abc2-aae7e5f93997", "embedding": null, "metadata": {"page_label": "25", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c6cc38c7-6e41-4e35-a796-7a6352bcecb6", "node_type": "4", "metadata": {"page_label": "25", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "9f1c2f1a54a5cc9d6e7755430b47d074c4c79275f4cdcc70ebf9e69c562f71dc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "562bcee7-4979-4141-af4b-061ac126fe93", "node_type": "1", "metadata": {"page_label": "24", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "0489052b52fa33bd0db894bc6c9e247513e45a038aac0cfa9aac31e8f776c6c5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "353ddba9-89a1-4894-8fed-be6422271f9d", "node_type": "1", "metadata": {}, "hash": "0fa7c82bde3d69f9b5e301e31f806c03eadf1e6de989d4b9e03b20579a0ff5da", "class_name": "RelatedNodeInfo"}}, "text": "Machines 2023 ,11, 677 25 of 25\n81. Jin, R.; Niu, Q. Automatic Fabric Defect Detection Based on an Improved YOLOv5. Math. Probl. Eng. 2021 ,2021 , 1\u201313. [CrossRef]\n82. NVIDIA Jetson TX2: High Performance AI at the Edge, NVIDIA. Available online: https://www.nvidia.com/en-gb/autonomous-\nmachines/embedded-systems/jetson-tx2/ (accessed on 30 January 2023).\n83. NVIDIA TensorRT. NVIDIA Developer. 18 July 2019. Available online: https://developer.nvidia.com/tensorrt (accessed on\n5 January 2023).\n84. Dlamini, S.; Kao, C.-Y.; Su, S.-L.; Kuo, C.-F.J. Development of a real-time machine vision system for functional textile fabric defect\ndetection using a deep YOLOv4 model. Text. Res. J. 2021 ,92, 675\u2013690. [CrossRef]\n85. Lin, G.; Liu, K.; Xia, X.; Yan, R. An Ef\ufb01cient and Intelligent Detection Method for Fabric Defects based on Improved YOLOv5.\nSensors 2022 ,23, 97. [CrossRef] [PubMed]\n86. Liu, Z.; Tan, Y.; He, Q.; Xiao, Y. SwinNet: Swin Transformer Drives Edge-Aware RGB-D and RGB-T Salient Object Detection. IEEE\nTrans. Circuits Syst. Video Technol. 2021 ,32, 4486\u20134497. [CrossRef]\n87. Zhang, M.; Yin, L. Solar Cell Surface Defect Detection Based on Improved YOLO v5. IEEE Access 2022 ,10, 80804\u201380815. [CrossRef]\n88. Binomairah, A.; Abdullah, A.; Khoo, B.E.; Mahdavipour, Z.; Teo, T.W.; Noor, N.S.M.; Abdullah, M.Z. Detection of microcracks\nand dark spots in monocrystalline PERC cells using photoluminescene imaging and YOLO-based CNN with spatial pyramid\npooling. EPJ Photovolt. 2022 ,13, 27. [CrossRef]\n89. Sun, T.; Xing, H.; Cao, S.; Zhang, Y.; Fan, S.; Liu, P . A novel detection method for hot spots of photovoltaic (PV) panels using\nimproved anchors and prediction heads of YOLOv5 network. Energy Rep. 2022 ,8, 1219\u20131229. [CrossRef]\n90. Yang, D.; Cui, Y.; Yu, Z.; Yuan, H. Deep Learning Based Steel Pipe Weld Defect Detection. Appl. Artif. Intell. 2021 ,35, 1237\u20131249.\n[CrossRef]\n91. Ma, Z.; Li, Y.; Huang, M.; Huang, Q.; Cheng, J.; Tang, S. A lightweight detector based on attention mechanism for aluminum strip\nsurface defect detection. Comput. Ind. 2021 ,136, 103585. [CrossRef]\n92. Shi, J.; Yang, J.; Zhang, Y. Research on Steel Surface Defect Detection Based on YOLOv5 with Attention Mechanism. Electronics\n2022 ,11, 3735. [CrossRef]\n93. CEP , F.A. 5 Insightful Statistics Related to Warehouse Safety. Available online: www.damotech.com (accessed on 11 January 2023).\n94. Armour, R. The Rack Group. Available online: https://therackgroup.com/product/rack-armour/ (accessed on 12 January 2023).\n95. Hussain, M.; Chen, T.; Hill, R. Moving toward Smart Manufacturing with an Autonomous Pallet Racking Inspection System\nBased on MobileNetV2. J. Manuf. Mater. Process. 2022 ,6, 75. [CrossRef]\n96.", "start_char_idx": 0, "end_char_idx": 2707, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "353ddba9-89a1-4894-8fed-be6422271f9d": {"__data__": {"id_": "353ddba9-89a1-4894-8fed-be6422271f9d", "embedding": null, "metadata": {"page_label": "25", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c6cc38c7-6e41-4e35-a796-7a6352bcecb6", "node_type": "4", "metadata": {"page_label": "25", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "9f1c2f1a54a5cc9d6e7755430b47d074c4c79275f4cdcc70ebf9e69c562f71dc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "873df3c6-ad2d-4813-abc2-aae7e5f93997", "node_type": "1", "metadata": {"page_label": "25", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}, "hash": "d6be34e9ad3eb67a3b7e072de332883654f17b77f4e57e8eefb0dde7e772ab4f", "class_name": "RelatedNodeInfo"}}, "text": "Shi, J.; Yang, J.; Zhang, Y. Research on Steel Surface Defect Detection Based on YOLOv5 with Attention Mechanism. Electronics\n2022 ,11, 3735. [CrossRef]\n93. CEP , F.A. 5 Insightful Statistics Related to Warehouse Safety. Available online: www.damotech.com (accessed on 11 January 2023).\n94. Armour, R. The Rack Group. Available online: https://therackgroup.com/product/rack-armour/ (accessed on 12 January 2023).\n95. Hussain, M.; Chen, T.; Hill, R. Moving toward Smart Manufacturing with an Autonomous Pallet Racking Inspection System\nBased on MobileNetV2. J. Manuf. Mater. Process. 2022 ,6, 75. [CrossRef]\n96. Hussain, M.; Al-Aqrabi, H.; Munawar, M.; Hill, R.; Alsboui, T. Domain Feature Mapping with YOLOv7 for Automated Edge-Based\nPallet Racking Inspections. Sensors 2022 ,22, 6927. [CrossRef] [PubMed]\n97. Farahnakian, F.; Koivunen, L.; Makila, T.; Heikkonen, J. Towards Autonomous Industrial Warehouse Inspection. In Proceedings of\nthe 2021 26th International Conference on Automation and Computing (ICAC), Portsmouth, UK, 2\u20134 September 2021. [CrossRef]\nDisclaimer/Publisher\u2019s Note: The statements, opinions and data contained in all publications are solely those of the individual\nauthor(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to\npeople or property resulting from any ideas, methods, instructions or products referred to in the content.", "start_char_idx": 2097, "end_char_idx": 3525, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"6f5ac2d6-458e-46bd-a16b-99e79c1169f0": {"node_ids": ["b25babec-d753-464f-9c63-da898433b600"], "metadata": {"page_label": "1", "file_name": "Attention Is ALl You Need.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\Attention Is ALl You Need.pdf", "file_type": "application/pdf", "file_size": 569417, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}}, "08df43f0-66b4-4f7b-a2a4-867e7ceed120": {"node_ids": ["37ee8168-10b0-43c1-8191-cf2875ce6c9e"], "metadata": {"page_label": "2", "file_name": "Attention Is ALl You Need.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\Attention Is ALl You Need.pdf", "file_type": "application/pdf", "file_size": 569417, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}}, "07f99e2a-8a29-4863-9dd7-8a6fc6669058": {"node_ids": ["41e021e8-4f27-40b0-a43b-edcd006fc13c"], "metadata": {"page_label": "3", "file_name": "Attention Is ALl You Need.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\Attention Is ALl You Need.pdf", "file_type": "application/pdf", "file_size": 569417, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}}, "0dd8e9fc-8037-4781-b79a-5ac2a43654a0": {"node_ids": ["94eef090-f7a4-42a4-ae6e-6e296fe45603"], "metadata": {"page_label": "4", "file_name": "Attention Is ALl You Need.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\Attention Is ALl You Need.pdf", "file_type": "application/pdf", "file_size": 569417, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}}, "3a7bbee4-26b3-484a-ae14-16f0033a6b78": {"node_ids": ["f10575cd-5ac6-4b3f-867e-c7e443de4786"], "metadata": {"page_label": "5", "file_name": "Attention Is ALl You Need.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\Attention Is ALl You Need.pdf", "file_type": "application/pdf", "file_size": 569417, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}}, "6fc3e839-d2ba-4c04-b128-29a06c66ce37": {"node_ids": ["aad02db8-130b-4882-b822-9b5604601742"], "metadata": {"page_label": "6", "file_name": "Attention Is ALl You Need.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\Attention Is ALl You Need.pdf", "file_type": "application/pdf", "file_size": 569417, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}}, "309371d3-c649-4d38-978d-129cda618a4b": {"node_ids": ["03dfa554-549e-4184-803b-ffd2aba6c2e9"], "metadata": {"page_label": "7", "file_name": "Attention Is ALl You Need.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\Attention Is ALl You Need.pdf", "file_type": "application/pdf", "file_size": 569417, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}}, "f83328ea-b7cd-4770-a9fc-0921614750d5": {"node_ids": ["d7f23b71-9f6b-4952-bc7e-1c816d0fb36d"], "metadata": {"page_label": "8", "file_name": "Attention Is ALl You Need.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\Attention Is ALl You Need.pdf", "file_type": "application/pdf", "file_size": 569417, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}}, "c2d98846-01f8-4c7b-93cc-58b59152ec6a": {"node_ids": ["bd70091a-7b9d-486a-8709-181ecf63a6b5"], "metadata": {"page_label": "9", "file_name": "Attention Is ALl You Need.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\Attention Is ALl You Need.pdf", "file_type": "application/pdf", "file_size": 569417, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}}, "097e20b6-c825-47ac-bde3-e819ada0d7a6": {"node_ids": ["c649e544-0987-4c6c-bc36-a03c5a599d32", "8a3726c7-9dbe-4ce1-b409-efeca9546a71"], "metadata": {"page_label": "10", "file_name": "Attention Is ALl You Need.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\Attention Is ALl You Need.pdf", "file_type": "application/pdf", "file_size": 569417, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}}, "b739cd9f-479e-40af-b5f9-ca1130fd22ef": {"node_ids": ["49a48f98-d8c5-4e15-854f-8d2fee6d5149"], "metadata": {"page_label": "11", "file_name": "Attention Is ALl You Need.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\Attention Is ALl You Need.pdf", "file_type": "application/pdf", "file_size": 569417, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}}, "fb93cfb2-c772-4087-b80f-6c33fa1ac276": {"node_ids": ["9e24a353-fe5c-40b7-9561-0ee1cad5acc7", "b98890a0-fc8f-4232-a2e8-d56e191d21a8"], "metadata": {"page_label": "1", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}}, "ba3bbb6e-e313-48ed-8ecb-22b67481b822": {"node_ids": ["0d5715c6-9a1a-4a43-b057-0a0c637f1e0b"], "metadata": {"page_label": "2", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}}, "000ec4ee-73c7-489a-a0a0-4f2ddbe96279": {"node_ids": ["c4ae4461-f176-47ec-a42e-2b0342905e08", "cd159237-020e-4168-9332-44b3a195357d"], "metadata": {"page_label": "3", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}}, "8218156a-6485-4c4f-87ff-d61a5c7713fb": {"node_ids": ["9f01979a-5629-442b-b668-b0dd8fee2294", "4b831f59-ee10-4f29-a349-2384d00f78f4"], "metadata": {"page_label": "4", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}}, "6640492e-0115-4fb7-8c98-f82749b683e1": {"node_ids": ["c552f6bc-9894-40b9-b6ba-945c256f37c4", "fc148e0d-c2c6-487a-8682-d08f2c02284d", "d32811f7-f324-4379-a1e2-33e35a000b54", "3c9ce8fa-7f23-49f9-9a51-789f4e03ff6c", "b204266c-faee-4602-a169-9b886c8a763d"], "metadata": {"page_label": "5", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}}, "8ad2a569-cd4b-4305-84e7-073c6fd01aa8": {"node_ids": ["37f30d8e-eac0-46c2-a85f-fe48615e784f", "190dc89d-720f-4245-a8e7-b5a38479c977", "f47f5810-f649-4fbd-b2f9-5e1330569c56", "0db7839f-7f06-4c25-be55-09aac901c7b4", "cd12dccb-a60b-45af-a183-436b82bdfb10", "6ad428dc-65b0-4505-bda8-97106168845e", "b3b1a13f-927d-4c6b-b6fb-f79dbb543aed", "62356069-fe6e-40f6-84f7-383d57128a4d", "10c6d2b7-9e6f-43c5-848c-ad53aa7f6455", "605490da-c024-48e5-91cd-803d434f2701", "b1b2e1a6-b0ca-4205-b2b3-4d6e393d1363"], "metadata": {"page_label": "6", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}}, "27a7f5ca-a0ad-4c4e-bc7a-e7ba1ddddd65": {"node_ids": ["04ca27d0-7812-42de-bbc6-24a6d2f4acdc", "6822cb04-b0a8-412a-8aeb-69b3d02ed83d"], "metadata": {"page_label": "7", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}}, "da46a3dd-d186-4e85-96d2-4ce903ed0f84": {"node_ids": ["2aab20ce-7dba-468d-b904-64998792342b", "0ac8ead8-2866-4dbc-8d68-af957df18bf7"], "metadata": {"page_label": "8", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}}, "1c4b5901-ea90-4603-9e64-f3e49de73a06": {"node_ids": ["a9155316-bdd4-43a4-a71e-8cafc54c5c05"], "metadata": {"page_label": "9", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}}, "d8c32baf-e976-4c41-9e49-c5fb6c01e93c": {"node_ids": ["ad1a9dda-5747-44f8-a302-6b3b9ac20cdf", "a3dc245f-7b97-4133-9223-110012d6ce3e"], "metadata": {"page_label": "10", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}}, "b709ba6e-cc70-4811-902c-a513de62c8ac": {"node_ids": ["655ecc22-74fb-4999-8586-e93b4beb56d0", "128a3420-93d6-4152-be00-bd6b56d67b01"], "metadata": {"page_label": "11", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}}, "5046b664-6d35-4b4d-9529-de82cf07730c": {"node_ids": ["204329bd-79eb-4787-a511-f017b6c322e5", "b53883d1-11f9-4df9-9cde-9cbd73e45c7e"], "metadata": {"page_label": "12", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}}, "801abaf9-b805-4411-8488-efee2556f926": {"node_ids": ["2c892c7b-3dc6-4ae9-9787-bec7737985ee"], "metadata": {"page_label": "13", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}}, "12073eb4-59a6-476f-896b-94c1a65bf9ab": {"node_ids": ["3564a4b7-270f-4311-80e5-71a0933efb0f"], "metadata": {"page_label": "14", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}}, "09b6b1c1-a608-47bb-978d-70d9e5a34236": {"node_ids": ["ec564054-994c-448b-91a9-7d2184f7b475", "fc0ef815-7688-4082-a740-dfb5782eb18c"], "metadata": {"page_label": "15", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}}, "883382e7-f13e-4195-99d7-08fc8c05a2de": {"node_ids": ["c2b13b35-b269-4f53-b4c4-599b82d2c7ca", "7a8fa370-76a5-48b2-84b9-f22e8077bfe1"], "metadata": {"page_label": "16", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}}, "2aed3d7f-8075-4833-a35e-7b41d917fbdd": {"node_ids": ["1e10b9a7-5cf8-444b-9c18-cee7ba2db12a"], "metadata": {"page_label": "17", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}}, "95fa3d9e-798b-4cce-90e9-16caeecf2d2c": {"node_ids": ["dbb96f28-a554-41b5-9791-74ee7bdc13ef", "4be0ac67-5599-4c5e-9d1f-13cc13249d39"], "metadata": {"page_label": "18", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}}, "d8877978-9e4a-4524-9da1-dde2d09bb59e": {"node_ids": ["2b1cfe90-f9d0-41ac-a129-46d147ad209d"], "metadata": {"page_label": "19", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}}, "66961e48-a891-4a3a-b1da-5b410e742284": {"node_ids": ["094ef49a-395e-4575-8d39-13c15a1c34d7"], "metadata": {"page_label": "20", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}}, "35a17c4d-a332-4238-894a-aa4d925b0281": {"node_ids": ["04a7ab6a-e449-49c2-88ff-551eaf09a7e5"], "metadata": {"page_label": "21", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}}, "d3aec5d3-5d75-4830-90fd-a7ac97c2f698": {"node_ids": ["084b76d2-9d47-49fe-a704-b3176e748bac", "344844a4-ac2d-4848-ba1f-ad29e005d00b"], "metadata": {"page_label": "22", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}}, "1e556286-3fcd-4921-a794-873084fd4807": {"node_ids": ["dac45c93-a982-4297-a161-7ffde6437e8d", "f81b47a7-9715-484a-b708-285119d6544b", "91640ef9-5917-42ca-9b8c-d7b1f107157e"], "metadata": {"page_label": "23", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}}, "d34d3d5f-b1d2-41f9-820e-9c007640f7a4": {"node_ids": ["96acc44e-3d51-402d-ae65-1743756780a8", "3448e5cd-a0d7-4de7-a954-26dfef75c13a", "562bcee7-4979-4141-af4b-061ac126fe93"], "metadata": {"page_label": "24", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}}, "c6cc38c7-6e41-4e35-a796-7a6352bcecb6": {"node_ids": ["873df3c6-ad2d-4813-abc2-aae7e5f93997", "353ddba9-89a1-4894-8fed-be6422271f9d"], "metadata": {"page_label": "25", "file_name": "YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_path": "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\data\\YOLO-v1_to_YOLO-v8_the_Rise_of_YOLO_and_Its_Comple.pdf", "file_type": "application/pdf", "file_size": 6049935, "creation_date": "2024-04-13", "last_modified_date": "2024-04-13"}}}}