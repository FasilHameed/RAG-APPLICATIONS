{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retreival augmentation\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abc\n"
     ]
    }
   ],
   "source": [
    "os.environ.setdefault('OPENAI_API_KEY', os.getenv('OPENAI_API_KEY'))\n",
    "print('abc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\faisa\\OneDrive\\Desktop\\Gen AI\\Llama And RAG\\RAG Application\\RAG\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Parsing nodes: 100%|██████████| 36/36 [00:00<00:00, 212.14it/s]\n",
      "Generating embeddings: 100%|██████████| 68/68 [00:03<00:00, 18.35it/s]\n"
     ]
    }
   ],
   "source": [
    "index = VectorStoreIndex.from_documents(documents,show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama_index.core.indices.vector_store.base.VectorStoreIndex at 0x20c214e0850>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine=index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO stands for \"You Only Look Once\" and refers to a variant of object detectors that have rapidly evolved since their inception in 2015, with the latest release being YOLO-v8 in January 2023. These YOLO variants are known for their real-time and high-classification performance based on efficient computational parameters. They are designed to detect objects of interest within images and videos with speed and accuracy, making them suitable for applications like industrial defect detection and quality inspection.\n"
     ]
    }
   ],
   "source": [
    "print(query_engine.query('what is a yolo'),end='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Response: YOLO stands for \"You Only Look Once\" and it is a\n",
      "variant of object detectors that focus on real-time and high-\n",
      "classification performance with efficient computational parameters.\n",
      "The YOLO variants are designed to detect objects of interest within\n",
      "images and videos quickly and accurately, making them suitable for\n",
      "applications like industrial defect detection and quality inspection.\n",
      "______________________________________________________________________\n",
      "Source Node 1/2\n",
      "Node ID: f3e64038-9436-4309-908f-e9674c3a210d\n",
      "Similarity: 0.7749241778771915\n",
      "Text: Citation: Hussain, M. YOLO-v1 to YOLO-v8, the Rise of YOLO and\n",
      "Its Complementary Nature toward Digital Manufacturing and Industrial\n",
      "Defect Detection. Machines 2023 ,11, 677. https://doi.org/10.3390/\n",
      "machines11070677 Academic Editor: Sang Do Noh Received: 30 May 2023\n",
      "Revised: 15 June 2023 Accepted: 21 June 2023 Published: 23 June 2023\n",
      "Copyright: ...\n",
      "______________________________________________________________________\n",
      "Source Node 2/2\n",
      "Node ID: 7730b41d-6e55-475c-9fad-3c0ba24071fe\n",
      "Similarity: 0.7698517787740589\n",
      "Text: Machines 2023 ,11, 677 14 of 25 2.8. YOLO-v8 The latest addition\n",
      "to the family of YOLO was conﬁrmed in January 2023 with the release of\n",
      "YOLO-v8 [ 80] by Ultralytics (also released YOLO-v5). Although a paper\n",
      "release is impending and many features are yet to be added to the\n",
      "YOLO-v8 repository, initial comparisons of the newcomer against its\n",
      "predec...\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.response.pprint_utils import pprint_response\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.core.response.pprint_utils import pprint_response\n",
    "\n",
    "response=query_engine.query('what is a yolo')\n",
    "pprint_response(response,show_source=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting more Enhanced Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever=VectorIndexRetriever(index=index,similarity_top_k=4)\n",
    "query_engine=RetrieverQueryEngine(retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "response2=query_engine.query('what is a yolo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Response: YOLO stands for \"You Only Look Once.\" It is a variant\n",
      "of object detectors that focuses on real-time and high-classification\n",
      "performance based on efficient computational parameters. The YOLO\n",
      "variants have evolved rapidly since the original release in 2015, with\n",
      "the latest version being YOLO-v8. These variants are designed to\n",
      "address the requirements of automated quality inspection within\n",
      "industrial settings, particularly for tasks like surface defect\n",
      "detection.\n",
      "______________________________________________________________________\n",
      "Source Node 1/4\n",
      "Node ID: f3e64038-9436-4309-908f-e9674c3a210d\n",
      "Similarity: 0.7749241778771915\n",
      "Text: Citation: Hussain, M. YOLO-v1 to YOLO-v8, the Rise of YOLO and\n",
      "Its Complementary Nature toward Digital Manufacturing and Industrial\n",
      "Defect Detection. Machines 2023 ,11, 677. https://doi.org/10.3390/\n",
      "machines11070677 Academic Editor: Sang Do Noh Received: 30 May 2023\n",
      "Revised: 15 June 2023 Accepted: 21 June 2023 Published: 23 June 2023\n",
      "Copyright: ...\n",
      "______________________________________________________________________\n",
      "Source Node 2/4\n",
      "Node ID: 7730b41d-6e55-475c-9fad-3c0ba24071fe\n",
      "Similarity: 0.7698517787740589\n",
      "Text: Machines 2023 ,11, 677 14 of 25 2.8. YOLO-v8 The latest addition\n",
      "to the family of YOLO was conﬁrmed in January 2023 with the release of\n",
      "YOLO-v8 [ 80] by Ultralytics (also released YOLO-v5). Although a paper\n",
      "release is impending and many features are yet to be added to the\n",
      "YOLO-v8 repository, initial comparisons of the newcomer against its\n",
      "predec...\n",
      "______________________________________________________________________\n",
      "Source Node 3/4\n",
      "Node ID: 0c76589c-a328-4c70-90e2-067bb5f9d1e3\n",
      "Similarity: 0.7679702546003019\n",
      "Text: Machines 2023 ,11, 677 21 of 25 Table 6. GitHub popularity\n",
      "comparison. YOLO Variant Stars (K) V3 9.3 V4 20.2 V5 34.7 V6 4.6 V7\n",
      "8.4 V8 2.9 4.2. YOLO and Industrial Defect Detection Manifestations of\n",
      "the fourth industrial revolution can be observed at present in an ad-\n",
      "hoc manner, spanning across various industries. With respect to the\n",
      "manufacturin...\n",
      "______________________________________________________________________\n",
      "Source Node 4/4\n",
      "Node ID: 0c38e4c7-dca9-49ba-a894-ac96f5422d82\n",
      "Similarity: 0.7672379606199955\n",
      "Text: Figure 1. Object detector anatomy.  2. Original YOLO Algorithm\n",
      "YOLO was introduced to the computer vision community via a paper\n",
      "release in 2015  by Joseph Redmon et al. [29] titled ‘You Only Look\n",
      "Once: Uni ﬁed, Real-Time Object De- tection’. The paper reframed\n",
      "object detection, presenting it essentially as a single pass re-\n",
      "gression problem, in...\n"
     ]
    }
   ],
   "source": [
    "pprint_response(response2,show_source=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "postprocessor=SimilarityPostprocessor(similarity_cutoff=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine=RetrieverQueryEngine(retriever=retriever,node_postprocessors=[postprocessor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "response3=query_engine.query('what is a yolo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Response: YOLO stands for \"You Only Look Once,\" which is a\n",
      "variant of object detectors that has rapidly evolved since its\n",
      "inception in 2015. The YOLO variants are known for their real-time and\n",
      "high-classification performance based on efficient computational\n",
      "parameters. These variants aim to detect objects of interest within\n",
      "images and videos with speed and accuracy, making them suitable for\n",
      "applications like industrial defect detection and quality inspection.\n",
      "______________________________________________________________________\n",
      "Source Node 1/4\n",
      "Node ID: f3e64038-9436-4309-908f-e9674c3a210d\n",
      "Similarity: 0.7749241778771915\n",
      "Text: Citation: Hussain, M. YOLO-v1 to YOLO-v8, the Rise of YOLO and\n",
      "Its Complementary Nature toward Digital Manufacturing and Industrial\n",
      "Defect Detection. Machines 2023 ,11, 677. https://doi.org/10.3390/\n",
      "machines11070677 Academic Editor: Sang Do Noh Received: 30 May 2023\n",
      "Revised: 15 June 2023 Accepted: 21 June 2023 Published: 23 June 2023\n",
      "Copyright: ...\n",
      "______________________________________________________________________\n",
      "Source Node 2/4\n",
      "Node ID: 7730b41d-6e55-475c-9fad-3c0ba24071fe\n",
      "Similarity: 0.7698517787740589\n",
      "Text: Machines 2023 ,11, 677 14 of 25 2.8. YOLO-v8 The latest addition\n",
      "to the family of YOLO was conﬁrmed in January 2023 with the release of\n",
      "YOLO-v8 [ 80] by Ultralytics (also released YOLO-v5). Although a paper\n",
      "release is impending and many features are yet to be added to the\n",
      "YOLO-v8 repository, initial comparisons of the newcomer against its\n",
      "predec...\n",
      "______________________________________________________________________\n",
      "Source Node 3/4\n",
      "Node ID: 0c76589c-a328-4c70-90e2-067bb5f9d1e3\n",
      "Similarity: 0.7679702546003019\n",
      "Text: Machines 2023 ,11, 677 21 of 25 Table 6. GitHub popularity\n",
      "comparison. YOLO Variant Stars (K) V3 9.3 V4 20.2 V5 34.7 V6 4.6 V7\n",
      "8.4 V8 2.9 4.2. YOLO and Industrial Defect Detection Manifestations of\n",
      "the fourth industrial revolution can be observed at present in an ad-\n",
      "hoc manner, spanning across various industries. With respect to the\n",
      "manufacturin...\n",
      "______________________________________________________________________\n",
      "Source Node 4/4\n",
      "Node ID: 0c38e4c7-dca9-49ba-a894-ac96f5422d82\n",
      "Similarity: 0.7672379606199955\n",
      "Text: Figure 1. Object detector anatomy.  2. Original YOLO Algorithm\n",
      "YOLO was introduced to the computer vision community via a paper\n",
      "release in 2015  by Joseph Redmon et al. [29] titled ‘You Only Look\n",
      "Once: Uni ﬁed, Real-Time Object De- tection’. The paper reframed\n",
      "object detection, presenting it essentially as a single pass re-\n",
      "gression problem, in...\n"
     ]
    }
   ],
   "source": [
    "pprint_response(response3,show_source=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers are a model architecture that relies entirely on an attention mechanism to draw global dependencies between input and output sequences. They do not use recurrent layers like traditional encoder-decoder architectures, but instead utilize stacked self-attention and point-wise, fully connected layers for both the encoder and decoder. This design allows for more parallelization during training and has shown significant improvements in translation quality and computational efficiency compared to models based on recurrent or convolutional layers.\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "\n",
    "# check if storage already exists\n",
    "PERSIST_DIR = \"./storage\"\n",
    "if not os.path.exists(PERSIST_DIR):\n",
    "    # load the documents and create the index\n",
    "    documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "    index = VectorStoreIndex.from_documents(documents)\n",
    "    # store it for later\n",
    "    index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
    "else:\n",
    "    # load the existing index\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
    "    index = load_index_from_storage(storage_context)\n",
    "\n",
    "# either way we can now query the index\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What are transformers?\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
